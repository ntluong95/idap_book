---
title: "Using LLMs in Python for Text Generation"
--- 

In this tutorial, we'll explore how to leverage Large Language Models (LLMs) to generate text using OpenAI's API. We'll use the `gpt-4o-mini` model to generate responses to fixed and variable prompts, optimize our code with helper functions and vectorization, and handle data using pandas DataFrames.

## Setting Up the OpenAI Client

First, we need to set up the OpenAI client using your API key. Ensure you have your API key stored securely, for example, in an environment variable called `OPENAI_API_KEY` within a `.env` file.

```{python}
import os
from openai import OpenAI

# Initialize the OpenAI client with your API key
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
)
```


Alternatively, you can pass your API key directly when initializing the client, but be cautious not to expose it in your code, especially if you plan to share or publish it.

## Defining Helper Functions

To simplify our code and avoid repetition, we'll define a helper function for making API calls. API calls often contain boilerplate code, so encapsulating this logic in a function makes our code cleaner and more maintainable.

If you ever forget how to structure the API calls, refer to the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/introduction) or search for "OpenAI Python API example" online.

Here's how we can define the `llm_chat` function:

```{python}
def llm_chat(message):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content
```

This function takes a `message` as input, sends it to the LLM, and returns the generated response. The `model` parameter specifies which model to useâ€”in this case, `gpt-4o-mini`.

## Fixed Questions

Let's start by sending a fixed question to the `gpt-4o-mini` model and retrieving a response.

```{python}
# Example usage
response = llm_chat("What is the most tourist-friendly city in France?")
print(response)
```

This code sends the question to the model and prints the response. You should see an answer like "Paris" or a similar response.

**Practice Question**

*Question*: Use the `llm_chat` function to ask the model for the most tourist-friendly city in Brazil.

*Solution*:

```{python}
response = llm_chat("What is the most tourist-friendly city in Brazil?")
print(response)
```

## Variable Questions

Often, you'll want to generate responses based on varying inputs. Let's create a function that takes a country as input and asks the model for the most tourist-friendly city in that country.

```{python}
def city_rec(country):
    prompt = f"What is the most tourist-friendly city in {country}?"
    return llm_chat(prompt)
```

Now, you can get recommendations for different countries by calling `city_rec("Country Name")`.

However, if we try to use this function on a list of countries directly, it won't process each country individually. Instead, it will attempt to concatenate the list into a single string, which isn't the desired behavior.

```{python}
# Incorrect usage
countries = ["Nigeria", "Chile", "France", "Canada"]
response = city_rec(countries)
```

This will likely result in an error or an unexpected prompt like "What is the most tourist-friendly city in ['Nigeria', 'Chile', 'France', 'Canada']?"

To process each country individually, we can use NumPy's `vectorize` function. This function transforms `city_rec` so that it can accept arrays (like lists or NumPy arrays) and apply the function element-wise.

```{python}
import numpy as np

# Vectorize the function
city_rec_vec = np.vectorize(city_rec)

# Apply the function to each country
countries = ["Nigeria", "Chile", "France", "Canada"]
cities = city_rec_vec(countries)
cities
```

This code will output an array of city recommendations corresponding to each country.

**Practice Question**

*Question*: Given a list of countries `["Japan", "Australia", "Brazil"]`, use the `city_rec_vec` function to get the most tourist-friendly cities in each country.

*Solution*:

```{python}
countries = ["Japan", "Australia", "Brazil"]
cities = city_rec_vec(countries)
cities
```

## Working with DataFrames

Handling data in bulk is streamlined using pandas DataFrames. Let's create a DataFrame that we'll use throughout our examples.

```{python}
import pandas as pd

# Create a DataFrame of countries
countries = ["United Kingdom", "Japan", "Brazil", "France"]
country_df = pd.DataFrame({"country": countries})
country_df
```

Now, we can apply the vectorized city recommendations to our DataFrame.

```{python}
# Apply the vectorized function to the DataFrame
country_df['city_rec'] = city_rec_vec(country_df['country'])
country_df
```

This adds a new column `city_rec` with the recommended city for each country.

**Practice Question**

*Question*: Modify the `get_local_dishes` function to take a country name as input and return some of the most famous local dishes from that country. Then apply it to the `country_df` DataFrame to add a column with the most famous local dishes for each country.

*Solution*:

```{python}
def get_local_dishes(country):
    prompt = f"What are some of the most famous local dishes from {country}?"
    return llm_chat(prompt)

# Vectorize the function
get_local_dishes_vec = np.vectorize(get_local_dishes)

# Apply to the DataFrame
country_df['local_dishes'] = get_local_dishes_vec(country_df['country'])
country_df
```


# Wrap Up

In this tutorial, we've learned how to use LLMs for text generation, how to handle structured outputs, and how to work with DataFrames. This knowledge will be useful for a wide range of applications, from simple chatbots to complex data analysis workflows.