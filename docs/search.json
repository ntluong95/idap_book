[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "Introduction\nDRAFT book for the course “Introduction to Data Science with Python”\nThis book is a compilation of lesson notes for a 3-month online course offered by The GRAPH Courses. To access the lesson videos, exercise files, and online quizzes, please visit our website, thegraphcourses.org.\nThe GRAPH Courses is a project of the Global Research and Analyses for Public Health (GRAPH) Network, a non-profit organization dedicated to making code and data skills accessible through affordable live bootcamps and free self-paced courses.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Introduction to Data Science with Python",
    "section": "Contributors",
    "text": "Contributors\nWe are extremely grateful to the following individuals who have contributed to the development of these materials over several years:\nAmanda McKinley, Andree Valle Campos, Aziza Merzouki, Benedict Nguimbis, Bennour Hsin, Camille Beatrice Valera, Daniel Camara, Eduardo Araujo, Elton Mukonda, Guy Wafeu, Imad El Badisy, Imane Bensouda Korachi, Joy Vaz, Kene David Nwosu, Lameck Agasa, Laure Nguemo, Laure Vancauwenberghe, Matteo Franza, Michal Shrestha, Olivia Keiser, Sabina Rodriguez Velasquez, Sara Botero Mesa.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#partners-funders",
    "href": "index.html#partners-funders",
    "title": "Introduction to Data Science with Python",
    "section": "Partners & Funders",
    "text": "Partners & Funders\n\nUniversity of Geneva\nUniversity of Oxford\nWorld Health Organization\nGlobal Fund\nErnst Goehner Foundation",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html",
    "href": "p_foundations_google_colab.html",
    "title": "1  Introduction to Google Colab",
    "section": "",
    "text": "1.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#learning-objectives",
    "href": "p_foundations_google_colab.html#learning-objectives",
    "title": "1  Introduction to Google Colab",
    "section": "",
    "text": "Understand what Google Colab is and its advantages for data science and AI\nLearn how to access and navigate Google Colab\nCreate and manage notebooks in Google Colab\nRun Python code in Colab cells\nUse text cells for explanations and formatting\nImport and use pre-installed libraries for data analysis\nImport and use data to perform analysis\nShare Colab notebooks",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#introduction",
    "href": "p_foundations_google_colab.html#introduction",
    "title": "1  Introduction to Google Colab",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\nGoogle Collaboratory, or Colab for short, is a free online platform that allows you to work with Python or R code in your browser. It’s a great way to get started with Python, as you don’t have to install anything on your computer.\nSome limitations if you’re running heavy workload though. Can get timeout. But for beginner data analysts, it’s perfect and free.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-started-with-colab",
    "href": "p_foundations_google_colab.html#getting-started-with-colab",
    "title": "1  Introduction to Google Colab",
    "section": "1.3 Getting Started with Colab",
    "text": "1.3 Getting Started with Colab\n\nSearch for “Google Colab” in your browser\nUsually the first option. Currently, it’s colab.research.google.com, but it may change in the future.\nSign in with your Google account (create a Gmail account if you don’t have one)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "href": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "title": "1  Introduction to Google Colab",
    "section": "1.4 Creating and Managing Notebooks",
    "text": "1.4 Creating and Managing Notebooks\n\nNotebooks are the main way to organize work in Colab. They contain code cells and text cells.\nCreate a new notebook: File &gt; New Notebook\nRename your notebook for better organization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#working-with-code-cells",
    "href": "p_foundations_google_colab.html#working-with-code-cells",
    "title": "1  Introduction to Google Colab",
    "section": "1.5 Working with Code Cells",
    "text": "1.5 Working with Code Cells\n\nCode cells are where you write and execute Python code\nType 1 + 1 in a cell then run it\nRun a cell by clicking the play button or using keyboard shortcuts:\n\nCommand + Enter: Run the current cell\nShift + Enter: Run the current cell and create a new one below\n\nTry to get comfortable with keyboard shortcuts\nMay take a while to run the first time. See it’s using Python\nCan change runtime to R actually\nWhen you run a cell, the output is displayed below the cell\nTo see multiple outputs, explicitly print them",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#text-cells",
    "href": "p_foundations_google_colab.html#text-cells",
    "title": "1  Introduction to Google Colab",
    "section": "1.6 Text Cells",
    "text": "1.6 Text Cells\n\nUse text cells for explanations and titles\nThe toolbar makes formatting easy, but pay attention to the generated markdown",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#example-of-working-with-data",
    "href": "p_foundations_google_colab.html#example-of-working-with-data",
    "title": "1  Introduction to Google Colab",
    "section": "1.7 Example of working with data",
    "text": "1.7 Example of working with data\n\nClick on the files tab to see the sample_data folder\nImport the California housing test dataset:\n\nimport pandas\nhousing_data = pandas.read_csv(\"/content/sample_data/california_housing_test.csv\")\n\nView the dataset by typing housing_data in a cell and running it",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#practice",
    "href": "p_foundations_google_colab.html#practice",
    "title": "1  Introduction to Google Colab",
    "section": "1.8 Practice",
    "text": "1.8 Practice\nImport the train dataset and repeat the process",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "href": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "title": "1  Introduction to Google Colab",
    "section": "1.9 Getting data from your Drive",
    "text": "1.9 Getting data from your Drive\n\nSearch for “[your city] housing filetype:csv” on Google\nDownload the file\nIn the files tab, click the button to mount your drive\nCreate a folder and upload the downloaded file\nImport it with pandas as before",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "href": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "title": "1  Introduction to Google Colab",
    "section": "1.10 Where is your notebook saved?",
    "text": "1.10 Where is your notebook saved?\n\nAll work is automatically saved to your Google Drive\nAccess your notebooks at drive.google.com in the “Colab Notebooks” folder",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#sharing-and-collaborating",
    "href": "p_foundations_google_colab.html#sharing-and-collaborating",
    "title": "1  Introduction to Google Colab",
    "section": "1.11 Sharing and Collaborating",
    "text": "1.11 Sharing and Collaborating\n\nShare notebooks with a link, giving viewer or editor access\nAccess notebooks later from your Google Drive\nDownload notebooks in various formats (ipynb, py)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#conclusion",
    "href": "p_foundations_google_colab.html#conclusion",
    "title": "1  Introduction to Google Colab",
    "section": "1.12 Conclusion",
    "text": "1.12 Conclusion\nGoogle Colab provides a powerful, accessible platform for data science and AI projects. Its pre-configured environment, free access to hardware accelerators, and easy sharing features make it an excellent choice for beginners and experienced practitioners alike.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html",
    "href": "p_foundations_coding_basics.html",
    "title": "2  Coding basics",
    "section": "",
    "text": "2.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#learning-objectives",
    "href": "p_foundations_coding_basics.html#learning-objectives",
    "title": "2  Coding basics",
    "section": "",
    "text": "You can write and use comments in Python (single-line and multi-line).\nYou know how to use Python as a calculator for basic arithmetic operations and understand the order of operations.\nYou can use the math library for more complex mathematical operations.\nYou understand how to use proper spacing in Python code to improve readability.\nYou can create, manipulate, and reassign variables of different types (string, int, float).\nYou can get user input and perform calculations with it.\nYou understand the basic rules and best practices for naming variables in Python.\nYou can identify and fix common errors related to variable usage and naming.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#introduction",
    "href": "p_foundations_coding_basics.html#introduction",
    "title": "2  Coding basics",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nIn this lesson, you will learn the basics of using Python.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “coding_basics.py” or “coding_basics.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#comments",
    "href": "p_foundations_coding_basics.html#comments",
    "title": "2  Coding basics",
    "section": "2.3 Comments",
    "text": "2.3 Comments\nComments are text that is ignored by Python. They are used to explain what the code is doing.\nYou use the symbol #, pronounced “hash” or “pound”, to start a comment. Anything after the # on the same line is ignored. For example:\n\n# Addition\n2 + 2\n\n4\n\n\nIf we just tried to write Addition above the code, it would cause an error:\n\nAddition\n2 + 2\n\nNameError: name 'Addition' is not defined\nWe can put the comment on the same line as the code, but it needs to come after the code.\n\n2 + 2  # Addition\n\n4\n\n\nTo write multiple lines of comments, you can either add more # symbols:\n\n# Addition\n# Add two numbers\n2 + 2\n\n4\n\n\nOr you can use triple quotes ''' or \"\"\":\n\n'''\nAddition:\nBelow we add two numbers\n'''\n2 + 2\n\n4\n\n\nOr:\n\n\"\"\"\nAddition:\nBelow we add two numbers\n\"\"\"\n2 + 2\n\n4\n\n\n\n\n\n\n\n\nVocab\n\n\n\nComment: A piece of text in your code that is ignored by Python. Comments are used to explain what the code is doing and are meant for human readers.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.3.1 Q: Commenting in Python\nWhich of the following code chunks are valid ways to comment code in Python?\n# add two numbers\n2 + 2\n2 + 2 # add two numbers\n''' add two numbers\n2 + 2\n# add two numbers 2 + 2\nCheck your answer by trying to run each code chunk.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#python-as-a-calculator",
    "href": "p_foundations_coding_basics.html#python-as-a-calculator",
    "title": "2  Coding basics",
    "section": "2.4 Python as a calculator",
    "text": "2.4 Python as a calculator\nAs you have already seen, Python works as a calculator in standard ways.\nBelow are some other examples of basic arithmetic operations:\n\n2 - 2 # two minus two\n\n0\n\n\n\n2 * 2  # two times two \n\n4\n\n\n\n2 / 2  # two divided by two\n\n1.0\n\n\n\n2 ** 2  # two raised to the power of two\n\n4\n\n\nThere are a few other operators you may come across. For example, % is the modulo operator, which returns the remainder of the division.\n\n10 % 3  # ten modulo three\n\n1\n\n\n// is the floor division operator, which divides then rounds down to the nearest whole number.\n\n10 // 3  # ten floor division three\n\n3\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.4.1 Q: Modulo and floor division\nGuess the result of the following code chunks then run them to check your answer:\n\n5 % 4\n\n\n5 // 4\n\n\n\n\n2.4.2 Order of operations\nPython obeys the standard PEMDAS order of operations (Parentheses, Exponents, Multiplication, Division, Addition, Subtraction).\nFor example, multiplication is evaluated before addition, so below the result is 6.\n\n2 + 2 * 2   \n\n6\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.4.3 Q: Evaluating arithmetic expressions\nWhich, if any, of the following code chunks will evaluate to 10?\n\n2 + 2 * 4\n\n\n6 + 2 ** 2\n\n\n\n\n\n2.4.4 Using the math library\nWe can also use the math library to do more complex mathematical operations. For example, we can use the math.sqrt function to calculate the square root of a number.\n\nimport math\nmath.sqrt(100)  # square root\n\n10.0\n\n\nOr we can use the math.log function to calculate the natural logarithm of a number.\n\nimport math\nmath.log(100)  # logarithm\n\n4.605170185988092\n\n\nmath.sqrt and math.log are examples of Python functions, where an argument (e.g., 100) is passed to the function to perform a calculation.\nWe will learn more about functions later.\n\n\n\n\n\n\nVocab\n\n\n\nFunction: A reusable block of code that performs a specific task. Functions often take inputs (called arguments) and return outputs.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\n2.4.5 Q: Using the math library\nUsing the math library, calculate the square root of 81.\nWrite your code below and run it to check your answers:\n\n# Your code here\n\n\n\n2.4.6 Q: Describing the use of the random library\nConsider the following code, which generates a random number between 1 and 10:\n\nimport random\nrandom.randint(1, 10)\n\n8\n\n\nIn that code, identify the library, the function, and the argument(s) to the function.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#spacing-in-code",
    "href": "p_foundations_coding_basics.html#spacing-in-code",
    "title": "2  Coding basics",
    "section": "2.5 Spacing in code",
    "text": "2.5 Spacing in code\nGood spacing makes your code easier to read. In Python, two simple spacing practices can greatly improve your code’s readability: using blank lines and adding spaces around operators.\n\n2.5.1 Blank Lines\nUse blank lines to separate different parts of your code:\nFor example, consider the following code chunk:\n\n# Set up numbers\nx = 5\ny = 10\n# Perform calculation\nresult = x + y\n# Display result\nprint(result)\n\n15\n\n\nWe can add blank lines to separate the different parts of the code:\n\n# Set up numbers\nx = 5\ny = 10\n\n# Perform calculation\nresult = x + y\n\n# Display result\nprint(result)\n\n15\n\n\nBlank lines help organize your code into logical sections, similar to paragraphs in writing.\n\n\n2.5.2 Spaces around operators\nAdding spaces around mathematical operators improves readability:\n\n# Hard to read\nx=5+3*2\n\n# Easy to read\nx = 5 + 3 * 2\n\nWhen listing items, add a space after each comma:\n\n# Hard to read\nprint(1,2,3)\n\n# Easy to read\nprint(1, 2, 3)\n\nThis practice follows the convention in written English, where we put a space after a comma. It makes lists of items in your code easier to read.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#variables-in-python",
    "href": "p_foundations_coding_basics.html#variables-in-python",
    "title": "2  Coding basics",
    "section": "2.6 Variables in Python",
    "text": "2.6 Variables in Python\n\n2.6.1 Create a variable\nAs you have seen, to store a value for future use in Python, we assign it to a variable with the assignment operator, =.\n\nmy_var = 2 + 2  # assign the result of `2 + 2 ` to the variable called `my_var`\nprint(my_var)  # print my_var\n\n4\n\n\nNow that you’ve created the variable my_var, Python knows about it and will keep track of it during this Python session.\nYou can open your environment to see what variables you have created. This looks different depending on your IDE.\nSo what exactly is a variable? Think of it as a named container that can hold a value. When you run the code below:\n\nmy_var = 20\n\nyou are telling Python, “store the number 20 in a variable named ‘my_var’”.\nOnce the code is run, we would say, in Python terms, that “the value of variable my_var is 20”.\nTry to come up with a similar sentence for this code chunk:\n\nfirst_name = \"Joanna\"\n\nAfter we run this code, we would say, in Python terms, that “the value of the first_name variable is Joanna”.\n\n\n\n\n\n\nVocab\n\n\n\nA text value like “Joanna” is called a string, while a number like 20 is called an integer. If the number has a decimal point, it is called a float, which is short for “floating-point number”.\n\n\n\n\n\n\n\n\nVocab\n\n\n\nVariable: A named container that can hold a value. In Python, variables can store different types of data, including numbers, strings, and more complex objects.\n\n\n\n\n2.6.2 Reassigning Variables\nReassigning a variable is like changing the contents of a container.\nFor example, previously we ran this code to store the value “Joanna” inside the first_name variable:\n\nfirst_name = \"Joanna\"\n\nTo change this to a different value, simply run a new assignment statement with a new value:\n\nfirst_name = \"Luigi\"\n\nYou can print the variable to observe the change:\n\nfirst_name\n\n'Luigi'\n\n\n\n\n2.6.3 Working with Variables\nMost of your time in Python will be spent manipulating variables. Let’s see some quick examples.\nYou can run simple commands on variables. For example, below we store the value 100 in a variable and then take the square root of the variable:\n\nimport math\n\nmy_number = 100\nmath.sqrt(my_number)\n\n10.0\n\n\nPython “sees” my_number as the number 100, and so is able to evaluate its square root.\n\nYou can also combine existing variables to create new variables. For example, type out the code below to add my_number to itself, and store the result in a new variable called my_sum:\n\nmy_sum = my_number + my_number\nmy_sum\n\n200\n\n\nWhat should be the value of my_sum? First take a guess, then check it by printing it.\n\nPython also allows us to concatenate strings with the + operator. For example, we can concatenate the first_name and last_name variables to create a new variable called full_name:\n\nfirst_name = \"Joanna\"\nlast_name = \"Luigi\"\nfull_name = first_name + \" \" + last_name\nfull_name\n\n'Joanna Luigi'\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.6.4 Q: Variable assignment and manipulation\nConsider the code below. What is the value of the answer variable? Think about it, then run the code to check your answer.\n\neight = 9\nanswer = eight - 8\nanswer\n\n\n\n\n\n2.6.5 Getting User Input\nThough it’s not used often in data analysis, the input() function from Python is a cool Python feature that you should know about. It allows you to get input from the user.\nHere’s a simple example. We can request user input and store it in a variable called name.\n\nname = input()\n\nAnd then we can print a greeting to the user.\n\nprint(\"Hello,\", name)\n\nWe can also include a question for the input prompt:\n\nname = input('What is your name? ')\nprint(\"Hello,\", name)\n\nLet’s see another example. We’ll tell the user how many letters are in their name.\n\nname = input('What is your name? ')\nprint(\"There are\", len(name), \"letters in your name\")\n\nFor instance, if you run this code and enter “Kene”, you might see:\nWhat is your name? Kene\nThere are 4 letters in your name\n\n\n\n\n\n\nPractice\n\n\n\n2.6.6 Q: Using input()\nWrite a short program that asks the user for their favorite color and then prints a message saying “Your favorite color is [color]!”. Test your program by running it and entering a color.\n\n\n\n\n2.6.7 Common Error with Variables\nOne of the most common errors you’ll encounter when working with variables in Python is the NameError. This occurs when you try to use a variable that hasn’t been defined yet. For example:\n\nmy_number = 48  # define `my_number`\nMy_number + 2  # attempt to add 2 to `my_number`\n\nIf you run this code, you’ll get an error message like this:\nNameError: name 'My_number' is not defined\nHere, Python returns an error message because we haven’t created (or defined) the variable My_number yet. Recall that Python is case-sensitive; we defined my_number but tried to use My_number.\nTo fix this, make sure you’re using the correct variable name:\n\nmy_number = 48\nmy_number + 2  # This will work and return 50\n\n50\n\n\nAlways double-check your variable names to avoid this error. Remember, in Python, my_number, My_number, and MY_NUMBER are all different variables.\n\nWhen you first start learning Python, dealing with errors can be frustrating. They’re often difficult to understand.\nBut it’s important to get used to reading and understanding errors, because you’ll get them a lot through your coding career.\nLater, we will show you how to use Large Language Models (LLMs) like ChatGPT to debug errors.\nAt the start though, it’s good to try to spot and fix errors yourself.\n\n\n\n\n\n\nPractice\n\n\n\n2.6.8 Q: Debugging variable errors\nThe code below returns an error. Why? (Look carefully)\n\nmy_1st_name = \"Kene\"\nmy_last_name = \"Nwosu\"\n\nprint(my_Ist_name, my_last_name)\n\nHint: look at the variable names. Are they consistent?\n\n\n\n\n2.6.9 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n— Phil Karlton.\n\nBecause much of your work in Python involves interacting with variables you have created, picking intelligent names for these variables is important.\nNaming variables is difficult because names should be both short (so that you can type them quickly) and informative (so that you can easily remember what the variable contains), and these two goals are often in conflict.\nSo names that are too long, like the one below, are bad because they take forever to type.\n\nsample_of_the_ebola_outbreak_dataset_from_sierra_leone_in_2014\n\nAnd a name like data is bad because it is not informative; the name does not give a good idea of what the variable contains.\nAs you write more Python code, you will learn how to write short and informative names.\n\nFor names with multiple words, there are a few conventions for how to separate the words:\n\nsnake_case = \"Snake case uses underscores\"\ncamelCase = \"Camel case capitalizes new words (but not the first word)\"\nPascalCase = \"Pascal case capitalizes all words including the first\"\n\nWe recommend snake_case, which uses all lower-case words, and separates words with _.\n\nNote too that there are some limitations on variable names:\n\nNames must start with a letter or underscore. So 2014_data is not a valid name (because it starts with a number). Try running the code chunk below to see what error you get.\n\n\n2014_data = \"This is not a valid name\"\n\n\nNames can only contain letters, numbers, and underscores (_). So ebola-data or ebola~data or ebola data with a space are not valid names.\n\n\nebola-data = \"This is not a valid name\"\n\n\nebola~data = \"This is not a valid name\"\n\n\n\n\n\n\n\nSide note\n\n\n\nWhile we recommend snake_case for variable names in Python, you might see other conventions like camelCase or PascalCase, especially when working with code from other languages or certain Python libraries. It’s important to be consistent within your own code and follow the conventions of any project or team you’re working with.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.6.10 Q: Valid variable naming conventions\nWhich of the following variable names are valid in Python? Try to determine this without running the code, then check your answers by attempting to run each line.\nThen fix the invalid variable names.\n\n1st_name = \"John\"\nlast_name = \"Doe\"\nfull-name = \"John Doe\"\nage_in_years = 30\ncurrent@job = \"Developer\"\nPhoneNumber = \"555-1234\"\n_secret_code = 42",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#wrap-up",
    "href": "p_foundations_coding_basics.html#wrap-up",
    "title": "2  Coding basics",
    "section": "2.7 Wrap-up",
    "text": "2.7 Wrap-up\nIn this lesson, we’ve covered the fundamental building blocks of Python programming:\n\nComments: Using # for single-line and triple quotes for multi-line comments.\nBasic Arithmetic: Using Python as a calculator and understanding order of operations.\nMath Library: Performing complex mathematical operations.\nCode Spacing: Improving readability with proper spacing.\nVariables: Creating, manipulating, and reassigning variables of different types.\nGetting User Input: Using the input() function to get input from the user.\nVariable Naming: Following rules and best practices for naming variables.\nCommon Errors: Identifying and fixing errors related to variables.\n\nThese concepts form the foundation of Python programming. As you continue your journey, you’ll build upon these basics to create more complex and powerful programs. Remember, practice is key to mastering these concepts!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html",
    "href": "p_foundations_functions_methods.html",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "3.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#learning-objectives",
    "href": "p_foundations_functions_methods.html#learning-objectives",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "You understand what functions and methods are in Python.\nYou can identify and use arguments (parameters) in functions and methods.\nYou know how to call built-in functions and methods on objects.\nYou understand what libraries are in Python and how to import them.\nYou know how to install a simple external library and use it in your code.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#introduction",
    "href": "p_foundations_functions_methods.html#introduction",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn this lesson, you will learn about functions, methods, and libraries in Python, building on the basics we covered in the previous lesson.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “functions_and_libraries.py” or “functions_and_libraries.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#functions",
    "href": "p_foundations_functions_methods.html#functions",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.3 Functions",
    "text": "3.3 Functions\nA function is a block of code that performs a specific task. It can take inputs (arguments) and return outputs. Here’s an example of a built-in function with just one argument:\n\n# Using the len() function to get the length of a string\nlen(\"Python\")\n\n6\n\n\nThe round() function takes two arguments: the number to round and the number of decimal places to round to.\n\n# Using the round() function to round a number\nround(3.1415, 2)\n\n3.14\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.3.1 Q: Using built-in functions\nUse the abs() function to get the absolute value of -5.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#arguments-parameters",
    "href": "p_foundations_functions_methods.html#arguments-parameters",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.4 Arguments (Parameters)",
    "text": "3.4 Arguments (Parameters)\nArguments (also called parameters) are the values that you pass to a function (or method) when you call it.\nThere are different ways to pass arguments to a function.\nConsider again the round() function.\nIf we look at the documentation for the round() function, with :\n\nround?\n\nWe see that it takes two arguments:\n\nnumber: The number to round.\nndigits: The number of decimal places to round to.\n\nThere are two main ways to pass arguments to this function.\n\nPositional arguments: Passed in the order they are defined. Since the default order of the arguments is number then ndigits, we can pass the arguments in that order without specifying the argument names, as we did above.\n\n\nround(3.1415, 2)\n\n3.14\n\n\nIf we swap the order of the arguments, we get an error:\n\nround(2, 3.1415)\n\n\nKeyword arguments: Passed by specifying the argument name followed by a = and the argument value.\n\n\nround(number=3.1415, ndigits=2)\n\n3.14\n\n\nWith this method, we can pass the arguments in any order, as long as we use the argument names.\n\nround(ndigits=2, number=3.1415)\n\n3.14\n\n\nSpecifying the keyword is usually recommended, except for simple functions with very few arguments, or when the order of the arguments is obvious from context.\n\n\n\n\n\n\nPractice\n\n\n\n\n3.4.1 Q: Using Positional Arguments with pow()\nUse the pow() function to calculate 2 raised to the power of 5 by passing positional arguments. You may need to consult the documentation for the pow() function to see how it works.\nWrite your code below and run it to check your answer:\n\n# Your code here\n\n\n\n3.4.2 Q: Using Keyword Arguments with round()\nUse the round() function to round the number 9.8765 to 3 decimal places by specifying keyword arguments.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#methods",
    "href": "p_foundations_functions_methods.html#methods",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.5 Methods",
    "text": "3.5 Methods\nMethods are similar to functions, but they are associated with specific objects or data types. They are called using dot notation.\nFor example, every string object comes with a range of built-in methods, like upper() to convert to uppercase, lower() to convert to lowercase, replace() to replace substrings, and many more.\nLet’s see how to use these:\n\nname = \"python\"\nprint(name.upper())\nprint(name.lower())\nprint(name.replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nWe can also call the methods directly on the string object, without assigning it to a variable:\n\n# Using the upper() method on a string\nprint(\"python\".upper())\nprint(\"PYTHON\".lower())\nprint(\"python\".replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nSimilarly, numbers in Python come with some built-in methods. For example, the as_integer_ratio() (added in Python 3.8) method converts a decimalnumber to a ratio of two integers.\n\n# Using the as_integer_ratio() method on a float\nexample_decimal = 1.5\nexample_decimal.as_integer_ratio()\n\n(3, 2)\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.5.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nFunction\nMethod\nArgument (Parameter)\nDot Notation\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.5.2 Q: Using methods\n\nCall the replace() method on the string “Helo” to replace the single l with two ls.\nCall the split() method on the string “Hello World” to split the string into a list of words.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#libraries-in-python",
    "href": "p_foundations_functions_methods.html#libraries-in-python",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.6 Libraries in Python",
    "text": "3.6 Libraries in Python\nLibraries are collections of pre-written code that you can use in your programs. They extend the functionality of Python by providing additional functions and tools.\nFor example, the math library provides mathematical functions like sqrt() for square roots and sin() for sine.\nIf we try to use the sqrt() function without importing the math library, we get an error:\n\n# This will cause a NameError\nsqrt(16)\n\nWe can import the math library and use the sqrt() function like this:\n\n# Import the library\nimport math\n\nThen we can use the sqrt() function like this:\n\n# Use the sqrt() function\nmath.sqrt(16)\n\n4.0\n\n\nWe can get help on a function in a similar way, calling both the function and the library it’s in:\n\n# Get help on the sqrt() function\nmath.sqrt?\n\nWe can also import libraries with aliases. For example, we can import the math library with the alias m:\n\n# Import the entire library with an alias\nimport math as m\n# Then we can use the alias to call the function\nm.sqrt(16)\n\n4.0\n\n\nFinally, if you want to skip the alias/library name, you can either import the functions individually:\n\n# Import specific functions from a library\nfrom math import sqrt, sin\n# Then we can use the function directly\nsqrt(16)\nsin(0)\n\n0.0\n\n\nOr import everything from the library:\n\n# Import everything from the library\nfrom math import *\n# Then we can all functions directly, such as sqrt() and sin()\nsqrt(16)\ncos(0)\ntan(0)\nsin(0)\n\n0.0\n\n\nPhew that’s a lot of ways to import libraries! You’ll mostly see the import ... as ... syntax, and sometimes the from ... import ... syntax.\nNote that we typically import all required libraries at the top of the file, in a single code chunk. This is a good practice to follow.\n\n\n\n\n\n\nPractice\n\n\n\n3.6.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nLibrary (Module)\nImport\nAlias\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.6.2 Q: Using functions from imported libraries\n\nImport the random library and use the randint() function to generate a random integer between 1 and 10. You can use the ? operator to get help on the function after importing it.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#installing-libraries",
    "href": "p_foundations_functions_methods.html#installing-libraries",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.7 Installing Libraries",
    "text": "3.7 Installing Libraries\nWhile Python comes with many built-in libraries, there are thousands of additional libraries available that you can install to extend Python’s functionality even further. Let’s look at how to install and use a simple external library, with the cowsay library as an example.\nIf we try to import this library without first installing it, we get an error:\n\nimport cowsay\n\nTo install the library, you can use the !pip install command in a code cell in Google Colab. For cowsay, you would run:\n\n!pip install cowsay\n\nPip installs packages from a remote repository called PyPI. Anyone can create and upload a package to PyPI. After a few checks, it’s then available for anyone to install.\n\n\n\n\n\n\nSide-note\n\n\n\nFor those working on local Python instances, you can install cowsay using pip in your terminal:\npip install cowsay\n\n\nOnce installed, we can now import and use the cowsay library:\n\nimport cowsay\n\n# Make the cow say something\ncowsay.cow('Moo!')\n\n  ____\n| Moo! |\n  ====\n    \\\n     \\\n       ^__^\n       (oo)\\_______\n       (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n\n\nThis should display an ASCII art cow saying “Moo!”.\n\n\n\n\n\n\nPractice\n\n\n\n3.7.1 Q: Using the emoji library\n\nInstall the emoji library.\nImport the emoji library.\nConsult the help for the emojize() function in the emoji library.\nUse the emojize() function to display an emoji for “thumbs up”.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#wrap-up",
    "href": "p_foundations_functions_methods.html#wrap-up",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.8 Wrap-up",
    "text": "3.8 Wrap-up\nIn this lesson, we’ve covered:\n\nFunctions and methods in Python\nArguments (parameters) and how to use them\nImporting and using libraries\nInstalling and using an external library\n\nThese concepts are fundamental to Python programming and will be used extensively as you continue to develop your skills. Practice using different functions, methods, and libraries to become more comfortable with these concepts.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html",
    "href": "p_foundations_data_structures.html",
    "title": "4  Data Structures in Python",
    "section": "",
    "text": "4.1 Intro\nSo far in our Python explorations, we’ve been working with simple, single values, like numbers and strings. But, as you know, data usually comes in the form of larger structures. The structure most familiar to you is a table, with rows and columns.\nIn this lesson, we’re going to explore the building blocks for organizing data in Python, building up through lists, dictionaries, series, and finally tables, or, more formally,dataframes.\nLet’s dive in!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#learning-objectives",
    "href": "p_foundations_data_structures.html#learning-objectives",
    "title": "4  Data Structures in Python",
    "section": "4.2 Learning objectives",
    "text": "4.2 Learning objectives\n\nCreate and work with Python lists and dictionaries\nUnderstand and use Pandas Series\nExplore Pandas DataFrames for organizing structured data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#imports",
    "href": "p_foundations_data_structures.html#imports",
    "title": "4  Data Structures in Python",
    "section": "4.3 Imports",
    "text": "4.3 Imports\nWe need pandas for this lesson. You can import it like this:\n\nimport pandas as pd\n\nIf you get an error, you probably need to install it. You can do this by running !pip install pandas in a cell.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-lists",
    "href": "p_foundations_data_structures.html#python-lists",
    "title": "4  Data Structures in Python",
    "section": "4.4 Python Lists",
    "text": "4.4 Python Lists\nLists are like ordered containers that can hold different types of information. For example, you might have a list of things to buy:\n\nshopping = [\"apples\", \"bananas\", \"milk\", \"bread\"] \nshopping\n\n['apples', 'bananas', 'milk', 'bread']\n\n\nIn Python, we use something called “zero-based indexing” to access items in a list. This means we start counting positions from 0, not 1.\nLet’s see some examples:\n\nprint(shopping[0])  # First item (remember, we start at 0!)\nprint(shopping[1])  # Second item\nprint(shopping[2])  # Third item\n\napples\nbananas\nmilk\n\n\nIt might seem odd at first, but it’s a common practice in many programming languages. It has to do with how computers store information, and the ease of writing algorithms.\nWe can change the contents of a list after we’ve created it, using the same indexing system.\n\nshopping[1] = \"oranges\"  # Replace the second item (at index 1)\nshopping\n\n['apples', 'oranges', 'milk', 'bread']\n\n\nThere are many methods accessible to lists. For example, we can add elements to a list using the append() method.\n\nshopping.append(\"eggs\")\nshopping\n\n['apples', 'oranges', 'milk', 'bread', 'eggs']\n\n\nIn the initial stages of your Python data journey, you may not work with lists too often, so we’ll keep this intro brief.\n\n\n\n\n\n\nPractice\n\n\n\n4.4.1 Practice: Working with Lists\n\nCreate a list called temperatures with these values: 1,2,3,4\nPrint the first element of the list\nChange the last element to 6\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-dictionaries",
    "href": "p_foundations_data_structures.html#python-dictionaries",
    "title": "4  Data Structures in Python",
    "section": "4.5 Python Dictionaries",
    "text": "4.5 Python Dictionaries\nDictionaries are like labeled storage boxes for your data. Each piece of data (value) has a unique label (key). Below, we have a dictionary of grades for some students.\n\ngrades = {\"Alice\": 90, \"Bob\": 85, \"Charlie\": 92}\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92}\n\n\nAs you can see, dictionaries are defined using curly braces {}, with keys and values separated by colons :, and the key-value pairs are separated by commas.\nWe use the key to get the associated value.\n\ngrades[\"Bob\"]\n\n85\n\n\n\n4.5.1 Adding/Modifying Entries\nWe can easily add new information or change existing data in a dictionary.\n\ngrades[\"David\"] = 88  # Add a new student\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\ngrades[\"Alice\"] = 95  # Update Alice's grade\ngrades\n\n{'Alice': 95, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.5.2 Practice: Working with Dictionaries\n\nCreate a dictionary called prices with these pairs: “apple”: 0.50, “banana”: 0.25, “orange”: 0.75\nPrint the price of an orange by using the key\nAdd a new fruit “grape” with a price of 1.5\nChange the price of “banana” to 0.30\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-series",
    "href": "p_foundations_data_structures.html#pandas-series",
    "title": "4  Data Structures in Python",
    "section": "4.6 Pandas Series",
    "text": "4.6 Pandas Series\nPandas provides a data structure called a Series that is similar to a list, but with additional features that are particularly useful for data analysis.\nLet’s create a simple Series:\n\ntemps = pd.Series([1, 2, 3, 4, 5])\ntemps\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\nWe can use built-in Series methods to calculate summary statistics.\n\ntemps.mean()\ntemps.median()\ntemps.std()\n\nnp.float64(1.5811388300841898)\n\n\nAn important feature of Series is that they can have a custom index for intuitive access.\n\ntemps_labeled = pd.Series([1, 2, 3, 4], index=['Mon', 'Tue', 'Wed', 'Thu'])\ntemps_labeled\ntemps_labeled['Wed']\n\nnp.int64(3)\n\n\nThis makes them similar to dictionaries.\n\n\n\n\n\n\nPractice\n\n\n\n4.6.1 Practice: Working with Series\n\nCreate a Series called rainfall with these values: 5, 2, 7, 4, 1\nGet the mean and median rainfall\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-dataframes",
    "href": "p_foundations_data_structures.html#pandas-dataframes",
    "title": "4  Data Structures in Python",
    "section": "4.7 Pandas DataFrames",
    "text": "4.7 Pandas DataFrames\nNext up, let’s consider Pandas DataFrames, which are like Series but in two dimensions - think spreadsheets or database tables.\nThis is the most important data structure for data analysis.\nA DataFrame is like a spreadsheet in Python. It has rows and columns, making it perfect for organizing structured data.\nMost of the time, you will be importing external data frames, but you should know how to data frames from scratch within Python as well.\nLet’s create three lists first:\n\n# Create three lists\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 28]\ncities = [\"Lagos\", \"London\", \"Lima\"]\n\nThen we combined them into a dictionary, and finally into a dataframe.\n\ndata = {'name': names,\n        'age': ages,\n        'city': cities}\n\npeople_df = pd.DataFrame(data)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nNote that we could have created the dataframe without the intermediate series:\n\npeople_df = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [25, 30, 28],\n        \"city\": [\"Lagos\", \"London\", \"Lima\"],\n    }\n)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nWe can select specific columns or rows from our DataFrame.\n\npeople_df[\"city\"]  # Selecting a column. Note that this returns a Series.\npeople_df.loc[0]  # Selecting a row by its label. This also returns a Series.\n\nname    Alice\nage        25\ncity    Lagos\nName: 0, dtype: object\n\n\nWe can call methods on the dataframe.\n\npeople_df.describe() # This is a summary of the numerical columns\npeople_df.info() # This is a summary of the data types\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   name    3 non-null      object\n 1   age     3 non-null      int64 \n 2   city    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n\n\nAnd we can call methods on the Series objects that result from selecting columns.\nFor example, we can get summary statistics on the “city” column.\n\npeople_df[\"city\"].describe()  # This is a summary of the \"city\" column\npeople_df[\"age\"].mean()  # This is the mean of the \"age\" column\n\nnp.float64(27.666666666666668)\n\n\nIn a future series of lessons, we’ll dive deeper into slicing and manipulating DataFrames. Our goal in this lesson is just to get you familiar with the basic syntax and concepts.\n\n\n\n\n\n\nPractice\n\n\n\n4.7.1 Practice: Working with DataFrames\n\nCreate a DataFrame called students with this information:\n\nColumns: “Name”, “Age”, “Grade”\nData:\n\n[“Alice”, 15, “A”]\n[“Bob”, 16, “B”]\n[“Charlie”, 15, “A”]\n\n\nDisplay the entire DataFrame\nShow only the “Grade” column\nDisplay the row for Bob\nCalculate and show the average age of the students\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#wrap-up",
    "href": "p_foundations_data_structures.html#wrap-up",
    "title": "4  Data Structures in Python",
    "section": "4.8 Wrap-up",
    "text": "4.8 Wrap-up\nWe’ve explored the main data structures for Python data analysis. From basic lists and dictionaries to Pandas Series and DataFrames, these tools are essential for organizing and analyzing data. They will be the foundation for more advanced data work in future lessons.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html",
    "href": "p_foundations_for_loops.html",
    "title": "5  Intro to Loops in Python",
    "section": "",
    "text": "5.1 Introduction\nAt the heart of programming is the concept of repeating a task multiple times. A for loop is one fundamental way to do that. Loops enable efficient repetition, saving time and effort.\nMastering this concept is essential for writing intelligent Python code.\nLet’s dive in and enhance your coding skills!/Applications/Python 3.12/Install Certificates.command",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#learning-objectives",
    "href": "p_foundations_for_loops.html#learning-objectives",
    "title": "5  Intro to Loops in Python",
    "section": "5.2 Learning Objectives",
    "text": "5.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nUse basic for loops in Python\nUse index variables to iterate through lists in a loop\nFormat output using f-strings within loops\nApply loops to generate multiple plots for data visualization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#packages",
    "href": "p_foundations_for_loops.html#packages",
    "title": "5  Intro to Loops in Python",
    "section": "5.3 Packages",
    "text": "5.3 Packages\nIn this lesson, we will use the following Python libraries:\n\nimport pandas as pd\nimport plotly.express as px\nfrom vega_datasets import data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#intro-to-for-loops",
    "href": "p_foundations_for_loops.html#intro-to-for-loops",
    "title": "5  Intro to Loops in Python",
    "section": "5.4 Intro to for Loops",
    "text": "5.4 Intro to for Loops\nLet’s start with a simple example. Suppose we have a list of children’s ages in years, and we want to convert these to months:\n\nages = [7, 8, 9]  # List of ages in years\n\nWe could try to directly multiply the list by 12:\n\nages * 12\n\n[7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9]\n\n\nBut this does not do what we want. It repeats the list 12 times.\nRather, we need to loop through each element in the list and multiply it by 12:\n\nfor age in ages:\n    print(age * 12)\n\n84\n96\n108\n\n\nfor and in are required keywords in the loop. The colon and the indentation on the second line are also required.\nIn this loop, age is a temporary variable that takes the value of each element in ages during each iteration. First, age is 7, then 8, then 9.\nYou can choose any name for this variable:\n\nfor random_name in ages:\n    print(random_name * 12)\n\n84\n96\n108\n\n\nNote that we need the print statement since the loop does not automatically print the result:\n\nfor age in ages:\n    age * 12\n\n\n\n\n\n\n\nPractice\n\n\n\n5.4.1 Hours to Minutes Basic Loop\nTry converting hours to minutes using a for loop. Start with this list of hours:\n\nhours = [3, 4, 5]  # List of hours\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#printing-with-f-strings",
    "href": "p_foundations_for_loops.html#printing-with-f-strings",
    "title": "5  Intro to Loops in Python",
    "section": "5.5 Printing with f-strings",
    "text": "5.5 Printing with f-strings\nWe might want to print both the result and the original age. We could do this by concatenating strings with the + operator. But we need to convert the age to a string with str().\n\nfor age in ages:\n    print(str(age) + \" years is \" + str(age * 12) + \" months\" )\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nAlternatively, we can use something called an f-string. This is a string that allows us to embed variables directly.\n\nfor age in ages:\n    print(f\"{age} years is {age * 12} months\")\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nWithin the f-string, we use curly braces {} to embed the variables.\n\n\n\n\n\n\nPractice\n\n\n\n5.5.1 Practice: F-String\nAgain convert the list of hours below to minutes. Use f-strings to print both the original hours and the converted minutes.\n\nhours = [3, 4, 5]  # List of hours\n# Your code here\n# Example output \"3 hours is 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "href": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "title": "5  Intro to Loops in Python",
    "section": "5.6 Are for Loops Useful in Python?",
    "text": "5.6 Are for Loops Useful in Python?\nWhile for loops are useful, in many cases there are more efficient ways to perform operations over collections of data.\nFor example, our initial age conversion could be achieved using pandas Series:\n\nimport pandas as pd\n\nages = pd.Series([7, 8, 9])\nmonths = ages * 12\nprint(months)\n\n0     84\n1     96\n2    108\ndtype: int64\n\n\nBut while libraries like pandas offer powerful ways to work with data, for loops are essential for tasks that can’t be easily vectorized or when you need fine-grained control over the iteration process.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "href": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "title": "5  Intro to Loops in Python",
    "section": "5.7 Looping with an Index and Value",
    "text": "5.7 Looping with an Index and Value\nSometimes, we want to access both the position (index) and the value of items in a list. The enumerate() function helps us do this easily.\nLet’s look at our ages list again:\n\nages = [7, 8, 9]  # List of ages in years\n\nFirst, let’s see what enumerate() actually does:\n\nfor item in enumerate(ages):\n    print(item)\n\n(0, 7)\n(1, 8)\n(2, 9)\n\n\nAs you can see, enumerate() gives us pairs of (index, value).\nWe can unpack these pairs directly in the for loop:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age}\")\n\nThe person at index 0 is aged 7\nThe person at index 1 is aged 8\nThe person at index 2 is aged 9\n\n\nHere, i is the index, and age is the value at that index.\nNow, let’s create a more detailed output using both the index and value:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age} years which is {age * 12} months\")\n\nThe person at index 0 is aged 7 years which is 84 months\nThe person at index 1 is aged 8 years which is 96 months\nThe person at index 2 is aged 9 years which is 108 months\n\n\nThis is particularly useful when you need both the position and the value in your loop.\n\n\n\n\n\n\nPractice\n\n\n\n5.7.1 Practice: Enumerate with F-strings\nUse enumerate() and f-strings to print a sentence for each hour in the list:\n\nhours = [3, 4, 5]  # List of hours\n\n# Your code here\n# Example output: \"Hour 3 at index 0 is equal to 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html",
    "href": "p_foundations_writing_functions.html",
    "title": "6  Intro to Functions and Conditionals",
    "section": "",
    "text": "6.1 Intro\nSo far in this course you have mostly used functions written by others. In this lesson, you will learn how to write your own functions in Python.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#learning-objectives",
    "href": "p_foundations_writing_functions.html#learning-objectives",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.2 Learning Objectives",
    "text": "6.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate and use your own functions in Python.\nDesign function arguments and set default values.\nUse conditional logic like if, elif, and else within functions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#packages",
    "href": "p_foundations_writing_functions.html#packages",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.3 Packages",
    "text": "6.3 Packages\nRun the following code to install and load the packages needed for this lesson:\n\n# Import packages\nimport pandas as pd\nimport numpy as np\nimport vega_datasets as vd",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#basics-of-a-function",
    "href": "p_foundations_writing_functions.html#basics-of-a-function",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.4 Basics of a Function",
    "text": "6.4 Basics of a Function\nLet’s start by creating a very simple function. Consider the following function that converts pounds (a unit of weight) to kilograms (another unit of weight):\n\ndef pounds_to_kg(pounds):\n    return pounds * 0.4536\n\nIf you execute this code, you will create a function named pounds_to_kg, which can be used directly in a script or in the console:\n\nprint(pounds_to_kg(150))\n\n68.04\n\n\nLet’s break down the structure of this first function step by step.\nFirst, a function is created using the def keyword, followed by a pair of parentheses and a colon.\n\ndef function_name():\n    # Function body\n\nInside the parentheses, we indicate the arguments of the function. Our function only takes one argument, which we have decided to name pounds. This is the value that we want to convert from pounds to kilograms.\n\ndef pounds_to_kg(pounds):\n    # Function body\n\nOf course, we could have named this argument anything we wanted. E.g. p or weight.\nThe next element, after the colon, is the body of the function. This is where we write the code that we want to execute when the function is called.\n\ndef pounds_to_kg(pounds):\n    return pounds * 0.4536\n\nWe use the return statement to specify what value the function should output.\nYou could also assign the result to a variable and then return that variable:\n\ndef pounds_to_kg(pounds):\n    kg = pounds * 0.4536\n    return kg\n\nThis is a bit more wordy, but it makes the function clearer.\nWe can now use our function like this with a named argument:\n\npounds_to_kg(pounds=150)\n\n68.04\n\n\nOr without a named argument:\n\npounds_to_kg(150)\n\n68.04\n\n\nTo use this in a DataFrame, you can create a new column:\n\npounds_df = pd.DataFrame({'pounds': [150, 200, 250]})\npounds_df['kg'] = pounds_to_kg(pounds_df['pounds'])\npounds_df\n\n\n\n\n\n\n\n\npounds\nkg\n\n\n\n\n0\n150\n68.04\n\n\n1\n200\n90.72\n\n\n2\n250\n113.40\n\n\n\n\n\n\n\nAnd that’s it! You have just created and usedyour first function in Python.\n\n\n\n\n\n\nPractice\n\n\n\n6.4.1 Age in Months Function\nCreate a simple function called years_to_months that transforms age in years to age in months.\nUse it on the riots_df DataFrame imported below to create a new column called age_months:\n\nriots_df = vd.data.la_riots()\nriots_df \n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n60\nElbert O.\nWilkins\n33.0\nMale\nBlack\n1992-04-30\nWestern Avenue & 92nd Street\nGramercy Park\nHomicide\n-118.310004\n33.952767\n\n\n61\nJohn H.\nWillers\n37.0\nMale\nWhite\n1992-04-29\n10621 Sepulveda Blvd.\nMission Hills\nHomicide\n-118.467770\n34.263184\n\n\n62\nWillie Bernard\nWilliams\n29.0\nMale\nBlack\n1992-04-29\nGage & Western avenues\nChesterfield Square\nDeath\n-118.308952\n33.982363\n\n\n\n\n63 rows × 11 columns",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#functions-with-multiple-arguments",
    "href": "p_foundations_writing_functions.html#functions-with-multiple-arguments",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.5 Functions with Multiple Arguments",
    "text": "6.5 Functions with Multiple Arguments\nMost functions take multiple arguments rather than just one. Let’s look at an example of a function that takes three arguments:\n\ndef calc_calories(carb_grams, protein_grams, fat_grams):\n    result = (carb_grams * 4) + (protein_grams * 4) + (fat_grams * 9)\n    return result\n\ncalc_calories(carb_grams=50, protein_grams=25, fat_grams=10)\n\n390\n\n\nThe calc_calories function computes the total calories based on the grams of carbohydrates, protein, and fat. Carbohydrates and proteins are estimated to be 4 calories per gram, while fat is estimated to be 9 calories per gram.\nIf you attempt to use the function without supplying all the arguments, it will yield an error.\n\ncalc_calories(carb_grams=50, protein_grams=25)\n\nTypeError: calc_calories() missing 1 required positional argument: 'fat_grams'\nYou can define default values for your function’s arguments. If an argument is called without a value assigned to it, then this argument assumes its default value. Let’s make all arguments optional by giving them all default values:\n\ndef calc_calories(carb_grams=0, protein_grams=0, fat_grams=0):\n    result = (carb_grams * 4) + (protein_grams * 4) + (fat_grams * 9)\n    return result\n\nNow, we can call the function with only some arguments without getting an error:\n\ncalc_calories(carb_grams=50, protein_grams=25)\n\n300\n\n\nLet’s use this on a sample dataset:\n\nfood_df = pd.DataFrame({\n    'food': ['Apple', 'Avocado'],\n    'carb_grams': [25, 10],\n    'protein_grams': [0, 1],\n    'fat_grams': [0, 14]\n})\nfood_df['calories'] = calc_calories(food_df['carb_grams'], food_df['protein_grams'], food_df['fat_grams'])\nfood_df\n\n\n\n\n\n\n\n\nfood\ncarb_grams\nprotein_grams\nfat_grams\ncalories\n\n\n\n\n0\nApple\n25\n0\n0\n100\n\n\n1\nAvocado\n10\n1\n14\n170\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n6.5.1 BMI Function\nCreate a function named calc_bmi that calculates the Body Mass Index (BMI) for one or more individuals, then apply the function by running the code chunk further below. The formula for BMI is weight (kg) divided by height (m) squared.\n\n# Your code here\n\n\nbmi_df = pd.DataFrame({\n    'Weight': [70, 80, 100],  # in kg\n    'Height': [1.7, 1.8, 1.2]  # in meters\n})\nbmi_df['BMI'] = calc_bmi(bmi_df['Weight'], bmi_df['Height'])\nbmi_df",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#intro-to-conditionals-if-elif-and-else",
    "href": "p_foundations_writing_functions.html#intro-to-conditionals-if-elif-and-else",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.6 Intro to Conditionals: if, elif, and else",
    "text": "6.6 Intro to Conditionals: if, elif, and else\nConditional statements allow you to execute code only when certain conditions are met. The basic syntax in Python is:\n\nif condition:\n    # Code to execute if condition is True\nelif another_condition:\n    # Code to execute if the previous condition was False and this condition is True\nelse:\n    # Code to execute if all previous conditions were False\n\nLet’s look at an example of using conditionals within a function. Suppose we want to write a function that classifies a number as positive, negative, or zero.\n\ndef class_num(num):\n    if num &gt; 0:\n        return \"Positive\"\n    elif num &lt; 0:\n        return \"Negative\"\n    else:\n        return \"Zero\"\n\nprint(class_num(10))    # Output: Positive\nprint(class_num(-5))    # Output: Negative\nprint(class_num(0))     # Output: Zero\n\nPositive\nNegative\nZero\n\n\nIf you try to use this function the way we have done above for, for example the BMI function, you will get an error:\n\nnum_df = pd.DataFrame({'num': [10, -5, 0]})\nnum_df\n\n\n\n\n\n\n\n\nnum\n\n\n\n\n0\n10\n\n\n1\n-5\n\n\n2\n0\n\n\n\n\n\n\n\n\nnum_df['category'] = class_num(num_df['num'])\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nThe reason for this is that if statements are not built to work with series (they are not inherently vectorized); but rather work with single values. To get around this, we can use the np.vectorize function to create a vectorized version of the function:\n\nclass_num_vec = np.vectorize(class_num)\nnum_df['category'] = class_num_vec(num_df['num'])\nnum_df\n\n\n\n\n\n\n\n\nnum\ncategory\n\n\n\n\n0\n10\nPositive\n\n\n1\n-5\nNegative\n\n\n2\n0\nZero\n\n\n\n\n\n\n\nTo get more practice with conditionals, let’s write a function that categorizes grades into simple categories:\n\nIf the grade is 85 or above, the category is ‘Excellent’.\nIf the grade is between 60 and 84, the category is ‘Pass’.\nIf the grade is below 60, the category is ‘Fail’.\nIf the grade is negative or invalid, return ‘Invalid grade’.\n\n\ndef categorize_grade(grade):\n    if grade &gt;= 85 and grade &lt;= 100:\n        return 'Excellent'\n    elif grade &gt;= 60 and grade &lt; 85:\n        return 'Pass'\n    elif grade &gt;= 0 and grade &lt; 60:\n        return 'Fail'\n    else:\n        return 'Invalid grade'\n\ncategorize_grade(95)  # Output: Excellent\n\n'Excellent'\n\n\nWe can apply this function to a column in a DataFrame but first we need to vectorize it:\n\ncategorize_grade = np.vectorize(categorize_grade)\n\n\ngrades_df = pd.DataFrame({'grade': [95, 82, 76, 65, 58, -5]})\ngrades_df['grade_cat'] = categorize_grade(grades_df['grade'])\ngrades_df\n\n\n\n\n\n\n\n\ngrade\ngrade_cat\n\n\n\n\n0\n95\nExcellent\n\n\n1\n82\nPass\n\n\n2\n76\nPass\n\n\n3\n65\nPass\n\n\n4\n58\nFail\n\n\n5\n-5\nInvalid grade\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n6.6.1 Age Categorization Function\nNow, try writing a function that categorizes age into different life stages as described earlier. You should use the following criteria:\n\nIf the age is under 18, the category is ‘Minor’.\nIf the age is greater than or equal to 18 and less than 65, the category is ‘Adult’.\nIf the age is greater than or equal to 65, the category is ‘Senior’.\nIf the age is negative or invalid, return ‘Invalid age’.\n\nUse it on the riots_df DataFrame printed below to create a new column called Age_Category.\n\n# Your code here\n\nriots_df = vd.data.la_riots()\nriots_df\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n60\nElbert O.\nWilkins\n33.0\nMale\nBlack\n1992-04-30\nWestern Avenue & 92nd Street\nGramercy Park\nHomicide\n-118.310004\n33.952767\n\n\n61\nJohn H.\nWillers\n37.0\nMale\nWhite\n1992-04-29\n10621 Sepulveda Blvd.\nMission Hills\nHomicide\n-118.467770\n34.263184\n\n\n62\nWillie Bernard\nWilliams\n29.0\nMale\nBlack\n1992-04-29\nGage & Western avenues\nChesterfield Square\nDeath\n-118.308952\n33.982363\n\n\n\n\n63 rows × 11 columns\n\n\n\n\n\n\n\n\n\n\n\nSide Note\n\n\n\n6.6.2 Apply vs Vectorize\nAnother way to use functions with if statements on a dataframe is to use the apply method. Here is how you can do the grade categorization function with apply:\n\ngrades_df['grade_cat'] = grades_df['grade'].apply(categorize_grade)\ngrades_df\n\n\n\n\n\n\n\n\ngrade\ngrade_cat\n\n\n\n\n0\n95\nExcellent\n\n\n1\n82\nPass\n\n\n2\n76\nPass\n\n\n3\n65\nPass\n\n\n4\n58\nFail\n\n\n5\n-5\nInvalid grade\n\n\n\n\n\n\n\nThe vectorize method is easier to use with multiple arguments, but you will encounter the apply further down the road.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_foundations_writing_functions.html#conclusion",
    "href": "p_foundations_writing_functions.html#conclusion",
    "title": "6  Intro to Functions and Conditionals",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nIn this lesson, we’ve introduced the basics of writing functions in Python and how to use conditional statements within those functions. Functions are essential building blocks in programming that allow you to encapsulate code for reuse and better organization. Conditional statements enable your functions to make decisions based on input values or other conditions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to Functions and Conditionals</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html",
    "href": "p_data_on_display_data_viz_types.html",
    "title": "7  Data Visualization Types",
    "section": "",
    "text": "8 Slides",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric",
    "href": "p_data_on_display_data_viz_types.html#numeric",
    "title": "7  Data Visualization Types",
    "section": "10.1 Numeric",
    "text": "10.1 Numeric\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\n\npx.box(tips, x='tip')\n\n                                                \n\n\n\npx.violin(tips, x='tip', box=True, points=\"all\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "href": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "title": "7  Data Visualization Types",
    "section": "12.1 Numeric vs Numeric",
    "text": "12.1 Numeric vs Numeric\n\npx.scatter(tips, x='total_bill', y='tip')",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html",
    "href": "p_data_on_display_univariate.html",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "",
    "text": "8.1 Intro\nIn this lesson, you’ll learn how to create univariate graphs using Plotly Express. Univariate graphs are essential for understanding the distribution of a single variable, whether it’s categorical or quantitative.\nLet’s get started!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#learning-objectives",
    "href": "p_data_on_display_univariate.html#learning-objectives",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "8.2 Learning objectives",
    "text": "8.2 Learning objectives\n\nCreate bar charts, pie charts, and treemaps for categorical data using Plotly Express\nGenerate histograms for quantitative data using Plotly Express\nCustomize graph appearance and labels",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#imports",
    "href": "p_data_on_display_univariate.html#imports",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "8.3 Imports",
    "text": "8.3 Imports\nThis lesson requires plotly.express, pandas, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#quantitative-data",
    "href": "p_data_on_display_univariate.html#quantitative-data",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "8.4 Quantitative Data",
    "text": "8.4 Quantitative Data\n\n8.4.1 Histogram\nHistograms are used to visualize the distribution of continuous variables.\nLet’s make a histogram of the tip amounts in the tips dataset.\n\ntips = px.data.tips()\ntips.head() # view the first 5 rows\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nWe can see that the highest bar, corresponding to tips between 1.75 and 2.24, has a frequency of 55. This means that there were 55 tips between 1.75 and 2.24.\n\n\n\n\n\n\nSide-note\n\n\n\nNotice that plotly charts are interactive. You can hover over the bars to see the exact number of tips in each bin.\nTry playing with the buttons at the top right. The button to download the chart as a png is especially useful.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.4.2 Practice: Speed Distribution Histogram\nFollowing the example of the histogram of tips, create a histogram of the speed distribution (Speed_IAS_in_knots) using the birdstrikes dataset.\n\nbirdstrikes = data.birdstrikes()\nbirdstrikes.head()\n# Your code here\n\n\n\n\n\n\n\n\nAirport__Name\nAircraft__Make_Model\nEffect__Amount_of_damage\nFlight_Date\nAircraft__Airline_Operator\nOrigin_State\nWhen__Phase_of_flight\nWildlife__Size\nWildlife__Species\nWhen__Time_of_day\nCost__Other\nCost__Repair\nCost__Total_$\nSpeed_IAS_in_knots\n\n\n\n\n0\nBARKSDALE AIR FORCE BASE ARPT\nT-38A\nNone\n1/8/90 0:00\nMILITARY\nLouisiana\nClimb\nLarge\nTurkey vulture\nDay\n0\n0\n0\n300.0\n\n\n1\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/9/90 0:00\nMILITARY\nLouisiana\nApproach\nMedium\nUnknown bird or bat\nNight\n0\n0\n0\n200.0\n\n\n2\nBARKSDALE AIR FORCE BASE ARPT\nB-52\nNone\n1/11/90 0:00\nMILITARY\nLouisiana\nTake-off run\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n130.0\n\n\n3\nNEW ORLEANS INTL\nB-737-300\nSubstantial\n1/11/90 0:00\nSOUTHWEST AIRLINES\nLouisiana\nTake-off run\nSmall\nRock pigeon\nDay\n0\n0\n0\n140.0\n\n\n4\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/12/90 0:00\nMILITARY\nLouisiana\nClimb\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n160.0\n\n\n\n\n\n\n\n\n\nWe can view the help documentation for the function by typing px.histogram? in a cell and running it.\n\npx.histogram?\n\nFrom the help documentation, we can see that the px.histogram function has many arguments that we can use to customize the graph.\nLet’s make the histogram a bit nicer by adding a title, customizing the x axis label, and changing the color.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"lightseagreen\"]\n)\n\n                                                \n\n\nColor names are based on standard CSS color naming from Mozilla. You can see the full list here.\nAlternatively, you can use hex color codes, like #1f77b4. You can get these easily by using a color picker. Search for “color picker” on Google.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"#6a5acd\"]\n)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.4.3 Practice: Bird Strikes Histogram Custom\nUpdate your birdstrikes histogram to use a hex code color, add a title, and change the x-axis label to “Speed (Nautical Miles Per Hour)”.\n\n# Your code here\n\n\n\n\n\n8.4.4 Counts on bars\nWe can add counts to the bars with the text_auto argument.\n\npx.histogram(tips, x='tip', text_auto= True)\n\n                                                \n\n\n\n8.4.4.1 Bins and bandwidths\nWe can adjust the number of bins or bin width to better represent the data using the nbins argument. Let’s make a histogram with just 10 bins:\n\npx.histogram(tips, x='tip', nbins=10)\n\n                                                \n\n\nNow we have broader tip amount groups.\n\n\n\n\n\n\nPractice\n\n\n\n8.4.5 Practice: Speed Distribution Histogram Custom\nCreate a histogram of the speed distribution (Speed_IAS_in_knots) with 15 bins using the birdstrikes dataset. Add counts to the bars, use a color of your choice, and add an appropriate title.\n\n# Your code here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#categorical-data",
    "href": "p_data_on_display_univariate.html#categorical-data",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "8.5 Categorical Data",
    "text": "8.5 Categorical Data\n\n8.5.1 Bar chart\nBar charts can be used to display the frequency of a single categorical variable.\nPlotly has a px.bar function that we will see later. But for single categorical variables, the function plotly wants you to use is actually px.histogram. (Statisticians everywhere are crying; histograms are supposed to be used for just quantitative data!)\nLet’s create a basic bar chart showing the distribution of sex in the tips dataset:\n\npx.histogram(tips, x='sex')   \n\n                                                \n\n\nLet’s add counts to the bars.\n\npx.histogram(tips, x='sex', text_auto= True)\n\n                                                \n\n\nWe can enhance the chart by adding a color axis, and customizing the labels and title.\n\npx.histogram(tips, x='sex', text_auto=True, color='sex', \n             labels={'sex': 'Gender'},\n             title='Distribution of Customers by Gender')\n\n                                                \n\n\nArguably, in this plot, we do not need the color axis, since the sex variable is already represented by the x axis. But public audiences like colors, so it may still be worth including.\nHowever, we should remove the legend. Let’s also use custom colors.\nFor this, we can first create a figure object, then use the .layout.update method from that object to update the legend.\n\ntips_by_sex = px.histogram(\n    tips,\n    x=\"sex\",\n    text_auto=True,\n    color=\"sex\",\n    labels={\"sex\": \"Gender\"},\n    title=\"Distribution of Customers by Gender\",\n    color_discrete_sequence=[\"#1f77b4\", \"#ff7f0e\"],\n)\n\ntips_by_sex.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.5.2 Practice: Bird Strikes by Phase of Flight\nCreate a bar chart showing the frequency of bird strikes by the phase of flight, When__Phase_of_flight. Add appropriate labels and a title. Use colors of your choice, and remove the legend.\n\n# Your code here\n\n\n\n\n8.5.2.1 Sorting categories\nIt is sometimes useful to dictate a specific order for the categories in a bar chart.\nConsider this bar chart of the election winners by district in the 2013 Montreal mayoral election.\n\nelection = px.data.election()\nelection.head()\n\n\n\n\n\n\n\n\ndistrict\nCoderre\nBergeron\nJoly\ntotal\nwinner\nresult\ndistrict_id\n\n\n\n\n0\n101-Bois-de-Liesse\n2481\n1829\n3024\n7334\nJoly\nplurality\n101\n\n\n1\n102-Cap-Saint-Jacques\n2525\n1163\n2675\n6363\nJoly\nplurality\n102\n\n\n2\n11-Sault-au-Récollet\n3348\n2770\n2532\n8650\nCoderre\nplurality\n11\n\n\n3\n111-Mile-End\n1734\n4782\n2514\n9030\nBergeron\nmajority\n111\n\n\n4\n112-DeLorimier\n1770\n5933\n3044\n10747\nBergeron\nmajority\n112\n\n\n\n\n\n\n\n\npx.histogram(election, x='winner')\n\n                                                \n\n\nLet’s define a custom order for the categories. “Bergeron” will be first, then “Joly” then “Coderre”.\n\ncustom_order = [\"Bergeron\", \"Joly\", \"Coderre\"]\nelection_chart = px.histogram(election, x='winner', category_orders={'winner': custom_order})\nelection_chart\n\n                                                \n\n\nWe can also sort the categories by frequency.\nWe can sort the categories by frequency using the categoryorder attribute of the x axis.\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total descending\")\n\n                                                \n\n\nOr in ascending order:\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total ascending\")\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.5.3 Practice: Sorted Origin State Bar Chart\nCreate a sorted bar chart showing the distribution of bird strikes by origin state. Sort the bars in ascending order of frequency.\n\n# Your code here\n\n\n\n\n\n\n8.5.4 Horizontal bar chart\nWhen you have many categories, horizontal bar charts are often easier to read than vertical bar charts. To make a horizontal bar chart, simply use the y axis instead of the x axis.\n\npx.histogram(tips, y='day')\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.5.5 Practice: Horizontal Bar Chart of Origin State\nCreate a horizontal bar chart showing the distribution of bird strikes by origin state.\n\n# Your code here\n\n\n\n\n\n8.5.6 Pie chart\nPie charts are also useful for showing the proportion of categorical variables. They are best used when you have a small number of categories. For larger numbers of categories, pie charts are hard to read.\nLet’s make a pie chart of the distribution of tips by day of the week.\n\npx.pie(tips, names=\"day\")\n\n                                                \n\n\nWe can add labels to the pie chart to make it easier to read.\n\ntips_by_day = px.pie(tips, names=\"day\")\ntips_by_day_with_labels = tips_by_day.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\ntips_by_day_with_labels\n\n                                                \n\n\nThe legend is no longer needed, so we can remove it.\n\ntips_by_day_with_labels.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPro\n\n\n\nIf you forget how to make simple changes like this, don’t hesitate to consult the plotly documentation, Google or ChatGPT.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.5.7 Practice: Wildlife Size Pie Chart\nCreate a pie chart showing the distribution of bird strikes by wildlife size. Include percentages and labels inside the pie slices.\n\n# Your code here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#summary",
    "href": "p_data_on_display_univariate.html#summary",
    "title": "8  Univariate Graphs with Plotly Express",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this lesson, you learned how to create univariate graphs using Plotly Express. You should now feel confident in your ability to create bar charts, pie charts, and histograms. You should also feel comfortable customizing the appearance of your graphs.\nSee you in the next lesson.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html",
    "href": "p_data_on_display_multivariate.html",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "",
    "text": "9.1 Introduction\nIn this lesson, you’ll learn how to create bivariate and multivariate graphs using Plotly Express. These types of graphs are essential for exploring relationships between two or more variables, whether they are quantitative or categorical. Understanding these relationships can provide deeper insights into your data.\nLet’s dive in!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#learning-objectives",
    "href": "p_data_on_display_multivariate.html#learning-objectives",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.2 Learning Objectives",
    "text": "9.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate scatter plots for quantitative vs. quantitative data\nGenerate grouped histograms and violin plots for quantitative vs. categorical data\nCreate grouped, stacked, and percent-stacked bar charts for categorical vs. categorical data\nVisualize time series data using bar charts and line charts\nCreate bubble charts to display relationships between three or more variables\nUse faceting to compare distributions across subsets of data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#imports",
    "href": "p_data_on_display_multivariate.html#imports",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.3 Imports",
    "text": "9.3 Imports\nThis lesson requires plotly.express, pandas, numpy, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.4 Numeric vs. Numeric Data",
    "text": "9.4 Numeric vs. Numeric Data\nWhen both variables are quantitative, scatter plots are an excellent way to visualize their relationship.\n\n9.4.1 Scatter Plot\nLet’s create a scatter plot to examine the relationship between total_bill and tip in the tips dataset. The tips dataset is included in Plotly Express and contains information about restaurant bills and tips that were collected by a waiter in a US restaurant.\nFirst, we’ll load the dataset and view the first five rows:\n\ntips = px.data.tips()\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nNext, we’ll create a basic scatter plot. We do this with the px.scatter function.\n\npx.scatter(tips, x='total_bill', y='tip')\n\n                                                \n\n\nFrom the scatter plot, we can observe that as the total bill increases, the tip amount tends to increase as well.\nLet’s enhance the scatter plot by adding labels and a title.\n\npx.scatter(\n    tips,\n    x=\"total_bill\",\n    y=\"tip\",\n    labels={\"total_bill\": \"Total Bill ($)\", \"tip\": \"Tip ($)\"},\n    title=\"Relationship Between Total Bill and Tip Amount\",\n)\n\n                                                \n\n\nRecall that you can see additional information about the function by typing px.scatter? in a cell and executing the cell.\n\npx.scatter?\n\n\n\n\n\n\n\nPractice\n\n\n\n9.4.2 Practice: Life Expectancy vs. GDP Per Capita\nUsing the Gapminder dataset (the 2007 subset, g_2007, defined below), create a scatter plot showing the relationship between gdpPercap (GDP per capita) and lifeExp (life expectancy).\nAccording to the plot, what is the relationship between GDP per capita and life expectancy?\n\ngapminder = px.data.gapminder()\ng_2007 = gapminder.query('year == 2007')\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.5 Numeric vs. Categorical Data",
    "text": "9.5 Numeric vs. Categorical Data\nWhen one variable is quantitative and the other is categorical, we can use grouped histograms, violin plots, or box plots to visualize the distribution of the quantitative variable across different categories.\n\n9.5.1 Grouped Histograms\nFirst, here’s how you can create a regular histogram of all tips:\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nTo create a grouped histogram, use the color parameter to specify the categorical variable. Here, we’ll color the histogram by sex:\n\npx.histogram(tips, x='tip', color='sex')\n\n                                                \n\n\nBy default, the histograms for each category are stacked. To change this behavior, you can use the barmode parameter. For example, barmode='overlay' will create an overlaid histogram:\n\npx.histogram(tips, x=\"tip\", color=\"sex\", barmode=\"overlay\")\n\n                                                \n\n\nThis creates two semi-transparent histograms overlaid on top of each other, allowing for direct comparison of the distributions.\n\n\n\n\n\n\nPractice\n\n\n\n9.5.2 Practice: Age Distribution by Gender\nUsing the la_riots dataset from vega_datasets, create a grouped histogram of age by gender. Compare the age distributions between different genders.\nAccording to the plot, was the oldest victim male or female?\n\nla_riots = data.la_riots()\nla_riots.head()\n# Your code here\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n3\nBrian E.\nAndrew\n30.0\nMale\nBlack\n1992-04-30\nRosecrans & Chester avenues\nCompton\nOfficer-involved shooting\n-118.215390\n33.903457\n\n\n4\nVivian\nAustin\n87.0\nFemale\nBlack\n1992-05-03\n1600 W. 60th St.\nHarvard Park\nDeath\n-118.304741\n33.985667\n\n\n\n\n\n\n\n\n\n\n\n9.5.3 Violin & Box Plots\nViolin plots are useful for comparing the distribution of a quantitative variable across different categories. They show the probability density of the data at different values and can include a box plot to summarize key statistics.\nFirst, let’s create a violin plot of all tips:\n\npx.violin(tips, y=\"tip\")\n\n                                                \n\n\nWe can add a box plot to the violin plot by setting the box parameter to True:\n\npx.violin(tips, y=\"tip\", box=True)\n\n                                                \n\n\nFor just the box plot, we can use px.box:\n\npx.box(tips, y=\"tip\")\n\n                                                \n\n\nTo add jitter points to the violin or box plots, we can use the points = 'all' parameter.\n\npx.violin(tips, y=\"tip\", points=\"all\")\n\n                                                \n\n\nNow, to create a violin plot of tips by gender, use the x parameter to specify the categorical variable:\n\npx.violin(tips, y=\"tip\", x=\"sex\", box=True)\n\n                                                \n\n\nWe can also add a color axis to differentiate the violins:\n\npx.violin(tips, y=\"tip\", x=\"sex\", color=\"sex\", box=True)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n9.5.4 Practice: Life Expectancy by Continent\nUsing the g_2007 dataset, create a violin plot showing the distribution of lifeExp by continent.\nAccording to the plot, which continent has the highest median country life expectancy?\n\ng_2007 = gapminder.query(\"year == 2007\")\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32\n\n\n\n\n\n\n\n\n\n\n\n9.5.5 Summary Bar Charts (Mean and Standard Deviation)\nSometimes it’s useful to display the mean and standard deviation of a quantitative variable across different categories. This can be visualized using a bar chart with error bars.\nFirst, let’s calculate the mean and standard deviation of tips for each gender. You have not yet learned how to do this, but you will in a later lesson.\n\n# Calculate the mean and standard deviation\nsummary_df = (\n    tips.groupby(\"sex\")\n    .agg(mean_tip=(\"tip\", \"mean\"), std_tip=(\"tip\", \"std\"))\n    .reset_index()\n)\nsummary_df\n\n\n\n\n\n\n\n\nsex\nmean_tip\nstd_tip\n\n\n\n\n0\nFemale\n2.833448\n1.159495\n\n\n1\nMale\n3.089618\n1.489102\n\n\n\n\n\n\n\nNext, we’ll create a bar chart using px.bar and add error bars using the error_y parameter:\n\n# Create the bar chart\npx.bar(summary_df, x=\"sex\", y=\"mean_tip\", error_y=\"std_tip\")\n\n                                                \n\n\nThis bar chart displays the average tip amount for each gender, with error bars representing the standard deviation.\n\n\n\n\n\n\nPractice\n\n\n\n9.5.6 Practice: Average Total Bill by Day\nUsing the tips dataset, create a bar chart of mean total_bill by day with standard deviation error bars. You should copy and paste the code from the example above and modify it to create this plot.\nAccording to the plot, which day has the highest average total bill?\n\ntips.head()  # View the tips dataset\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Note: Difference between px.bar and px.histogram\n\n\n\nNotice that this is the first time we are using the px.bar function. For past plots, we have used px.histogram to make bar charts.\nThe bar chart function generally expects that the numeric variable being plotted is already in it’s own column, while the histogram function does the grouping for you.\nFor example, in the cell below, we use px.histogram to make a bar chart of the sex column. The resulting plot compares the number of male and female customers in the dataset.\n\npx.histogram(tips, x='sex')\n\n                                                \n\n\nTo make the same plot using px.bar, we first need to group by the sex column and count the number of rows for each sex.\n\nsex_counts = tips['sex'].value_counts().reset_index()\nsex_counts\n\n\n\n\n\n\n\n\nsex\ncount\n\n\n\n\n0\nMale\n157\n\n\n1\nFemale\n87\n\n\n\n\n\n\n\nWe can then plot the day column using px.bar:\n\npx.bar(sex_counts, x=\"sex\", y=\"count\")\n\n                                                \n\n\nThis produces a bar chart with one bar for each sex.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.6 Categorical vs. Categorical Data",
    "text": "9.6 Categorical vs. Categorical Data\nWhen both variables are categorical, bar charts with a color axis are effective for visualizing the frequency distribution across categories. We will focus on three types of bar charts: stacked bar charts, percent-stacked bar charts, and grouped/clustered bar charts.\n\n9.6.1 Stacked Bar Charts\nStacked bar charts show the total counts and the breakdown within each category. To make a stacked bar chart, use the color parameter to specify the categorical variable:\n\npx.histogram(\n    tips,\n    x='day',\n    color='sex'\n)\n\n                                                \n\n\nLet’s add numbers to the bars to show the exact counts, and also improve the color palette with custom colors.\n\npx.histogram(\n    tips,\n    x=\"day\",\n    color=\"sex\",\n    text_auto=True,\n    color_discrete_sequence=[\"#deb221\", \"#2f828a\"],\n)\n\n                                                \n\n\nThis stacked bar chart shows the total number of customers each day, broken down by gender.\n\n\n\n\n\n\nPractice\n\n\n\n9.6.2 Practice: High and Low Income Countries by Continent\nUsing the g_2007_income dataset, create a stacked bar chart showing the count of high and low income countries in each continent.\n\ngap_dat = px.data.gapminder()\n\ng_2007_income = (\n    gap_dat.query(\"year == 2007\")\n    .drop(columns=[\"year\", \"iso_alpha\", \"iso_num\"])\n    .assign(\n        income_group=lambda df: np.where(\n            df.gdpPercap &gt; 15000, \"High Income\", \"Low & Middle Income\"\n        )\n    )\n)\n\ng_2007_income.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nlifeExp\npop\ngdpPercap\nincome_group\n\n\n\n\n11\nAfghanistan\nAsia\n43.828\n31889923\n974.580338\nLow & Middle Income\n\n\n23\nAlbania\nEurope\n76.423\n3600523\n5937.029526\nLow & Middle Income\n\n\n35\nAlgeria\nAfrica\n72.301\n33333216\n6223.367465\nLow & Middle Income\n\n\n47\nAngola\nAfrica\n42.731\n12420476\n4797.231267\nLow & Middle Income\n\n\n59\nArgentina\nAmericas\n75.320\n40301927\n12779.379640\nLow & Middle Income\n\n\n\n\n\n\n\n\n\n\n\n9.6.3 Percent-Stacked Bar Charts\nTo show proportions instead of counts, we can create percent-stacked bar charts by setting the barnorm parameter to 'percent':\n\n# Create the percent-stacked bar chart\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\")\n\n                                                \n\n\nThis chart normalizes the bar heights to represent percentages, showing the proportion of each gender for each day.\nWe can also add text labels to the bars to show the exact percentages:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\", text_auto=\".1f\")\n\n                                                \n\n\nThe symbol .1f in the text_auto parameter formats the text labels to one decimal place.\n\n\n\n\n\n\nPractice\n\n\n\n9.6.4 Practice: Proportion of High and Low Income Countries by Continent\nAgain using the g_2007_income dataset, create a percent-stacked bar chart showing the proportion of high and low income countries in each continent. Add text labels to the bars to show the exact percentages.\nAccording the plot, which continent has the highest proportion of high income countries? Are there any limitations to this plot?\n\n# Your code here\n\n\n\n\n\n9.6.5 Clustered Bar Charts\nFor clustered bar charts, set the barmode parameter to 'group' to place the bars for each category side by side:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barmode=\"group\")\n\n                                                \n\n\nThis layout makes it easier to compare values across categories directly.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#time-series-data",
    "href": "p_data_on_display_multivariate.html#time-series-data",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.7 Time Series Data",
    "text": "9.7 Time Series Data\nTime series data represents observations collected at different points in time. It’s crucial for analyzing trends, patterns, and changes over time. Let’s explore some basic time series visualizations using Nigeria’s population data from the Gapminder dataset.\nFirst, let’s prepare our data:\n\n# Load the Gapminder dataset\ngapminder = px.data.gapminder()\n\n# Subset the data for Nigeria\nnigeria_pop = gapminder.query('country == \"Nigeria\"')[['year', 'pop']]\nnigeria_pop\n\n\n\n\n\n\n\n\nyear\npop\n\n\n\n\n1128\n1952\n33119096\n\n\n1129\n1957\n37173340\n\n\n1130\n1962\n41871351\n\n\n1131\n1967\n47287752\n\n\n1132\n1972\n53740085\n\n\n1133\n1977\n62209173\n\n\n1134\n1982\n73039376\n\n\n1135\n1987\n81551520\n\n\n1136\n1992\n93364244\n\n\n1137\n1997\n106207839\n\n\n1138\n2002\n119901274\n\n\n1139\n2007\n135031164\n\n\n\n\n\n\n\n\n9.7.1 Bar Chart\nA bar chart can be used to plot time series data.\n\n# Bar chart\npx.bar(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThis bar chart gives us a clear view of how Nigeria’s population has changed over the years, with each bar representing the population at a specific year.\n\n\n9.7.2 Line Chart\nA line chart is excellent for showing continuous changes over time:\n\n# Line chart\npx.line(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThe line chart connects the population values, making it easier to see the overall trend of population growth.\nAdding markers to a line chart can highlight specific data points:\n\n# Line chart with points\npx.line(nigeria_pop, x='year', y='pop', markers=True)\n\n                                                \n\n\nWe can also compare the population growth of multiple countries by adding a color parameter:\n\nnigeria_ghana = gapminder.query('country in [\"Nigeria\", \"Ghana\"]')\npx.line(nigeria_ghana, x=\"year\", y=\"pop\", color=\"country\", markers=True)\n\n                                                \n\n\nThis chart allows us to compare the population trends of Nigeria and Ghana over time.\n\n\n\n\n\n\nPractice\n\n\n\n9.7.3 Practice: GDP per Capita Time Series\nUsing the Gapminder dataset, create a time series visualization for the GDP per capita of Iraq.\n\n# Your code here\n\nWhat happened to Iraq in the 1980s that might explain the graph shown?",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "href": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.8 Plots with three or more variables",
    "text": "9.8 Plots with three or more variables\nAlthough bivariate visualizations are the most common types of visualizations, plots with three or more variables are also sometimes useful. Let’s explore a few examples.\n\n9.8.1 Bubble Charts\nBubble charts show the relationship between three variables by mapping the size of the points to a third variable. Below, we plot the relationship between gdpPercap and lifeExp with the size of the points representing the population of the country.\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\")\n\n                                                \n\n\nWe can easily spot the largest countries by population, such as China, India, and the United States. We can also add a color axis to differentiate between continents:\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", color=\"continent\")\n\n                                                \n\n\nNow we have four different variables being plotted:\n\ngdpPercap on the x-axis\nlifeExp on the y-axis\npop as the size of the points\ncontinent as the color of the points\n\n\n\n\n\n\n\nPractice\n\n\n\n9.8.2 Practice: Tips Bubble Chart\nUsing the tips dataset, create a bubble chart showing the relationship between total_bill and tip with the size of the points representing the size of the party, and the color representing the day of the week.\nUse the plot to answer the question:\n\nThe highest two tip amounts were on which days and what was the table size?\n\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n9.8.3 Facet Plots\nFaceting splits a single plot into multiple plots, with each plot showing a different subset of the data. This is useful for comparing distributions across subsets.\nFor example, we can facet the bubble chart by continent:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n)\n\n                                                \n\n\nWe can change the arrangement of the facets by changing the facet_col_wrap parameter. For example, facet_col_wrap=2 will wrap the facets into two columns:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nSimilarly, we can facet the violin plots of tips by day of the week:\n\npx.violin(\n    tips,\n    x=\"sex\",\n    y=\"tip\",\n    color=\"sex\",\n    facet_col=\"day\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nFaceting allows us to compare distributions across different days, providing more granular insights.\n\n\n\n\n\n\nPractice\n\n\n\n9.8.4 Practice: Tips Facet Plot\nUsing the tips dataset, create a percent-stacked bar chart of the time column, colored by the sex column, and facetted by the day column.\nWhich day-time has the highest proportion of male customers (e.g. Friday Lunch, Saturday Dinner, etc.)?\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#summary",
    "href": "p_data_on_display_multivariate.html#summary",
    "title": "9  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "9.9 Summary",
    "text": "9.9 Summary\nIn this lesson, you learned how to create bivariate and multivariate graphs using Plotly Express. Understanding these visualization techniques will help you explore and communicate relationships in your data more effectively.\nSee you in the next lesson!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html",
    "href": "p_tools_installing_python.html",
    "title": "10  Installing Python",
    "section": "",
    "text": "10.1 Introduction\nSo far in our learning sequence, you’ve been working mostly in Google Colab, a convenient, cloud-based environment for running Python code. Now it’s time to tackle working locally on your own machine. This allows you to work offline and gives you more control over your development environment.\nWe’ll guide you through installing Python 3.12.0 to ensure compatibility with the examples and exercises we’ll cover. Let’s get started!\nBefore we proceed, we’ll split the instructions into two tracks:\nPlease follow the instructions specific to your operating system.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#introduction",
    "href": "p_tools_installing_python.html#introduction",
    "title": "10  Installing Python",
    "section": "",
    "text": "Windows Users\nmacOS Users",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#windows-users",
    "href": "p_tools_installing_python.html#windows-users",
    "title": "10  Installing Python",
    "section": "10.2 Windows Users",
    "text": "10.2 Windows Users\n\n10.2.1 Step 1: Check Your Current Python Version\n\nSearch for “Command Prompt” in your start menu.\nInto the terminal window type python --version and press Enter.\nIf Python is installed, it will display the version number.\nIf you already have version 3.12.0 installed, you can skip the rest of the steps. If you have any other version (higher or lower), proceed to the next step to install 3.12.0.\n\n\n\n10.2.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nScroll down to the bottom of the Python 3.12.0 page to find the “Windows installer (64-bit)”. Then click on the link to download the installer.\n\n\n\n10.2.3 Step 3: Install Python\n\nLocate the downloaded file (usually in your Downloads folder).\nDouble-click the installer to run it.\nImportant: Check the box that says “Add Python 3.12 to PATH” at the bottom of the installer window.\nIf you have the option, also elect to use admin privileges.\nAt the end you may be asked whether to disable path length limit. Click yes to this too.\nWait for the installation to finish.\n\n\n\n10.2.4 Step 4: Verify the Installation\n\nClose your old command prompt window if it is still open.\nIn your start menu, again search for “Command Prompt” and open a new terminal window.\nType python --version and press Enter.\nYou should see Python 3.12.0. (You may see a newer version if you already had Python on your computer. That’s okay. From our IDE, we will be able to select the correct version of Python that we just installed.)\n\n\n\n10.2.5 Step 5: Run Python Locally\n\nIn Command Prompt, type python and press Enter.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Enter.\nYou should see 4.\nType exit() and press Enter.\n\nYay! You’ve successfully installed Python and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#macos-users",
    "href": "p_tools_installing_python.html#macos-users",
    "title": "10  Installing Python",
    "section": "10.3 macOS Users",
    "text": "10.3 macOS Users\n\n10.3.1 Step 1: Check Your Current Python Version\n\nGo to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nIf Python is installed, it will display the version number.\nYou may get a pop-up asking to install command line developer tools if you do not have them. Go ahead and accept and install that.\nIf your computer does not have Python installed, you will get an error.\nOtherwise, you should see a version number.\nIf you already have version 3.12.0 installed, you can skip the rest of the steps. If you have any other version (higher or lower), proceed to the next step to install 3.12.0.\n\n\n\n10.3.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nEnsure the URL is from www.python.org to avoid unofficial sources.\nScroll down to the bottom of the Python 3.12.0 page.\nUnder the “Files” section, find “macOS 64-bit universal2 installer”.\nClick on the link to download the installer.\n\n\n\n10.3.3 Step 3: Install Python 3.12.0\n\nLocate the downloaded file (usually in your Downloads folder).\nDouble-click the installer to run it.\nWait for the installation to finish.\nClick “Close” once the installation is complete.\n\n\n\n10.3.4 Step 4: Verify the Installation\n\nClose your old terminal window if it is still open.\nAgain go to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nYou should see Python 3.12.0.\n\n\n\n10.3.5 Step 5: Run Python Locally\n\nIn Terminal, type python3 and press Return.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Return.\nYou should see 4.\nType exit() and press Return.\n\nYay! You’ve successfully installed Python 3.12.0 on your Mac and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html",
    "href": "p_tools_using_vscode.html",
    "title": "11  Installing and Using VS Code",
    "section": "",
    "text": "11.1 Introduction\nIn this lesson, we will explore Visual Studio Code (VS Code), a powerful and versatile editor for writing, running, and debugging Python code. VS Code is widely used due to its rich feature set and extensive extension library, which makes coding more efficient and enjoyable.\nNote that this lesson is better consumed as a video, since there are many Graphic-user-interface steps that are hard to describe in writing.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#learning-objectives",
    "href": "p_tools_using_vscode.html#learning-objectives",
    "title": "11  Installing and Using VS Code",
    "section": "11.2 Learning Objectives",
    "text": "11.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nOpen VS Code and navigate to the editor, terminal, and explorer tabs.\nCreate a new Python file and run it using the Python extension.\nUse the Command Palette to search for and select a color theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-vs-code",
    "href": "p_tools_using_vscode.html#installing-vs-code",
    "title": "11  Installing and Using VS Code",
    "section": "11.3 Installing VS Code",
    "text": "11.3 Installing VS Code\nVS Code is available for Windows, macOS, and Linux. Search for “VS Code” in your favorite search engine and go to the official website, code.visualstudio.com. Download the version for your computer’s operating system.\nFor macOs users, after installing VS Code, you may need to drag the icon to your applications folder.\nFrom your applications folder, open VS Code.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "href": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "title": "11  Installing and Using VS Code",
    "section": "11.4 Navigating the Explorer Tab",
    "text": "11.4 Navigating the Explorer Tab\nThe Explorer tab displays the files and folders in your current workspace. When you open VS Code for the first time, it may indicate that you have not yet opened a folder. Most of our work in Python will be organized in folders (also known as workspaces) that contain multiple files.\nLet’s create our first workspace:\n\nOn your computer, navigate to your desktop or another memorable location.\nCreate a new folder called first_python_workspace.\nIn VS Code, click on the “Open Folder” button and locate your newly created folder. Alternatively, you can drag the folder into the VS Code window.\n\nNow that your workspace is open in VS Code, you can create a new file:\n\nIn the Explorer tab, right-click inside the workspace folder.\nSelect “New File”.\nName the file first_script.py. The file will automatically open in the editor.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "href": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "title": "11  Installing and Using VS Code",
    "section": "11.5 Writing and Saving Python Code",
    "text": "11.5 Writing and Saving Python Code\nLet’s write some Python code in your new file:\nprint(2 + 2)\nTo save the file, press Ctrl + S (or Cmd + S on macOS).",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-the-python-extension",
    "href": "p_tools_using_vscode.html#installing-the-python-extension",
    "title": "11  Installing and Using VS Code",
    "section": "11.6 Installing the Python Extension",
    "text": "11.6 Installing the Python Extension\nTo run Python code within VS Code, we need to install the Python extension:\n\nClick on the Extensions tab on the left sidebar (it looks like four squares).\nIn the search bar at the top, type “Python”.\nFind the extension named “Python” published by Microsoft, and click Install.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#running-your-python-script",
    "href": "p_tools_using_vscode.html#running-your-python-script",
    "title": "11  Installing and Using VS Code",
    "section": "11.7 Running Your Python Script",
    "text": "11.7 Running Your Python Script\nWith the Python extension installed, you can now run your script:\n\nOpen first_script.py if it’s not already open.\nClick on the Run icon (a play button) in the top right corner of the editor.\n\nAlternatively, you can right-click inside the editor and select RUn python then “Run Python File in Terminal”.\n\nA terminal window will open at the bottom of the screen, and your code will execute. You should see the output:\n4",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#using-the-command-palette",
    "href": "p_tools_using_vscode.html#using-the-command-palette",
    "title": "11  Installing and Using VS Code",
    "section": "11.8 Using the Command Palette",
    "text": "11.8 Using the Command Palette\nThe Command Palette is a powerful feature in VS Code that allows you to access various commands and settings:\nTo access it, click on the search bar at the top of vscode window, then either press &gt; or select the ‘show and run commands’ option.\nThen type ‘theme’ and select ‘Preferences: Color Theme’.\nUse the up and down arrow keys to cycle through the available color themes.\nPress Enter to select your preferred theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#wrap-up",
    "href": "p_tools_using_vscode.html#wrap-up",
    "title": "11  Installing and Using VS Code",
    "section": "11.9 Wrap Up",
    "text": "11.9 Wrap Up\nIn this lesson, we’ve:\n\nInstalled VS Code and set up a workspace.\nCreated and saved a Python script.\nInstalled the Python extension to enable running and debugging code.\nUsed the Command Palette to customize the editor’s appearance.\n\nHappy coding!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html",
    "href": "p_tools_venv.html",
    "title": "12  Folders and Virtual Environments",
    "section": "",
    "text": "12.1 Introduction\nIn this lesson, we will explore virtual environments in Python using Visual Studio Code (VS Code). We’ll create a new workspace, set up a virtual environment, and install packages specific to our project.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#learning-objectives",
    "href": "p_tools_venv.html#learning-objectives",
    "title": "12  Folders and Virtual Environments",
    "section": "12.2 Learning Objectives",
    "text": "12.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate a new workspace in VS Code\nUnderstand the concept of virtual environments\nCreate and use a virtual environment in VS Code\nInstall and use packages within a virtual environment",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-new-workspace",
    "href": "p_tools_venv.html#creating-a-new-workspace",
    "title": "12  Folders and Virtual Environments",
    "section": "12.3 Creating a New Workspace",
    "text": "12.3 Creating a New Workspace\nOn your desktop or in your Documents folder, create a new folder and name it graph_courses_python. This is going to be the main folder for many of the projects for this course so make sure you put it in an easy to reach place.\nOpen VS Code.\nGo to File &gt; Open Folder.\nNavigate to the graph_courses_python folder you just created and select it.\nNow create a new script and call it test_cowsay.py.\nType print(2 + 2) in the file then run it by clicking the run button to make sure everything is working.\nNext, let’s try to import a package we haven’t installed yet. Add the following line to your file:\n\nimport cowsay\n\ncowsay.cow(\"Hello, World!\")\n\nIf you try to run this, you’ll get an error.\nThis will not work because we haven’t installed the cowsay package yet. And to install it properly, we’ll need to use virtual environments.\n\n12.3.1 Vocab: Environments & Interpreter\n\nAn environment is a folder that contains a specific version of Python and any packages you install.\nThe Python Interpreter is the specific",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-virtual-environment",
    "href": "p_tools_venv.html#creating-a-virtual-environment",
    "title": "12  Folders and Virtual Environments",
    "section": "12.4 Creating a Virtual Environment",
    "text": "12.4 Creating a Virtual Environment\n\nOpen the Command Palette\nType Python: Create Environment and select it.\nChoose Venv as the environment type.\nSelect the Python interpreter you want to use (e.g., Python 3.12.0).\n\nYou should now see a new folder called .venv. This is the virtual environment. Inside it is a folder called lib, which contains packages.\nNext, tell VS Code to use this virtual environment:\n\nOpen the Command Palette again.\nType Python: Select Interpreter and choose it.\nSelect the interpreter associated with your virtual environment (it should be listed under .venv).\n\nNow we’ve created and selected our virtual environment. We can install packages without affecting other projects.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-packages",
    "href": "p_tools_venv.html#installing-packages",
    "title": "12  Folders and Virtual Environments",
    "section": "12.5 Installing Packages",
    "text": "12.5 Installing Packages\nLet’s install the cowsay package.\n\nOpen a new terminal in VS Code. You can do this with the terminal file menu option then selecting ‘new terminal’.\nEnsure the terminal is using your virtual environment. You can do this by hovering over the terminal icon in terminal window. It should mention that there is an activated environment for ….venv among other things\nRun the following command:\n\n#| eval: false\npip install cowsay\nOccasionally, you might encounter an issue where pip is not recognized in the terminal. This is likely due to a temporary glitch in VS Code’s environment detection. If this happens, try the following steps:\n\nClose and reopen VS Code.\nUse the Command Palette to select the Python interpreter again.\nOpen a new terminal.\n\nThese steps should resolve the issue and restore access to pip. If the problem persists, it may indicate a more complex configuration problem that requires further investigation.\nNow we should be able to use the cowsay package. Open test_cowsay.py and click the run button to execute the script.\nYou should see a cow saying hello!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#summary-of-key-steps",
    "href": "p_tools_venv.html#summary-of-key-steps",
    "title": "12  Folders and Virtual Environments",
    "section": "12.6 Summary of Key Steps",
    "text": "12.6 Summary of Key Steps\nCongrats! You have now created a virtual environment and installed a package.\nThese are key steps for any new Python project:\n\nFolder: Create a project folder.\nEnvironment: Set up a virtual environment.\nInterpreter: Select the appropriate Python interpreter.\nLibraries: Install the necessary packages.\n\nRemember the acronym FEIL to help you recall these steps. (If you don’t complete these steps you increase your chances of “FEIL”ure 😅)\nIt’s a bit of a pain to have to set up a virtual environment every time you start a new project, but it’s a good habit to get into.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#demonstrating-environment-isolation",
    "href": "p_tools_venv.html#demonstrating-environment-isolation",
    "title": "12  Folders and Virtual Environments",
    "section": "12.7 Demonstrating Environment Isolation",
    "text": "12.7 Demonstrating Environment Isolation\nLet’s demonstrate that the virtual environment is isolated.\n\nOpen your previous workspace my_first_workspace with the File &gt; Open Folder menu option.\nCreate a Python file and try to use the cowsay package:\n::: {#758198e3 .cell execution_count=2} {.python .cell-code}  import cowsay :::\n\nThis will probably not work because cowsay is not installed in that environment. If it does work, it means you have cowsay installed globally, which is okay.\nNow, let’s return to our main folder/workspace. This is where you’ll conduct most of your analysis for this course.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-course-packages",
    "href": "p_tools_venv.html#installing-course-packages",
    "title": "12  Folders and Virtual Environments",
    "section": "12.8 Installing Course Packages",
    "text": "12.8 Installing Course Packages\nAs a final step, let’s install the packages we’ll need for the course. While we could install each package as we encounter it, it’s more efficient to install them all at once. In the terminal, run the following command. Type these very carefully.\n#| eval: false\npip install plotly pandas jupyter ipykernel kaleido itables\n\npandas: Data manipulation library.\nplotly: Visualization library.\njupyter and ipykernel: Allow us to use Quarto to display our plots.\nkaleido: Library for saving plots in different formats.\nitables: Library for displaying tables in Quarto.\n\nWhen its done installing, your cursor in the terminal should be active again. e.g. you should be able to press enter to start a new command.\nKeep this list of packages handy for future reference, as you’ll likely need them for most projects.\nThis command will install all the required packages in one go. If your installation stops at some point, try rerunning the command. Sometimes network issues may cause the installation to fail. If it freezes for more than 10 minutes, close the terminal and rerun the command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#conclusion",
    "href": "p_tools_venv.html#conclusion",
    "title": "12  Folders and Virtual Environments",
    "section": "12.9 Conclusion",
    "text": "12.9 Conclusion\nYou’ve now learned how to create a workspace, set up a virtual environment, install packages, and use them in your Python projects. Remember that each project should have its own virtual environment to keep dependencies isolated and manageable.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html",
    "href": "p_tools_quarto.html",
    "title": "13  Using Quarto",
    "section": "",
    "text": "13.1 Introduction\nA significant part of your role as a data analyst involves communicating results to others through reports. Quarto is one of the most powerful and versatile tools for producing such reports. It enables you to generate dynamic documents by combining formatted text and results produced by code. With Quarto, you can create documents in various formats such as HTML, PDF, Word, PowerPoint slides, web dashboards, and many others.\nMost of our documents in the GRAPH courses are actually written in Quarto!\nIn this lesson, we will cover the basics of this powerful tool.\nNote that for this lesson, the video may be easier to understand than the lesson notes, since there are many graphical user interface steps that are hard to describe in writing.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#learning-objectives",
    "href": "p_tools_quarto.html#learning-objectives",
    "title": "13  Using Quarto",
    "section": "13.2 Learning Objectives",
    "text": "13.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate and render a Quarto document that includes Python code and narrative text.\nOutput documents in multiple formats, including HTML, PDF, Word, etc.\nUnderstand basic Markdown syntax.\nUse code chunk options to control code execution and output display.\nUse Python packages to display tables and figures in Quarto documents.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#installing-quarto",
    "href": "p_tools_quarto.html#installing-quarto",
    "title": "13  Using Quarto",
    "section": "13.3 Installing Quarto",
    "text": "13.3 Installing Quarto\nTo get started, you first need to install Quarto.\nSearch for “quarto download” in your favorite search engine. Follow the results to the “Quarto.org” website, and then follow the instructions for your operating system. (We are not providing a direct link here since the exact link may change over time).\nAfter installing, you can check that it is installed by running the following command in the command line or terminal:\nquarto --version\nNow that Quarto is installed, use it to install the tinytex package, which we will need to compile our PDFs:\nquarto install tinytex\nTo use all the features of Quarto in VSCode, we need to install the Quarto extension and the Jupyter extension. You can install these in the Extensions tab of VSCode. Be sure to install the official versions of these extensions; they should have checkmarks next to them.\nThat’s a lot of installations, but fear not—you only have to do these steps once on your computer.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#project-setup",
    "href": "p_tools_quarto.html#project-setup",
    "title": "13  Using Quarto",
    "section": "13.4 Project Setup",
    "text": "13.4 Project Setup\nTo begin, open your graph_courses_python project in VSCode.\nIf you did not watch the previous video explaining project setup, please do so now. In that video, we explain how to create a project folder, create a virtual environment, select an interpreter, and install the jupyter, ipykernel, kaleido, itables, plotly, and pandas packages.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#create-a-new-document",
    "href": "p_tools_quarto.html#create-a-new-document",
    "title": "13  Using Quarto",
    "section": "13.5 Create a New Document",
    "text": "13.5 Create a New Document\nA Quarto document is a simple text file with the .qmd extension.\nTo create a new Quarto document, create a new file and save it with a .qmd extension, for example, first_quarto_doc.qmd.\nAdd two sections to your document with the following text:\n# Section 1\n\nHello\n\n---\n\n# Section 2\n\nWorld",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#adding-code-chunks",
    "href": "p_tools_quarto.html#adding-code-chunks",
    "title": "13  Using Quarto",
    "section": "13.6 Adding Code Chunks",
    "text": "13.6 Adding Code Chunks\nYou can add code chunks to your document by using the following syntax with the shortcut Cmd + Shift + I (on Mac) or Ctrl + Shift + I (on Windows). Alternatively, you can click on the “…” button at the top right of the screen.\nLet’s create a code chunk that adds two numbers together and displays the result:\n\n2 + 2\n\n4\n\n\nYou should see a “Run Cell” button in the toolbar. Click this to run the code chunk.\nVSCode may prompt you to install the ipykernel package if you have not yet installed it in your current environment. Go ahead and install it.\nNow practice adding one more code chunk at the end of the document that multiplies 3 by 3.\nAs you add these, you should see the buttons “Run Next Cell” and “Run Above” also appear.\nThere are two shortcuts you should also get used to:\n\nCmd + Enter (Mac) or Ctrl + Enter (Windows/Linux) to run the code chunk.\nOption + Enter (Mac) or Alt + Enter (Windows/Linux) to run the current line or the highlighted section of code.\n\nTo test these, add multiple lines of code to one code chunk, then practice running the whole chunk with the first shortcut, and line by line with the second.\nAs you can see, we can use Quarto as an interactive document similar to a Jupyter notebook or Google Colab. But what makes it really shine is its ability to output to many formats.\nLet’s see how to use this functionality.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#quarto-document-header-yaml",
    "href": "p_tools_quarto.html#quarto-document-header-yaml",
    "title": "13  Using Quarto",
    "section": "13.7 Quarto Document Header (YAML)",
    "text": "13.7 Quarto Document Header (YAML)\nAt the top of the document, let’s add a YAML section. This is where we can specify details about the document, such as its title, author, and format.\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\nformat: html\n---\nThe format is where we can specify the output format of the document. For now, we are trying to keep things simple and just use HTML.\nFor things to render properly, you need the jupyter package. Watch our video on setting up a virtual environment if you have not done this yet and are not sure how to install packages.\nTo render the document, click on the “Render” button in the top right of the screen.\nYou should see a new tab open in your VSCode.\nIf you go to the explorer, you should see a new file called first_quarto_doc.html.\nSo now we have the main elements of a Quarto document:\n\nThe YAML header\nSection headers\nText\nCode chunks\nThe outputs of those code chunks\n\nThese elements together make Quarto a very powerful tool for reporting.\nYou can also customize the output format with additional options. For example, to embed resources directly into the HTML file, you can modify the format section in the YAML header:\nformat:\n  html:\n    embed-resources: true\nAnother powerful feature of Quarto is its ability to output to many formats.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#output-formats",
    "href": "p_tools_quarto.html#output-formats",
    "title": "13  Using Quarto",
    "section": "13.8 Output Formats",
    "text": "13.8 Output Formats\nYou can change the format value in the YAML header to experiment with other formats.\nTry the following formats:\n\nhtml: Renders the document as an HTML webpage.\npdf: Renders the document as a PDF. You will need to have LaTeX (or tinytex) installed on your computer to use this format.\ndocx: Renders the document as a Microsoft Word document.\npptx: Renders the document as a PowerPoint presentation.\nrevealjs: Renders the document as an HTML slideshow.\ndashboard: Renders the document as an interactive dashboard.\n\nThere is a chance some of these may not work on your computer due to different operating systems or versions of software.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#markdown",
    "href": "p_tools_quarto.html#markdown",
    "title": "13  Using Quarto",
    "section": "13.9 Markdown",
    "text": "13.9 Markdown\nThe text inside Quarto documents is written in Markdown.\nMarkdown is a simple set of conventions for adding formatting to plain text. For example, to italicize text, you wrap it in asterisks *text here*, and to start a new header, you use the pound sign #. We will learn these in detail below.\nIn your editor, you can refer to the “Markdown Quick Reference” or “Cheatsheet” to learn more about Markdown syntax.\nYou can define titles of different levels by starting a line with one or more #:\n# Level 1 Title\n\n## Level 2 Title\n\n### Level 3 Title\nThe body of the document consists of text that follows the Markdown syntax. A Markdown file is a text file that contains lightweight markup to help set heading levels or format text. For example, the following text:\nThis is text with *italics* and **bold**.\n\nYou can define bulleted lists:\n\n- First element\n- Second element\nWill generate the following formatted text:\n\nThis is text with italics and bold.\nYou can define bulleted lists:\n\nFirst element\nSecond element\n\n\nNote that you need spaces before and after lists, as well as keeping the listed items on separate lines. Otherwise, they will all crunch together rather than making a list.\nWe see that words placed between asterisks are italicized, and lines that begin with a dash are transformed into a bulleted list.\nThe Markdown syntax allows for other formatting, such as the ability to insert links or images. For example, the following code:\n[Example Link](https://example.com)\n… will give the following link:\n\nExample Link\n\nWe can also embed images. In your document, you can type:\n![Alt text](images/picture_name.jpg)\nReplace “Alt text” with a description of the image (it can also be blank), “images” with the name of the image folder in your project, and “picture_name.jpg” with the name of the image you want to use. Alternatively, in some editors, you can drag and drop the image into your document.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#code-chunk-options",
    "href": "p_tools_quarto.html#code-chunk-options",
    "title": "13  Using Quarto",
    "section": "13.10 Code Chunk Options",
    "text": "13.10 Code Chunk Options\nIt is possible to pass options to each code chunk to modify its behavior.\nFor example, a code chunk looks like this:\n\n# Your code here\nx = 2 + 2\nprint(x)\n\n4\n\n\nBut you can add options to control the code chunk’s execution and display:\n\n\n4\n\n\nIn this example, the echo: false option tells Quarto not to display the code in the rendered document, only the output.\n\n13.10.1 Global Options\nYou may want to apply options globally to all code chunks in your document. You can set default code execution options in the YAML header under the execute key.\nFor example:\n---\ntitle: \"Quarto Document\"\nformat: html\nexecute:\n  echo: false\n---\nThis will set echo: false for all code chunks.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-tables",
    "href": "p_tools_quarto.html#displaying-tables",
    "title": "13  Using Quarto",
    "section": "13.11 Displaying Tables",
    "text": "13.11 Displaying Tables\nBy default, pandas DataFrames display neatly in Quarto. However, for interactive tables, we can use the itables package.\nEnsure that you have the itables package installed. If not, you can install it using the following command:\n\n!pip install itables\n\nThen, you can run code similar to the following:\n\nimport plotly.express as px\nfrom itables import show\n\ntips = px.data.tips()\nshow(tips)\n\n\n\n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.2 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNote that interactive tables will only work in HTML formats. We will look at tables for other formats later.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-plots",
    "href": "p_tools_quarto.html#displaying-plots",
    "title": "13  Using Quarto",
    "section": "13.12 Displaying Plots",
    "text": "13.12 Displaying Plots\nFor interactive plots, the plotly package is very useful.\n\ntips = px.data.tips()\ntips_sex = px.violin(tips, x=\"day\", y=\"total_bill\", color=\"sex\")\ntips_sex.show()\n\n                                                \n\n\nThis will display an interactive Plotly plot in the HTML output.\nFor static outputs like PDFs and Word documents, we need to save the plot as an image file and then include it in the document.\nFirst, save the plot as an image:\n\ntips_sex.write_image(\"tips_sex_plot.png\")\n\nThis command will create a static image file in the same folder as your document. We can then include it in the document as follows:\n![Violin plot of total bill by day and sex](tips_sex_plot.png)\nThis will display the image in the output.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#wrapping-up",
    "href": "p_tools_quarto.html#wrapping-up",
    "title": "13  Using Quarto",
    "section": "13.13 Wrapping Up",
    "text": "13.13 Wrapping Up\nIn this lesson, we covered how to create and render Quarto documents, add formatting, and include code chunks. We also learned how to use code chunk options to control the behavior of our documents. We experimented with different output formats and how to customize the display of our documents.\nWith these tools, you can create dynamic and interactive reports that are easily shareable in various formats. Quarto’s flexibility and integration with Python make it an excellent choice for data analysts and researchers alike.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html",
    "href": "p_untangled_subset_columns.html",
    "title": "14  Subsetting columns",
    "section": "",
    "text": "14.1 Introduction\nToday we will begin our exploration of pandas for data manipulation!\nOur first focus will be on selecting and renaming columns. Often your dataset comes with many columns that you do not need, and you would like to narrow it down to just a few. Pandas makes this easy. Let’s see how.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#learning-objectives",
    "href": "p_untangled_subset_columns.html#learning-objectives",
    "title": "14  Subsetting columns",
    "section": "14.2 Learning objectives",
    "text": "14.2 Learning objectives\n\nYou can keep or drop columns from a DataFrame using square brackets [], filter(), and drop().\nYou can select columns based on regex patterns with filter().\nYou can use rename() to change column names.\nYou can use regex to clean column names.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#about-pandas",
    "href": "p_untangled_subset_columns.html#about-pandas",
    "title": "14  Subsetting columns",
    "section": "14.3 About pandas",
    "text": "14.3 About pandas\nPandas is a popular library for data manipulation and analysis. It is designed to make it easy to work with tabular data in Python.\nInstall pandas with the following command in your terminal if it is not already installed:\n\npip install pandas \n\nThen import pandas with the following command in your script:\n\nimport pandas as pd",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "title": "14  Subsetting columns",
    "section": "14.4 The Yaounde COVID-19 dataset",
    "text": "14.4 The Yaounde COVID-19 dataset\nIn this lesson, we analyse results from a COVID-19 survey conducted in Yaounde, Cameroon in late 2020. The survey estimated how many people had been infected with COVID-19 in the region, by testing for antibodies.\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nTo download the dataset, visit this link: https://raw.githubusercontent.com/the-graph-courses/idap_book/main/data/yaounde_data.zip\nThen unzip the file and place the yaounde_data.csv file in the data folder in the same directory as your notebook.\nLet’s load and examine the dataset:\n\nyao = pd.read_csv(\"data/yaounde_data.csv\")\nyao\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "href": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "title": "14  Subsetting columns",
    "section": "14.5 Selecting columns with square brackets []",
    "text": "14.5 Selecting columns with square brackets []\nIn pandas, the most common way to select a column is simply to use square brackets [] and the column name. For example, to select the age and sex columns, we type:\n\nyao[[\"age\", \"sex\"]]\n\n\n\n\n\n\n\n\nage\nsex\n\n\n\n\n0\n45\nFemale\n\n\n1\n55\nMale\n\n\n...\n...\n...\n\n\n969\n31\nFemale\n\n\n970\n17\nFemale\n\n\n\n\n971 rows × 2 columns\n\n\n\nNote the double square brackets [[]]. Without it, you will get an error:\n\nyao[\"age\", \"sex\"]\n\nKeyError: ('age', 'sex')\nIf you want to select a single column, you may omit the double square brackets, but your output will no longer be a DataFrame. Compare the following:\n\nyao[\"age\"] # does not return a DataFrame\n\n0      45\n1      55\n       ..\n969    31\n970    17\nName: age, Length: 971, dtype: int64\n\n\n\nyao[[\"age\"]]  # returns a DataFrame\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\n14.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.6.1 Practice Q: Select Columns with []\n\nUse the [] operator to select the “weight_kg” and “height_cm” variables in the yao DataFrame. Assign the result to a new DataFrame called yao_weight_height. Then print this new DataFrame.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro tip\n\n\n\nThere are many ways to select columns in pandas. In your free time, you may choose to explore the .loc[] and .take() methods, which provide additional functionality.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#storing-data-subsets",
    "href": "p_untangled_subset_columns.html#storing-data-subsets",
    "title": "14  Subsetting columns",
    "section": "14.6 Storing data subsets",
    "text": "14.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "href": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "title": "14  Subsetting columns",
    "section": "14.7 Excluding columns with drop()",
    "text": "14.7 Excluding columns with drop()\nSometimes it is more useful to drop columns you do not need than to explicitly select the ones that you do need.\nTo drop columns, we can use the drop() method with the columns argument. To drop the age column, we type:\n\nyao.drop(columns=[\"age\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\nNon-smoker\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\nEx-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 52 columns\n\n\n\nTo drop several columns:\n\nyao.drop(columns=[\"age\", \"sex\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\nAgain, note that this is not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset age and sex dropped:\n\nyao_subset = yao.drop(columns=[\"age\", \"sex\"])\nyao_subset\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.7.1 Practice Q: Drop Columns with drop()\n\nFrom the yao DataFrame, remove the columns highest_education and consultation. Assign the result to a new DataFrame called yao_no_education_consultation. Print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "href": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "title": "14  Subsetting columns",
    "section": "14.8 Using filter() to select columns by regex",
    "text": "14.8 Using filter() to select columns by regex\nThe filter() method and its regex argument offer a powerful way to select columns based on patterns in their names. As an example, to select columns containing the string “ig”, we can write:\n\nyao.filter(regex=\"ig\")\n\n\n\n\n\n\n\n\nhighest_education\nweight_kg\nheight_cm\nneighborhood\nigg_result\nigm_result\nsymp_fatigue\n\n\n\n\n0\nSecondary\n95\n169\nBriqueterie\nNegative\nNegative\nNo\n\n\n1\nUniversity\n96\n185\nBriqueterie\nPositive\nNegative\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nSecondary\n66\n169\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n970\nSecondary\n67\n162\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n\n\n971 rows × 7 columns\n\n\n\nThe argument regex specifies the pattern to match. Regex stands for regular expression and refers to a sequence of characters that define a search pattern.\nTo select columns starting with the string “ig”, we write:\n\nyao.filter(regex=\"^ig\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe symbol ^ is a regex character that matches the beginning of the string.\nTo select columns ending with the string “result”, we can write:\n\nyao.filter(regex=\"result$\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe character $ is regex that matches the end of the string.\n\n\n\n\n\n\nPro Tip\n\n\n\nRegex is notoriously difficult to remember, but LLMs like ChatGPT are very good at generating the right patterns. Simply ask, for example, “What is the regex for strings starting with ‘ig’”\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.8.1 Practice Q: Select Columns with Regex\n\nSelect all columns in the yao DataFrame that start with “is_”. Assign the result to a new DataFrame called yao_is_columns. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "href": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "title": "14  Subsetting columns",
    "section": "14.9 Change column names with rename()",
    "text": "14.9 Change column names with rename()\nWe can use the rename() method to change column names:\n\nyao.rename(columns={\"age\": \"patient_age\", \"sex\": \"patient_sex\"})\n\n\n\n\n\n\n\n\nid\ndate_surveyed\npatient_age\nage_category\nage_category_3\npatient_sex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.9.1 Practice Q: Rename Columns with rename()\n\nRename the age_category column in the yao DataFrame to age_cat. Assign the result to a new DataFrame called yao_age_cat. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "href": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "title": "14  Subsetting columns",
    "section": "14.10 Cleaning messy column names",
    "text": "14.10 Cleaning messy column names\nFor cleaning column names, you can use regular expressions with the str.replace() method in pandas.\nHere’s how you can do it on a test DataFrame with messy column names. Messy column names are names with spaces, special characters, or other non-alphanumeric characters.\n\ntest_df = pd.DataFrame(\n    {\"good_name\": range(3), \"bad name\": range(3), \"bad*@name*2\": range(3)}\n)\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad name\nbad*@name*2\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2\n\n\n\n\n\n\n\nSuch column names are not ideal because, for example, we cannot select them with the dot operator the way we can for clean names:\n\ntest_df.good_name  # this works\n\n0    0\n1    1\n2    2\nName: good_name, dtype: int64\n\n\nBut this does not work:\n\ntest_df.bad name\n\n      test_df.bad name\n                 ^\nSyntaxError: invalid syntax\nWe can automatically clean such names using the str.replace() method along with regular expressions.\n\nclean_names = test_df.columns.str.replace(r'[^a-zA-Z0-9]', '_', regex=True)\n\nThe regular expression r'[^a-zA-Z0-9]' matches any character that is not a letter (either uppercase or lowercase) or a digit. The str.replace() method replaces these characters with an underscore (‘_’) to make the column names more legible and usable in dot notation.\nNow we can replace the column names in the DataFrame with the cleaned names:\n\ntest_df.columns = clean_names\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad_name\nbad__name_2\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.10.1 Practice Q: Clean Column Names with Regex\n\nConsider the data frame defined below with messy column names. Use the str.replace() method to clean the column names.\n\n\ncleaning_practice = pd.DataFrame(\n    {\"Aloha\": range(3), \"Bell Chart\": range(3), \"Animals@the zoo\": range(3)}\n)\ncleaning_practice\n\n\n\n\n\n\n\n\nAloha\nBell Chart\nAnimals@the zoo\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#wrap-up",
    "href": "p_untangled_subset_columns.html#wrap-up",
    "title": "14  Subsetting columns",
    "section": "14.11 Wrap up",
    "text": "14.11 Wrap up\nHopefully this lesson has shown you how intuitive and useful pandas is for data manipulation!\nThis is the first of a series of basic data wrangling techniques: see you in the next lesson to learn more.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html",
    "href": "p_untangled_query_rows.html",
    "title": "15  Querying rows",
    "section": "",
    "text": "15.1 Intro\nQuerying rows is one of the most frequently used operations in data analysis. It allows you to filter your dataset to focus on specific subsets of interest, enabling more targeted and efficient analysis.\nIn this lesson, we’ll explore various techniques to subset rows in pandas.\nLet’s get started!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#learning-objectives",
    "href": "p_untangled_query_rows.html#learning-objectives",
    "title": "15  Querying rows",
    "section": "15.2 Learning objectives",
    "text": "15.2 Learning objectives\n\nYou can use the query() method to keep or drop rows from a DataFrame.\nYou can specify conditions using relational operators like greater than (&gt;), less than (&lt;), equal to (==), not equal to (!=), and is an element of (isin()).\nYou can combine conditions with & and |.\nYou can negate conditions with ~.\nYou can use the isna() and notna() methods.\nYou can query based on string patterns using str.contains().",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "title": "15  Querying rows",
    "section": "15.3 The Yaounde COVID-19 dataset",
    "text": "15.3 The Yaounde COVID-19 dataset\nIn this lesson, we will again use the data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nLet’s load the data into a pandas DataFrame.\n\nimport pandas as pd\n\nyaounde = pd.read_csv(\"data/yaounde_data.csv\")\n# a smaller subset of variables\nyao = yaounde[\n    [\n        \"age\",\n        \"sex\",\n        \"weight_kg\",\n        \"neighborhood\",\n        \"occupation\",\n        \"symptoms\",\n        \"is_smoker\",\n        \"is_pregnant\",\n        \"igg_result\",\n        \"igm_result\",\n    ]\n]\nyao.head()\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n4\n55\nFemale\n67\nBriqueterie\nTrader--Farmer\nNo symptoms\nNon-smoker\nNo\nPositive\nNegative",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#introducing-query",
    "href": "p_untangled_query_rows.html#introducing-query",
    "title": "15  Querying rows",
    "section": "15.4 Introducing query()",
    "text": "15.4 Introducing query()\nWe can use the query() method to keep rows that satisfy a set of conditions. Let’s take a look at a simple example. If we want to keep just the male records, we run:\n\nyao.query('sex == \"Male\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nAs you can see, the query() syntax is quite simple. (It may be a bit surprising to have to put code in quotes, but it is quite readable.)\nNote the use of double equals (==) instead of single equals (=) there. The == sign tests for equality, while the single equals sign assigns a value. This is a common source of errors when you are a beginner, so watch out for it.\nWe can chain query() with shape[0] to count the number of male respondents.\n\nyao.query('sex == \"Male\"').shape[0]\n\n422\n\n\n\n\n\n\n\n\nReminder\n\n\n\nThe shape property returns the number of rows and columns in a DataFrame. The first element, shape[0], is the number of rows, and the second element, shape[1], is the number of columns.\nFor example:\n\nyao.shape\n\n(971, 10)\n\n\n\nyao.shape[0] # rows\n\n971\n\n\n\nyao.shape[1]  # columns\n\n10\n\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\nNote that these subsets are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset of male respondents:\n\nyao_male = yao.query('sex == \"Male\"')\nyao_male\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nBut for ease of explanation, in the examples below, we are simply printing the result, without storing it in a variable.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.4.1 Practice Q: Subset for Pregnant Respondents\nSubset the yao data frame to respondents who were pregnant during the survey (The is_pregnant column contains “Yes”, “No” or NaN). Assign the result to a new DataFrame called yao_pregnant. Then print this new DataFrame. There should be 24 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#relational-operators",
    "href": "p_untangled_query_rows.html#relational-operators",
    "title": "15  Querying rows",
    "section": "15.5 Relational operators",
    "text": "15.5 Relational operators\nThe == operator introduced above is an example of a “relational” operator, as it tests the relation between two values. Here is a list of some more of these operators. You will use these often when you are querying rows in your data.\n\n\n\nOperator\nis True if\n\n\nA == B\nA is equal to B\n\n\nA != B\nA is not equal to B\n\n\nA &lt; B\nA is less than B\n\n\nA &lt;= B\nA is less than or equal to B\n\n\nA &gt; B\nA is greater than B\n\n\nA &gt;= B\nA is greater than or equal to B\n\n\nA.isin([B])\nA is an element of B\n\n\n\nLet’s see how to use these with query():\n\nyao.query('sex == \"Female\"')  # keep rows where `sex` is female\nyao.query('sex != \"Male\"')  # keep rows where `sex` is not \"Male\"\nyao.query(\"age &lt; 6\")  # keep respondents under 6\nyao.query(\"age &gt;= 70\")  # keep respondents aged at least 70\n\n# keep respondents whose neighbourhood is \"Tsinga\" or \"Messa\"\nyao.query('neighborhood.isin([\"Tsinga\", \"Messa\"])')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n605\n55\nMale\n70\nMessa\nInformal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n606\n59\nFemale\n59\nMessa\nTrader\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n902\n25\nFemale\n41\nTsinga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n903\n28\nMale\n69\nTsinga\nTrader\nSneezing--Headache\nEx-smoker\nNaN\nNegative\nNegative\n\n\n\n\n129 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.5.1 Practice Q: Subset for Children\n\nFrom yao, keep only respondents who were children (under 18). Assign the result to a new DataFrame called yao_children. There should be 291 rows.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.5.2 Practice Q: Subset for Tsinga and Messa\n\nWith isin(), keep only respondents who live in the “Carriere” or “Ekoudou” neighborhoods. Assign the result to a new DataFrame called yao_carriere_ekoudou. There should be 426 rows.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "href": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "title": "15  Querying rows",
    "section": "15.6 Accessing external variables in query()",
    "text": "15.6 Accessing external variables in query()\nThe query() method allows you to access variables outside the DataFrame using the @ symbol. This is useful when you want to use dynamic values in your query conditions.\nFor example, say you have the variable min_age that you want to use in your query. You can do this as follows:\n\nmin_age = 25\n\n# Query using external variables\nyao.query('age &gt;= @min_age')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n524 rows × 10 columns\n\n\n\nThis feature is helpful when you need to filter data based on values that may change or are determined at runtime.\n\n\n\n\n\n\nPractice\n\n\n\n15.6.1 Practice Q: Subset for Young Respondents\n\nFrom yao, keep respondents who are less than or equal to the variable max_age, defined below. Assign the result to a new DataFrame called yao_young. There should be 590 rows.\n\n\nmax_age = 30\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#combining-conditions-with-and",
    "href": "p_untangled_query_rows.html#combining-conditions-with-and",
    "title": "15  Querying rows",
    "section": "15.7 Combining conditions with & and |",
    "text": "15.7 Combining conditions with & and |\nWe can pass multiple conditions to query() using & (the “ampersand” symbol) for AND and | (the “vertical bar” or “pipe” symbol) for OR.\nFor example, to keep respondents who are either younger than 18 OR older than 65, we can write:\n\nyao.query(\"age &lt; 18 | age &gt; 65\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n331 rows × 10 columns\n\n\n\nTo keep respondents who are pregnant and are ex-smokers, we write:\n\nyao.query('is_pregnant == \"Yes\" & is_smoker == \"Ex-smoker\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n273\n25\nFemale\n90\nCarriere\nHome-maker\nCough--Rhinitis--Sneezing\nEx-smoker\nYes\nPositive\nNegative\n\n\n\n\n\n\n\nTo keep all respondents who are pregnant or ex-smokers, we write:\n\nyao.query('is_pregnant == \"Yes\" | is_smoker == \"Ex-smoker\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n14\n42\nMale\n71\nBriqueterie\nTrader\nNo symptoms\nEx-smoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n953\n31\nFemale\n90\nTsinga Oliga\nSalaried worker\nFever--Cough--Sore throat--Headache\nEx-smoker\nNo\nPositive\nNegative\n\n\n967\n23\nFemale\n76\nTsinga Oliga\nInformal worker--Trader\nHeadache\nNon-smoker\nYes\nNegative\nNegative\n\n\n\n\n94 rows × 10 columns\n\n\n\n\n\n\n\n\n\nSide note\n\n\n\nTo get the unique values in a column, you can use the value_counts() method.\n\nyao.is_smoker.value_counts()\n\nis_smoker\nNon-smoker    859\nEx-smoker      71\nSmoker         39\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.7.1 Practice Q: Subset for IgG Positive Men\nSubset yao to only keep men who tested IgG positive. Assign the result to a new DataFrame called yao_igg_positive_men. There should be 148 rows after your query. Think carefully about whether to use & or |.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "href": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "title": "15  Querying rows",
    "section": "15.8 Negating conditions with the ~ operator",
    "text": "15.8 Negating conditions with the ~ operator\nTo negate conditions in query(), we use the ~ operator (pronounced “tilde”).\nLet’s use this to drop respondents who are students:\n\nyao.query('~ (occupation == \"Student\")')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n588 rows × 10 columns\n\n\n\nNotice that we have to enclose the condition in parentheses.\nWe can also enclose multiple conditions in parentheses.\nImagine we want to give out a drug, but since it is a strong drug, we don’t want children or lightweight (under 30kg) respondents to take it. First, we can write a query to select the children and these light respondents:\n\nyao.query(\"age &lt; 18 | weight_kg &lt; 30\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n291 rows × 10 columns\n\n\n\nNow to drop these individuals, we can negate the condition with ~:\n\nyao.query(\"~ (age &lt; 18 | weight_kg &lt; 30)\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n680 rows × 10 columns\n\n\n\nThis could also be written as:\n\nyao.query(\"age &gt;= 18 & weight_kg &gt;= 30\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n680 rows × 10 columns\n\n\n\nBut sometimes negated conditions are easier to read.\n\n\n\n\n\n\nPractice\n\n\n\n15.8.1 Practice Q: Drop Smokers and drop those over 50\nWe want to avoid giving a drug to older individuals and smokers. From yao, drop respondents that are either above 50 or who are smokers. Use ~ to negate the conditions. Assign the result to a new DataFrame called yao_dropped. Your output should have 810 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#nan-values",
    "href": "p_untangled_query_rows.html#nan-values",
    "title": "15  Querying rows",
    "section": "15.9 NaN values",
    "text": "15.9 NaN values\nThe relational operators introduced so far do not work with null values like NaN.\nFor example, the is_pregnant column contains (NA) values for men. To keep the rows with missing is_pregnant values, we could try writing:\n\nyao.query(\"is_pregnant == NaN\")  # does not work\n\nBut this will not work. This is because NaN is a non-existent value. So the system cannot evaluate whether it is “equal to” or “not equal to” anything.\nInstead, we can use the isna() method to select rows with missing values:\n\nyao.query(\"is_pregnant.isna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 10 columns\n\n\n\nOr we can select rows that are not missing with notna():\n\nyao.query(\"is_pregnant.notna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n549 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.9.1 Practice Q: Keep Missing Smoking Status\nFrom the yao dataset, keep all the respondents who had NA records for the report of their smoking status.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "href": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "title": "15  Querying rows",
    "section": "15.10 Querying Based on String Patterns",
    "text": "15.10 Querying Based on String Patterns\nSometimes, we need to filter our data based on whether a string column contains a certain substring. This is particularly useful when dealing with multi-answer type variables, where responses may contain multiple values separated by delimiters. Let’s explore this using the occupation column in our dataset.\nFirst, let’s take a look at the unique values in the occupation column:\n\nyao.occupation.value_counts().to_dict()\n\n{'Student': 383,\n 'Informal worker': 189,\n 'Trader': 111,\n 'Unemployed': 68,\n 'Home-maker': 65,\n 'Salaried worker': 54,\n 'Retired': 27,\n 'Student--Informal worker': 13,\n 'Other': 13,\n 'No response': 9,\n 'Farmer': 5,\n 'Informal worker--Trader': 4,\n 'Student--Trader': 4,\n 'Trader--Farmer': 4,\n 'Home-maker--Informal worker': 3,\n 'Home-maker--Trader': 3,\n 'Retired--Informal worker': 3,\n 'Informal worker--Other': 2,\n 'Home-maker--Farmer': 2,\n 'Student--Other': 1,\n 'Farmer--Other': 1,\n 'Trader--Unemployed': 1,\n 'Retired--Other': 1,\n 'Informal worker--Unemployed': 1,\n 'Retired--Trader': 1,\n 'Home-maker--Informal worker--Farmer': 1,\n 'Student--Informal worker--Other': 1,\n 'Informal worker--Trader--Farmer--Other': 1}\n\n\nAs we can see, some respondents have multiple occupations, separated by “–”. To query based on string containment, we can use the str.contains() method within our query().\n\n15.10.1 Basic String Containment\nTo find all respondents who are students (either solely or in combination with other occupations), we can use:\n\nyao.query(\"occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n963\n26\nFemale\n63\nTsinga Oliga\nStudent\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n964\n28\nMale\n76\nTsinga Oliga\nStudent--Informal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n\n\n402 rows × 10 columns\n\n\n\nThis query will return all rows where the occupation column contains the word “Student”, regardless of whether it’s the only occupation or part of a multiple-occupation entry.\n\n\n15.10.2 Negating String Containment\nTo find respondents who are not students (i.e., their occupation does not contain “Student”), you can use the ~ operator:\n\nyao.query(\"~occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n569 rows × 10 columns\n\n\n\n\n\n15.10.3 Using | with string containment\nTo find respondents who are students or farmers, we can use:\n\nyao.query(\"occupation.str.contains('Student|Farmer')\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n2\n23\nMale\n74\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n963\n26\nFemale\n63\nTsinga Oliga\nStudent\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n964\n28\nMale\n76\nTsinga Oliga\nStudent--Informal worker\nNo symptoms\nNon-smoker\nNaN\nNegative\nNegative\n\n\n\n\n416 rows × 10 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.10.4 Practice Q: Symptoms\nThe symptoms column contains a list of symptoms that respondents reported.\nQuery yao to find respondents who reported “Cough” or “Fever” as symptoms. Your answer should have 219 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#wrap-up",
    "href": "p_untangled_query_rows.html#wrap-up",
    "title": "15  Querying rows",
    "section": "15.11 Wrap up",
    "text": "15.11 Wrap up\nGreat job! You’ve learned how to select specific columns and filter rows based on various conditions.\nThese skills allow you to focus on relevant data and create targeted subsets for analysis.\nNext, we’ll explore how to modify and transform your data, further expanding your data wrangling toolkit. See you in the next lesson!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Querying rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html",
    "href": "p_untangled_transform_variables.html",
    "title": "16  Transforming Variables in pandas",
    "section": "",
    "text": "16.1 Introduction\nIn data analysis, one of the most common tasks is transforming variables within your dataset. The pandas library provides straightforward and efficient ways to accomplish this.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#learning-objectives",
    "href": "p_untangled_transform_variables.html#learning-objectives",
    "title": "16  Transforming Variables in pandas",
    "section": "16.2 Learning Objectives",
    "text": "16.2 Learning Objectives\n\nUnderstand how to create new variables in a DataFrame.\nLearn how to modify existing variables.\nHandle potential issues with modifying variables on views.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#imports",
    "href": "p_untangled_transform_variables.html#imports",
    "title": "16  Transforming Variables in pandas",
    "section": "16.3 Imports",
    "text": "16.3 Imports\nFirst, let’s import the pandas package:\n\nimport pandas as pd\n\nNow we’ll set an important option that will help us avoid some warnings down the line. Later in the lesson, we’ll discuss this in more detail.\n\npd.options.mode.copy_on_write = True",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#dataset",
    "href": "p_untangled_transform_variables.html#dataset",
    "title": "16  Transforming Variables in pandas",
    "section": "16.4 Dataset",
    "text": "16.4 Dataset\nIn this lesson, we’ll use a dataset of United States counties with demographic and economic data. You can download the dataset from this link:https://github.com/the-graph-courses/idap_book/raw/refs/heads/main/data/us_counties_data.zip.\nOnce you’ve downloaded the file, unzip it and place the us_counties_data.csv file in the data folder for your project.\n\ncounties = pd.read_csv(\"data/us_counties_data.csv\")\ncounties\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\n58877.0\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nPR\nYabucoa, PR\n30364.0\n55.214614\nNaN\nNaN\nNaN\nNaN\n-19.807069\n0.1\n\n\n3225\nPR\nYauco, PR\n34062.0\n67.711484\nNaN\nNaN\nNaN\nNaN\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 10 columns\n\n\n\nThe variables in the dataset are:\n\nstate: US state\ncounty: US county\npop_20: Population estimate for 2020\narea_sq_miles: Area in square miles\nhh_inc_21: Median household income for 2021\necon_type: Economic type of the county\npop_change_2010_2020: Population change between 2010 and 2020 (%)\nunemp_20: Unemployment rate for 2020 (%)\npct_emp_change_2010_2021: Percentage change in employment between 2010 and 2021 (%)\nforeign_born_num: Number of foreign-born residents\n\nLet’s create a small subset of the dataset with just the area and population columns for illustration.\n\n# Small subset for illustration\narea_df = counties[[\"county\", \"area_sq_miles\", \"pop_20\"]]\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n\n\n\n\n3226 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#creating-new-variables",
    "href": "p_untangled_transform_variables.html#creating-new-variables",
    "title": "16  Transforming Variables in pandas",
    "section": "16.5 Creating New Variables",
    "text": "16.5 Creating New Variables\nSuppose we want to convert the area from square miles to square kilometers. Since 1 square mile is approximately 2.59 square kilometers, we can create a new variable area_sq_km by multiplying the area_sq_miles column by 2.59.\n\narea_df[\"area_sq_km\"] = area_df[\"area_sq_miles\"] * 2.59\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n\n\n\n\n3226 rows × 4 columns\n\n\n\nThe syntax is very easy to understand, although a bit hard to type.\nWith area_df[\"area_sq_km\"], we’re indicating that we want to create a new column called area_sq_km, then area_df[\"area_sq_miles\"] * 2.59 is the expression that computes the values for this new column.\nLet’s add another variable, this time in hectares. The conversion factor is 1 square mile = 259 hectares.\n\n# Convert area to hectares as well\narea_df[\"area_hectares\"] = area_df[\"area_sq_miles\"] * 259\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.5.1 Practice Q: Area in Acres\nUsing the area_df dataset, create a new column called area_acres by multiplying the area_sq_miles variable by 640. Store the result back into area_df and display the DataFrame.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#modifying-existing-variables",
    "href": "p_untangled_transform_variables.html#modifying-existing-variables",
    "title": "16  Transforming Variables in pandas",
    "section": "16.6 Modifying Existing Variables",
    "text": "16.6 Modifying Existing Variables\nSuppose we want to round the area_sq_km variable to one decimal place. We can call the round method on the area_sq_km column.\n\narea_df[\"area_sq_km\"] = area_df[\"area_sq_km\"].round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.6.1 Practice Q: Rounding area_acres\nUsing the area_df dataset, round the area_acres variable to one decimal place. Update the DataFrame area_df and display it.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#calculations-with-multiple-variables",
    "href": "p_untangled_transform_variables.html#calculations-with-multiple-variables",
    "title": "16  Transforming Variables in pandas",
    "section": "16.7 Calculations with Multiple Variables",
    "text": "16.7 Calculations with Multiple Variables\nWe can create new variables based on multiple existing variables.\nFor example, let’s calculate the population density per square kilometer.\n\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_20\"] / area_df[\"area_sq_km\"]\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.241751\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.618986\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.335664\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.196123\n\n\n\n\n3226 rows × 6 columns\n\n\n\nWe could tag on the round method to this output to round the result to one decimal place.\n\narea_df[\"pop_per_sq_km\"] = (area_df[\"pop_20\"] / area_df[\"area_sq_km\"]).round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.2\n\n\n\n\n3226 rows × 6 columns\n\n\n\nOr, if you prefer, you can do this in two steps:\n\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_20\"] / area_df[\"area_sq_km\"]\narea_df[\"pop_per_sq_km\"] = area_df[\"pop_per_sq_km\"].round(1)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n194.2\n\n\n\n\n3226 rows × 6 columns\n\n\n\nAfter calculating the population density, we might want to sort the DataFrame based on this new variable. Let’s sort in descending order.\n\n# Sort by population density in descending order\narea_df = area_df.sort_values(\"pop_per_sq_km\", ascending=False)\narea_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n1863\nNew York, NY\n22.656266\n1687834.0\n58.7\n5867.972888\n28753.6\n\n\n1856\nKings, NY\n69.376570\n2727393.0\n179.7\n17968.531752\n15177.5\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n98\nWrangell-Petersburg, AK\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2921\nBedford, VA\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3226 rows × 6 columns\n\n\n\nWe see that New York County has the highest population density in the dataset.\n\n\n\n\n\n\nPractice\n\n\n\n16.7.1 Practice Q: Calculate Foreign-Born Percentage\nUse the counties dataset to calculate the percentage of foreign-born residents in each county. The variable foreign_born_num shows the number of foreign-born residents and pop_20 shows the total population. Sort the DataFrame in descending order of the percentage of foreign-born residents. Which two counties have the highest percentage of foreign-born residents?\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#creating-boolean-variables",
    "href": "p_untangled_transform_variables.html#creating-boolean-variables",
    "title": "16  Transforming Variables in pandas",
    "section": "16.8 Creating Boolean Variables",
    "text": "16.8 Creating Boolean Variables\nIt is sometimes useful to create Boolean variables to categorize or flag data based on conditions. Boolean variables are variables that take on only two values: True or False.\nConsider the pop_change_2010_2020 variable in the counties dataset, which shows the percentage change in population between 2010 and 2020.\n\nchanges_df = counties[[\"county\", \"pop_change_2010_2020\", \"pct_emp_change_2010_2021\"]]\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\n\n\n1\nBaldwin, AL\n27.159356\n28.2\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 3 columns\n\n\n\nWe might want to create a Boolean variable to flag whether the population increased. For this, let’s set the pop_increase variable to True if the population increased and False otherwise.\nRunning the expression changes_df[\"pop_change_2010_2020\"] &gt; 0 returns a Series of Boolean values:\n\nchanges_df[\"pop_change_2010_2020\"] &gt; 0\n\n0        True\n1        True\n        ...  \n3224    False\n3225    False\nName: pop_change_2010_2020, Length: 3226, dtype: bool\n\n\nWe can assign this Series of Boolean values to the pop_increase variable.\n\nchanges_df[\"pop_increase\"] = changes_df[\"pop_change_2010_2020\"] &gt; 0\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\n\n\n\n\n3226 rows × 4 columns\n\n\n\nSimilarly, we can create a Boolean variable emp_increase for employment change.\n\nchanges_df[\"emp_increase\"] = changes_df[\"pct_emp_change_2010_2021\"] &gt; 0\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\nTrue\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\nFalse\n\n\n\n\n3226 rows × 5 columns\n\n\n\nWe can now filter the DataFrame to find counties where the population increased but employment decreased.\n\n# Counties where population increased but employment decreased\npop_up_emp_down = changes_df.query(\"pop_increase == True & emp_increase == False\")\npop_up_emp_down\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n71\nBethel, AK\n9.716099\n-0.7\nTrue\nFalse\n\n\n75\nDillingham, AK\n0.206313\n-16.1\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n3127\nCampbell, WY\n1.935708\n-14.8\nTrue\nFalse\n\n\n3137\nNatrona, WY\n5.970842\n-0.2\nTrue\nFalse\n\n\n\n\n242 rows × 5 columns\n\n\n\nYou could also write this in shorthand like so:\n\n# Counties where population increased but employment decreased\npop_up_emp_down = changes_df.query(\"pop_increase & ~(emp_increase)\")\npop_up_emp_down\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increase\nemp_increase\n\n\n\n\n71\nBethel, AK\n9.716099\n-0.7\nTrue\nFalse\n\n\n75\nDillingham, AK\n0.206313\n-16.1\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n3127\nCampbell, WY\n1.935708\n-14.8\nTrue\nFalse\n\n\n3137\nNatrona, WY\n5.970842\n-0.2\nTrue\nFalse\n\n\n\n\n242 rows × 5 columns\n\n\n\nThere are several such counties, which might be of interest for further analysis.\n\n\n\n\n\n\nPractice\n\n\n\n16.8.1 Practice Q: Categorize Counties by Foreign-Born Population\nIn a previous practice question, we calculated the percentage of foreign-born residents in each county. Now, create a Boolean variable foreign_born_pct_gt_30 that is True if the percentage is greater than 30%.\nWhen you’re done, query the DataFrame to show only counties where foreign_born_pct_gt_30 is True. You should get 24 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#the-copy-on-write-warning",
    "href": "p_untangled_transform_variables.html#the-copy-on-write-warning",
    "title": "16  Transforming Variables in pandas",
    "section": "16.9 The Copy-on-Write Warning",
    "text": "16.9 The Copy-on-Write Warning\nEarlier in this lesson, we enabled “copy-on-write” mode. Let’s see what happens when this feature is disabled.\n\npd.set_option(\"mode.copy_on_write\", False)\n\n# Create a small subset of our data\nsubset = counties.query(\"state == 'AL'\")\n\nsubset\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\n58877.0\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n65\nAL\nWilcox, AL\n10518.0\n887.857611\n30071.0\nManufacturing\n16.2\n36.0\n-9.168809\n3.8\n\n\n66\nAL\nWinston, AL\n23491.0\n612.998002\n47176.0\nManufacturing\n5.2\n408.0\n-3.855579\n18.1\n\n\n\n\n67 rows × 10 columns\n\n\n\nWhen we attempt to modify the subset, we receive a warning:\n\n# Modify the subset\nsubset['unemp_20'] = subset['unemp_20'].round(0)\n\n/var/folders/vr/shb6ffvj2rl61kh7qqczhrgh0000gp/T/ipykernel_74258/317403666.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nWhile we won’t delve deeply into the technical details of this warning (as it involves complex pandas internals), it’s worth noting that the warning includes a link to the pandas documentation. This documentation contains the setting we used at the beginning of our lesson.\nIf you ever need to reference this setting again, you can simply click the link in the warning message to access the documentation. The documentation page also provides more detailed information about this particular issue.\nAlso note that from Pandas 3.0 (probably to be released in 2025) this warning will be removed, as the default behavior will be to copy on write.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_transform_variables.html#wrap-up",
    "href": "p_untangled_transform_variables.html#wrap-up",
    "title": "16  Transforming Variables in pandas",
    "section": "16.10 Wrap-Up",
    "text": "16.10 Wrap-Up\nTransforming data is a fundamental step in any data analysis workflow. pandas makes it straightforward to create and modify variables within your DataFrames using simple and intuitive syntax.\nIn this lesson, you’ve learned how to:\n\nCreate new variables by assigning to new columns.\nModify existing variables.\nPerform calculations involving multiple variables.\nCreate Boolean variables based on conditions.\n\nCongratulations on completing this lesson!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Variables in pandas</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html",
    "href": "p_untangled_recode_variables.html",
    "title": "17  Conditional Transformations of Variables",
    "section": "",
    "text": "17.1 Introduction\nIn the previous lesson, you learned the basics of data transformation in pandas.\nIn this lesson, we will explore how to conditionally transform variables in pandas using methods like replace(), dictionaries, and custom functions. Conditional transformations are essential when you need to recode variables or create new ones based on specific criteria.\nLet’s dive in!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#learning-objectives",
    "href": "p_untangled_recode_variables.html#learning-objectives",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.2 Learning Objectives",
    "text": "17.2 Learning Objectives\nBy the end of this lesson, you will:\n\nBe able to transform or create new variables based on conditions using replace() and dictionaries.\nKnow how to handle NaN values in replace() transformations.\nBe able to define and apply custom functions to recode variables.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#packages",
    "href": "p_untangled_recode_variables.html#packages",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.3 Packages",
    "text": "17.3 Packages\nThis lesson will require pandas, numpy, plotly.express, and vega_datasets:\n\nimport pandas as pd\nimport numpy as np\nimport vega_datasets as vd\nimport plotly.express as px",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#introduction-to-replace",
    "href": "p_untangled_recode_variables.html#introduction-to-replace",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.4 Introduction to replace()",
    "text": "17.4 Introduction to replace()\nOne common task in data wrangling is to replace values in a column based on certain conditions. The replace() method in pandas is a versatile tool for this purpose.\n\n17.4.1 Example: Expanding Day Names\nIn the tips dataset, the day column contains abbreviated day names:\n\ntips = px.data.tips()\ntips['day'].unique()\n\narray(['Sun', 'Sat', 'Thur', 'Fri'], dtype=object)\n\n\nOur goal is to replace these abbreviations with the full day names.\nWe can create a mapping dictionary that maps the abbreviated names to the full names:\n\nday_mapping = {\n    \"Sun\": \"Sunday\",\n    \"Sat\": \"Saturday\",\n    \"Fri\": \"Friday\",\n    \"Thur\": \"Thursday\"\n}\n\nNow, we use the replace() method with the mapping dictionary:\n\ntips['day_full'] = tips['day'].replace(day_mapping)\ntips[['day', 'day_full']].head()\n\n\n\n\n\n\n\n\nday\nday_full\n\n\n\n\n0\nSun\nSunday\n\n\n1\nSun\nSunday\n\n\n2\nSun\nSunday\n\n\n3\nSun\nSunday\n\n\n4\nSun\nSunday\n\n\n\n\n\n\n\nAlternatively, we can perform the replacement directly within the replace() method without explicitly defining a dictionary:\n\ntips['day_full'] = tips['day'].replace({\n    \"Sun\": \"Sunday\",\n    \"Sat\": \"Saturday\",\n    \"Fri\": \"Friday\",\n    \"Thur\": \"Thursday\"\n})\ntips[['day', 'day_full']].head()\n\n\n\n\n\n\n\n\nday\nday_full\n\n\n\n\n0\nSun\nSunday\n\n\n1\nSun\nSunday\n\n\n2\nSun\nSunday\n\n\n3\nSun\nSunday\n\n\n4\nSun\nSunday\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.4.2 Practice Q: Abbreviate Gender\nUsing the tips dataset, replace the values in the sex column to abbreviate gender:\n\nReplace \"Female\" with \"F\".\nReplace \"Male\" with \"M\".\n\nAssign the result to a new column called sex_abbr and display the first few rows.\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#handling-missing-values-with-replace",
    "href": "p_untangled_recode_variables.html#handling-missing-values-with-replace",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.5 Handling Missing Values with replace()",
    "text": "17.5 Handling Missing Values with replace()\nSometimes, your dataset may contain missing values (NaN or None), and you may want to replace them with a placeholder or specific value.\n\n17.5.1 Example: Replacing Missing Values in the Movies Dataset\nLet’s examine the Creative_Type column in the movies dataset from vega_datasets:\n\nmovies = vd.data.movies()\nmovies['Creative_Type'].value_counts(dropna=False)\n\nCreative_Type\nContemporary Fiction       1453\nNone                        446\nHistorical Fiction          350\n                           ... \nFactual                      49\nSuper Hero                   49\nMultiple Creative Types       1\nName: count, Length: 10, dtype: int64\n\n\nYou’ll notice that there are missing values (represented as None in counts). Suppose we decide to replace None and \"Multiple Creative Types\" with \"Unknown/Unclear\".\nWe can do this using replace():\n\nmovies['Creative_Type'] = movies['Creative_Type'].replace({\n    None: \"Unknown/Unclear\",\n    \"Multiple Creative Types\": \"Unknown/Unclear\"\n})\n\nNow, let’s verify the replacement:\n\nmovies['Creative_Type'].value_counts(dropna=False)\n\nCreative_Type\nContemporary Fiction    1453\nUnknown/Unclear          447\nHistorical Fiction       350\n                        ... \nKids Fiction             141\nFactual                   49\nSuper Hero                49\nName: count, Length: 9, dtype: int64\n\n\n\n\n17.5.2 Practice: Standardize MPAA Ratings\nIn the movies dataset, the MPAA_Rating column contains movie ratings. Some entries are None or \"Not Rated\". Replace these values with \"Unrated\".\nThen, use value_counts() to see how many movies are unrated. There should be approximately 699 movies in this category.\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#complex-conditions",
    "href": "p_untangled_recode_variables.html#complex-conditions",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.6 Complex Conditions",
    "text": "17.6 Complex Conditions\nFor more complex conditional transformations that cannot be handled with replace(), you can define custom functions and apply them to your DataFrame.\n\n17.6.1 Example: Flagging Movies Based on Gross\nSuppose we want to flag movies as “US action movie” or “Global action movie” if they are superhero movies, based on their US and worldwide gross earnings.\n\nIf the US gross and worldwide gross are the same (indicating sales were only in the US), the movie is flagged as a US action movie.\nIf the worldwide gross is greater than the US gross, the movie is flagged as a global action movie.\nOtherwise, the movie is categorized as Other.\n\nWe can define a funcion that takes in three arguments and returns the appropriate flag:\n\n# Define the function to flag movies based on the conditions\ndef flag_movie(movie_type, us, worldwide):\n    if movie_type == 'Super Hero' and us == worldwide:\n        return 'US action movie'\n    elif movie_type == 'Super Hero' and worldwide &gt; us:\n        return 'Global action movie'\n    return 'Other'\n\nLet’s test it out with a few sets of values:\n\nflag_movie(movie_type='Super Hero', us=100, worldwide=100)\nflag_movie(movie_type='Super Hero', us=100, worldwide=200)\nflag_movie(movie_type='Comedy', us=100, worldwide=100)\n\n'Other'\n\n\nNow that it works, recall that we need to vectorize this function to apply it to the entire column.\n\nflag_movie_vec = np.vectorize(flag_movie)\n\nNow we can apply it to the entire column:\n\nmovies['Action_Flag'] = flag_movie_vec(movies['Creative_Type'], movies['US_Gross'], movies['Worldwide_Gross'])\nmovies\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\nAction_Flag\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nUnknown/Unclear\nNone\nNaN\n6.1\n1071.0\nOther\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nUnknown/Unclear\nNone\nNaN\n6.9\n207.0\nOther\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nUnknown/Unclear\nNone\nNaN\n6.8\n865.0\nOther\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3198\nZoom\n11989328.0\n12506188.0\n6679409.0\n35000000.0\nAug 11 2006\nPG\nNaN\nSony Pictures\nBased on Comic/Graphic Novel\nAdventure\nSuper Hero\nPeter Hewitt\n3.0\n3.4\n7424.0\nGlobal action movie\n\n\n3199\nThe Legend of Zorro\n45575336.0\n141475336.0\nNaN\n80000000.0\nOct 28 2005\nPG\n129.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n26.0\n5.7\n21161.0\nOther\n\n\n3200\nThe Mask of Zorro\n93828745.0\n233700000.0\nNaN\n65000000.0\nJul 17 1998\nPG-13\n136.0\nSony Pictures\nRemake\nAdventure\nHistorical Fiction\nMartin Campbell\n82.0\n6.7\n4789.0\nOther\n\n\n\n\n3201 rows × 17 columns\n\n\n\nTo see the distribution of movie categories based on our flag, we can use value_counts():\n\nmovies['Action_Flag'].value_counts(dropna=False)\n\nAction_Flag\nOther                  3152\nGlobal action movie      42\nUS action movie           7\nName: count, dtype: int64\n\n\n\n\n17.6.2 Practice: Flag Movies Based on Ratings\nIn the movies dataset, flag movies as Critic-friendly or Commercial based on their Rotten Tomatoes and IMDB ratings.\n\nIf the Rotten Tomatoes rating is above 80% and the IMDB rating is below 5, the movie is flagged as Critic-friendly.\nIf the Rotten Tomatoes rating is below 50% and the IMDB rating is above 8, the movie is flagged as Commercial.\nOtherwise, the movie is categorized as Other.\n\n\n# Your code here:",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_recode_variables.html#wrap-up",
    "href": "p_untangled_recode_variables.html#wrap-up",
    "title": "17  Conditional Transformations of Variables",
    "section": "17.7 Wrap-Up",
    "text": "17.7 Wrap-Up\nIn this lesson, you learned how to conditionally transform variables in pandas using:\n\nThe replace() method with dictionaries to map and replace specific values.\nHandling missing values (NaN or None) during replacements.\nDefining custom functions and applying them to handle complex conditions.\n\nThese techniques are powerful tools for data cleaning and preprocessing, allowing you to reshape your data to meet your analysis needs.\nSee you next time!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Conditional Transformations of Variables</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html",
    "href": "p_untangled_groupby_agg.html",
    "title": "18  Grouping and summarizing data",
    "section": "",
    "text": "18.1 Introduction\nIn this lesson, we’ll explore two powerful pandas methods: agg() and groupby(). These tools will enable you to extract summary statistics and perform operations on grouped data effortlessly.\nLet’s dive in and discover how to unlock deeper insights from your data!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#learning-objectives",
    "href": "p_untangled_groupby_agg.html#learning-objectives",
    "title": "18  Grouping and summarizing data",
    "section": "18.2 Learning objectives",
    "text": "18.2 Learning objectives\n\nYou can use pandas.DataFrame.agg() to extract summary statistics from datasets.\nYou can use pandas.DataFrame.groupby() to group data by one or more variables before performing operations on them.\nYou can use sum() together with groupby()-agg() to count rows that meet a condition.\nYou can pass custom functions to agg() to compute summary statistics.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#libraries",
    "href": "p_untangled_groupby_agg.html#libraries",
    "title": "18  Grouping and summarizing data",
    "section": "18.3 Libraries",
    "text": "18.3 Libraries\nRun the following lines to import the necessary libraries:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "title": "18  Grouping and summarizing data",
    "section": "18.4 The Yaounde COVID-19 dataset",
    "text": "18.4 The Yaounde COVID-19 dataset\nIn this lesson, we will again use data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\n\nyaounde = pd.read_csv('data/yaounde_data.csv')\n\n## A smaller subset of variables\nyao = yaounde[['age', 'age_category_3', 'sex', 'weight_kg', 'height_cm',\n               'neighborhood', 'is_smoker', 'is_pregnant', 'occupation',\n               'treatment_combinations', 'symptoms', 'n_days_miss_work', 'n_bedridden_days',\n               'highest_education', 'igg_result']]\n\nyao\n\n\n\n\n\n\n\n\nage\nage_category_3\nsex\nweight_kg\nheight_cm\nneighborhood\nis_smoker\nis_pregnant\noccupation\ntreatment_combinations\nsymptoms\nn_days_miss_work\nn_bedridden_days\nhighest_education\nigg_result\n\n\n\n\n0\n45\nAdult\nFemale\n95\n169\nBriqueterie\nNon-smoker\nNo\nInformal worker\nParacetamol\nMuscle pain\n0.0\n0.0\nSecondary\nNegative\n\n\n1\n55\nAdult\nMale\n96\n185\nBriqueterie\nEx-smoker\nNaN\nSalaried worker\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nPositive\n\n\n2\n23\nAdult\nMale\n74\n180\nBriqueterie\nSmoker\nNaN\nStudent\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nAdult\nMale\n77\n168\nTsinga Oliga\nSmoker\nNaN\nInformal worker\nParacetamol\nHeadache\n0.0\n0.0\nUniversity\nPositive\n\n\n969\n31\nAdult\nFemale\n66\n169\nTsinga Oliga\nNon-smoker\nNo\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n970\n17\nChild\nFemale\n67\n162\nTsinga Oliga\nNon-smoker\nNo response\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n\n\n971 rows × 15 columns\n\n\n\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#what-are-summary-statistics",
    "href": "p_untangled_groupby_agg.html#what-are-summary-statistics",
    "title": "18  Grouping and summarizing data",
    "section": "18.5 What are summary statistics?",
    "text": "18.5 What are summary statistics?\nA summary statistic is a single value (such as a mean or median) that describes a sequence of values (typically a column in your dataset).\nComputing summary statistics is a very common operation in most data analysis workflows, so it will be important to become fluent in extracting them from your datasets. And for this task, there is no better tool than the pandas method agg()! So let’s see how to use this powerful method.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "href": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "title": "18  Grouping and summarizing data",
    "section": "18.6 Introducing pandas.DataFrame.agg()",
    "text": "18.6 Introducing pandas.DataFrame.agg()\nTo get started, let’s consider how to get simple summary statistics without using agg(), then we will consider why you should actually use agg().\nImagine you were asked to find the mean age of respondents in the yao data frame. You can do this by calling the mean() method on the age column of the yao data frame:\n\nyao[[\"age\"]].mean()\n\nage    29.017508\ndtype: float64\n\n\nAnd that’s it! You now have a simple summary statistic. Extremely easy, right?\nSo why do we need agg() to get summary statistics if the process is already so simple without it? We’ll come back to the why question soon. First let’s see how to obtain summary statistics with agg().\nGoing back to the previous example, the correct syntax to get the mean age with agg() would be:\n\nyao.agg(mean_age=('age', 'mean'))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\n\n\n\n\n\nThe anatomy of this syntax is:\n\ndataframe.agg(new_column_name=(\"COLUMN_TO_SUMMARIZE\", \"SUMMARY_FUNCTION\"))\n\n\nYou can also compute multiple summary statistics in a single agg() statement. For example, if you wanted both the mean and the median age, you could run:\n\nyao.agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\nmedian_age\n26.000000\n\n\n\n\n\n\n\nNice! Try your hand at the practice question below.\n\n\n\n\n\n\nPractice\n\n\n\n18.6.1 Practice Q: Mean and median weight\nUse agg() and the relevant summary functions to obtain the mean and median of respondent weights from the weight_kg variable of the yao data frame.\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.6.2 Practice Q: Min and max height\nUse agg() and the relevant summary functions to obtain the minimum and maximum respondent heights from the height_cm variable of the yao data frame. You may need to use a search engine to find out the minimum and maximum functions in Python if you don’t remember them.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "href": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "title": "18  Grouping and summarizing data",
    "section": "18.7 Grouped summaries with pandas.DataFrame.groupby()",
    "text": "18.7 Grouped summaries with pandas.DataFrame.groupby()\nNow let’s see how to use groupby() to obtain grouped summaries, the primary reason for using agg() in the first place.\nAs its name suggests, pandas.DataFrame.groupby() lets you group a data frame by the values in a variable (e.g. male vs female sex). You can then perform operations that are split according to these groups.\nLet’s try to group the yao data frame by sex and observe the effect:\n\nyao.groupby(\"sex\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x121d36000&gt;\n\n\nHmm. Apparently nothing happened. We just get a GroupBy object.\nBut watch what happens when we chain the groupby() with the agg() call we used in the previous section:\n\n(\n    yao.groupby(\"sex\")\n    .agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n)\n\n\n\n\n\n\n\n\nmean_age\nmedian_age\n\n\nsex\n\n\n\n\n\n\nFemale\n29.495446\n26.0\n\n\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\nNow we get a different statistic for each group! The mean age for female respondents is about 29.5, while for male respondents it’s about 28.4.\nAs was mentioned earlier, this kind of grouped summary is the primary reason the agg() function is so useful.\nYou may notice that there are two header rows. This is because the output has a hierarchical index (called a MultiIndex in pandas). While this can be useful in some cases, it often makes further data manipulation more difficult. We can reset the index to convert the group labels back to a regular column with the reset_index() method.\n\n(\n    yao.groupby(\"sex\")\n    .agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nmean_age\nmedian_age\n\n\n\n\n0\nFemale\n29.495446\n26.0\n\n\n1\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\n\nLet’s see some more examples.\nSuppose you were asked to obtain the maximum and minimum weights for individuals in different neighborhoods, and also present the number of individuals in each neighborhood. We can write:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        max_weight=(\"weight_kg\", \"max\"),\n        min_weight=(\"weight_kg\", \"min\"),\n        count=(\"neighborhood\", \"size\"),  # the size function counts rows per group\n    )\n    .reset_index()\n) \n\n\n\n\n\n\n\n\nneighborhood\nmax_weight\nmin_weight\ncount\n\n\n\n\n0\nBriqueterie\n128\n20\n106\n\n\n1\nCarriere\n129\n14\n236\n\n\n2\nCité Verte\n118\n16\n72\n\n\n...\n...\n...\n...\n...\n\n\n6\nNkomkana\n161\n15\n75\n\n\n7\nTsinga\n105\n15\n81\n\n\n8\nTsinga Oliga\n100\n17\n67\n\n\n\n\n9 rows × 4 columns\n\n\n\nGreat! With just a few code lines you are able to extract quite a lot of information.\n\nLet’s see one more example for good measure. The variable n_days_miss_work tells us the number of days that respondents missed work due to COVID-like symptoms. Individuals who reported no COVID-like symptoms have an NA for this variable:\n\nyao[['n_days_miss_work']]\n\n\n\n\n\n\n\n\nn_days_miss_work\n\n\n\n\n0\n0.0\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n...\n...\n\n\n968\n0.0\n\n\n969\nNaN\n\n\n970\nNaN\n\n\n\n\n971 rows × 1 columns\n\n\n\nTo count the total number of work days missed for each sex group, we can write:\n\n(\n    yao.groupby(\"sex\")\n    .agg(total_days_missed=(\"n_days_miss_work\", \"sum\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\ntotal_days_missed\n\n\n\n\n0\nFemale\n256.0\n\n\n1\nMale\n272.0\n\n\n\n\n\n\n\nThe output tells us that across all women in the sample, 256 work days were missed due to COVID-like symptoms, and across all men, 272 days.\n\nHopefully now you see why agg() is so powerful. In combination with groupby(), it lets you obtain highly informative grouped summaries of your datasets with very few lines of code.\nProducing such summaries is a very important part of most data analysis workflows, so this skill is likely to come in handy soon!\n\n\n\n\n\n\nPractice\n\n\n\n18.7.1 Practice Q: Min and max height per sex\nUse groupby(), agg(), and the relevant summary functions to obtain the minimum and maximum heights for each sex in the yao data frame, as well as the number of individuals in each sex group.\nYour output should be a DataFrame that looks like this:\n\n\n\nsex\nmin_height_cm\nmax_height_cm\ncount\n\n\n\n\nFemale\n\n\n\n\n\nMale\n\n\n\n\n\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\nGroupby and agg are not always needed\nSometimes you just want a quick summary of a variable, not per group.\nRemember that there are quick ways to get such statistics. So you do not always need to use groupby() and agg().\n\nyao['age'].describe() # summary stats for a numeric variable\n\ncount    971.000000\nmean      29.017508\nstd       17.340397\n            ...    \n50%       26.000000\n75%       39.000000\nmax       79.000000\nName: age, Length: 8, dtype: float64\n\n\n\nyao['sex'].describe() # summary stats for a categorical variable\n\ncount        971\nunique         2\ntop       Female\nfreq         549\nName: sex, dtype: object\n\n\n\nyao['sex'].value_counts() # count of each category\n\nsex\nFemale    549\nMale      422\nName: count, dtype: int64\n\n\n\nyao.select_dtypes(object).describe() # summary stats for categorical variables\n\n\n\n\n\n\n\n\nage_category_3\nsex\nneighborhood\nis_smoker\nis_pregnant\noccupation\ntreatment_combinations\nsymptoms\nhighest_education\nigg_result\n\n\n\n\ncount\n971\n971\n971\n969\n549\n971\n262\n971\n971\n971\n\n\nunique\n3\n2\n9\n3\n3\n28\n31\n122\n7\n2\n\n\ntop\nAdult\nFemale\nCarriere\nNon-smoker\nNo\nStudent\nTraditional meds.\nNo symptoms\nSecondary\nNegative\n\n\nfreq\n635\n549\n236\n859\n464\n383\n57\n675\n433\n669\n\n\n\n\n\n\n\n\nyao.select_dtypes(np.number).describe() # summary stats for numeric variables\n\n\n\n\n\n\n\n\nage\nweight_kg\nheight_cm\nn_days_miss_work\nn_bedridden_days\n\n\n\n\ncount\n971.000000\n971.000000\n971.000000\n296.000000\n295.000000\n\n\nmean\n29.017508\n64.405767\n158.944387\n1.783784\n1.145763\n\n\nstd\n17.340397\n23.053370\n18.776356\n4.583006\n2.525201\n\n\n...\n...\n...\n...\n...\n...\n\n\n50%\n26.000000\n66.000000\n164.000000\n0.000000\n0.000000\n\n\n75%\n39.000000\n78.000000\n170.000000\n2.000000\n2.000000\n\n\nmax\n79.000000\n162.000000\n196.000000\n45.000000\n30.000000\n\n\n\n\n8 rows × 5 columns\n\n\n\n\n\n\n\n\n\n\n\nWatch-out\n\n\n\n18.8 NaN values in agg()\nWhen using agg() to compute grouped summary statistics, pay attention to whether your group of interest contains NaN values.\nFor example, to get mean weight by smoking status, we can write:\n\n(\n    yao.groupby(\"is_smoker\")\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n\n\n\n\n\nBut this actually excludes some rows with NaN smoking status from the summary table.\nWe can include these individuals in the summary table by setting dropna=False with the groupby() function.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n3\nNaN\n73.000000\n\n\n\n\n\n\n\nAlso recall that you can see how many individuals are in each smoking status group by using the size() function. It is often useful to include this information in your summary table, so that you know how many individuals are behind each summary statistic.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"), \n         count=(\"is_smoker\", \"size\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\ncount\n\n\n\n\n0\nEx-smoker\n76.366197\n71\n\n\n1\nNon-smoker\n63.033760\n859\n\n\n2\nSmoker\n72.410256\n39\n\n\n3\nNaN\n73.000000\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.8.1 Practice Q: Mean weight by pregnancy status\nUse groupby(), agg(), and the mean() function to obtain the mean weight (kg) by pregnancy status in the yao data frame. Include individuals with NaN pregnancy status in the summary table.\nThe output data frame should look something like this:\n\n\n\nis_pregnant\nweight_mean\n\n\n\n\nNo\n\n\n\nNo response\n\n\n\nYes\n\n\n\nNaN\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "href": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "title": "18  Grouping and summarizing data",
    "section": "18.8 NaN values in agg()",
    "text": "18.8 NaN values in agg()\nWhen using agg() to compute grouped summary statistics, pay attention to whether your group of interest contains NaN values.\nFor example, to get mean weight by smoking status, we can write:\n\n(\n    yao.groupby(\"is_smoker\")\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n\n\n\n\n\nBut this actually excludes some rows with NaN smoking status from the summary table.\nWe can include these individuals in the summary table by setting dropna=False with the groupby() function.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n3\nNaN\n73.000000\n\n\n\n\n\n\n\nAlso recall that you can see how many individuals are in each smoking status group by using the size() function. It is often useful to include this information in your summary table, so that you know how many individuals are behind each summary statistic.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"), \n         count=(\"is_smoker\", \"size\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\ncount\n\n\n\n\n0\nEx-smoker\n76.366197\n71\n\n\n1\nNon-smoker\n63.033760\n859\n\n\n2\nSmoker\n72.410256\n39\n\n\n3\nNaN\n73.000000\n2",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "href": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "title": "18  Grouping and summarizing data",
    "section": "18.9 Grouping by multiple variables (nested grouping)",
    "text": "18.9 Grouping by multiple variables (nested grouping)\nIt is possible to group a data frame by more than one variable. This is sometimes called “nested” grouping.\nSuppose you want to know the mean age of men and women in each neighbourhood, you could put both sex and neighborhood in the groupby() statement:\n\n(\n    yao\n    .groupby(['sex', 'neighborhood'])\n    .agg(mean_age=('age', 'mean'))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nneighborhood\nmean_age\n\n\n\n\n0\nFemale\nBriqueterie\n31.622951\n\n\n1\nFemale\nCarriere\n28.164286\n\n\n2\nFemale\nCité Verte\n31.750000\n\n\n...\n...\n...\n...\n\n\n15\nMale\nNkomkana\n29.812500\n\n\n16\nMale\nTsinga\n28.820513\n\n\n17\nMale\nTsinga Oliga\n24.297297\n\n\n\n\n18 rows × 3 columns\n\n\n\nFrom this output data frame you can tell that, for example, women from Briqueterie have a mean age of 31.6 years.\nThe order of the columns listed in groupby() is interchangeable. So if you run groupby(['neighborhood', 'sex']) instead of groupby(['sex', 'neighborhood']), you’ll get the same result, although it will be ordered differently:\n\n(\n    yao\n    .groupby(['neighborhood', 'sex'])\n    .agg(mean_age=('age', 'mean'))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nsex\nmean_age\n\n\n\n\n0\nBriqueterie\nFemale\n31.622951\n\n\n1\nBriqueterie\nMale\n33.711111\n\n\n2\nCarriere\nFemale\n28.164286\n\n\n...\n...\n...\n...\n\n\n15\nTsinga\nMale\n28.820513\n\n\n16\nTsinga Oliga\nFemale\n24.266667\n\n\n17\nTsinga Oliga\nMale\n24.297297\n\n\n\n\n18 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.9.1 Practice Q: Total bedridden days per age-sex group\nUse groupby(), agg(), and the sum() function to calculate the total number of bedridden days (from the n_bedridden_days variable) reported by respondents of each age-sex group. For age group, use the age_category_3 variable.\nYour output should be a data frame with three columns named as shown below:\n\n\n\nage_category_3\nsex\ntotal_bedridden_days\n\n\n\n\nAdult\nFemale\n\n\n\nAdult\nMale\n\n\n\nChild\nFemale\n\n\n\nChild\nMale\n\n\n\nSenior\nFemale\n\n\n\nSenior\nMale\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#custom-summary-statistics",
    "href": "p_untangled_groupby_agg.html#custom-summary-statistics",
    "title": "18  Grouping and summarizing data",
    "section": "18.10 Custom summary statistics",
    "text": "18.10 Custom summary statistics\nSometimes, you may want to apply custom summary statistics that aren’t available as built-in functions. In these cases, you can use lambda functions or define your own functions to use with agg().\nFor example, let’s say we want to calculate the range (difference between maximum and minimum) of weights in each neighborhood:\n\n(\n    yao\n    .groupby('neighborhood')\n    .agg(weight_range=('weight_kg', lambda x: x.max() - x.min()))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nweight_range\n\n\n\n\n0\nBriqueterie\n108\n\n\n1\nCarriere\n115\n\n\n2\nCité Verte\n102\n\n\n...\n...\n...\n\n\n6\nNkomkana\n146\n\n\n7\nTsinga\n90\n\n\n8\nTsinga Oliga\n83\n\n\n\n\n9 rows × 2 columns\n\n\n\nNotice that we still provide a tuple to the agg() function, ('weight_kg', lambda x: x.max() - x.min()), but the second element of the tuple is a lambda function.\nThis lambda function operates on the column provided in the tuple, weight_kg.\n(Unlike the lambda functions we saw in the assign() method, we do not need to access the column within the lambda with x.weight_kg.max() - x.weight_kg.min(). We simply use x.max() - x.min().)\n\n\n\n\n\n\nPractice\n\n\n\n18.10.1 Practice Q: IQR of age by neighborhood\nFind the interquartile range (IQR) of the age variable for each neighborhood. The IQR is the difference between the 75th and 25th percentiles. Your lambda will look like this: lambda x: x.quantile(0.75) - x.quantile(0.25)\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#counting-rows-that-meet-a-condition",
    "href": "p_untangled_groupby_agg.html#counting-rows-that-meet-a-condition",
    "title": "18  Grouping and summarizing data",
    "section": "18.11 Counting rows that meet a condition",
    "text": "18.11 Counting rows that meet a condition\nIt is sometimes useful to count the rows that meet specific conditions within a group. This can be done with the groupby and agg functions.\nFor example, to count the number of people under 18 in each neighborhood, we can write:\n\n(\n    yao\n    .groupby('neighborhood')\n    .agg(num_children=('age', lambda x: (x &lt; 18).sum()))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nnum_children\n\n\n\n\n0\nBriqueterie\n28\n\n\n1\nCarriere\n58\n\n\n2\nCité Verte\n19\n\n\n...\n...\n...\n\n\n6\nNkomkana\n22\n\n\n7\nTsinga\n23\n\n\n8\nTsinga Oliga\n25\n\n\n\n\n9 rows × 2 columns\n\n\n\nThe lambda function (x &lt; 18).sum() counts the number of values in the x series that are less than 18. The x series is the age variable for each neighborhood.\nLet’s broaden our scope and add the number of seniors (65+) to the summary table.\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        num_children=(\"age\", lambda x: (x &lt; 18).sum()),\n        num_seniors=(\"age\", lambda x: (x &gt;= 65).sum())\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nnum_children\nnum_seniors\n\n\n\n\n0\nBriqueterie\n28\n9\n\n\n1\nCarriere\n58\n9\n\n\n2\nCité Verte\n19\n4\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n22\n6\n\n\n7\nTsinga\n23\n5\n\n\n8\nTsinga Oliga\n25\n0\n\n\n\n\n9 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#calculate-percentage-shares",
    "href": "p_untangled_groupby_agg.html#calculate-percentage-shares",
    "title": "18  Grouping and summarizing data",
    "section": "18.12 Calculate percentage shares",
    "text": "18.12 Calculate percentage shares\nWe can expand our previous example to calculate the percentage of children and seniors in each neighborhood.\nCalculating such percentage shares is a common task in data analysis.\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        children_percent=(\"age\", lambda x: (x &lt; 18).sum() / len(x) * 100),\n        seniors_percent=(\"age\", lambda x: (x &gt;= 65).sum() / len(x) * 100),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nchildren_percent\nseniors_percent\n\n\n\n\n0\nBriqueterie\n26.415094\n8.490566\n\n\n1\nCarriere\n24.576271\n3.813559\n\n\n2\nCité Verte\n26.388889\n5.555556\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n29.333333\n8.000000\n\n\n7\nTsinga\n28.395062\n6.172840\n\n\n8\nTsinga Oliga\n37.313433\n0.000000\n\n\n\n\n9 rows × 3 columns\n\n\n\nHopefully the code above is not too confusing. To calculate the percentage of children, we divide the number of children (x &lt; 18).sum() by the total number of individuals in the series, len(x), and multiply by 100. Likewise for seniors.\nA more concise approach utilizes the fact that the mean of a boolean series represents the proportion of True values:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        children_percent=(\"age\", lambda x: (x &lt; 18).mean() * 100),\n        seniors_percent=(\"age\", lambda x: (x &gt;= 65).mean() * 100),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nchildren_percent\nseniors_percent\n\n\n\n\n0\nBriqueterie\n26.415094\n8.490566\n\n\n1\nCarriere\n24.576271\n3.813559\n\n\n2\nCité Verte\n26.388889\n5.555556\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n29.333333\n8.000000\n\n\n7\nTsinga\n28.395062\n6.172840\n\n\n8\nTsinga Oliga\n37.313433\n0.000000\n\n\n\n\n9 rows × 3 columns\n\n\n\nWhile this method is more efficient, it may be less intuitive. If you find it confusing, feel free to use the more explicit calculation shown earlier.\n\nAs a final example, to count the number and percentage of people with doctorate degrees in each neighborhood, we can write:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        count_w_doctorates=(\"highest_education\", lambda x: (x == \"Doctorate\").sum()),\n        pct_in_neighborhood_w_doctorates=(\n            \"highest_education\",\n            lambda x: (x == \"Doctorate\").mean() * 100,\n        ),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\ncount_w_doctorates\npct_in_neighborhood_w_doctorates\n\n\n\n\n0\nBriqueterie\n2\n1.886792\n\n\n1\nCarriere\n1\n0.423729\n\n\n2\nCité Verte\n1\n1.388889\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n4\n5.333333\n\n\n7\nTsinga\n3\n3.703704\n\n\n8\nTsinga Oliga\n3\n4.477612\n\n\n\n\n9 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.12.1 Practice Q: Smoker rate by sex and age category\nCalculate the percentage of people with are smokers (use the is_smoker variable) for each combination of sex and age category (use age_category_3). The output should look like this:\n\n\n\nsex\nage_category_3\nsmoker_rate\n\n\n\n\nFemale\nAdult\n\n\n\nFemale\nChild\n\n\n\nFemale\nSenior\n\n\n\nMale\nAdult\n\n\n\nMale\nChild\n\n\n\nMale\nSenior\n\n\n\n\nYour summary table should show that male adults have the highest smoker rate, followed by male seniors.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#wrap-up",
    "href": "p_untangled_groupby_agg.html#wrap-up",
    "title": "18  Grouping and summarizing data",
    "section": "18.13 Wrap up",
    "text": "18.13 Wrap up\nIn this lesson, you’ve learned how to obtain quick summary statistics from your data using agg(), group your data using groupby(), and combine groupby() with agg() for powerful data summarization.\nThese skills are essential for both exploratory data analysis and preparing data for presentation or plotting. The combination of groupby() and agg() is one of the most common and useful data manipulation techniques in pandas.\nIn our next lesson, we’ll explore ways to combine groupby() with other pandas methods.\nSee you there!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html",
    "href": "p_untangled_other_grouped_operations.html",
    "title": "19  Grouped Transformations and Filtering",
    "section": "",
    "text": "19.1 Introduction to Grouped Operations\nData wrangling often involves applying the same operations separately to different groups within the data. In this lesson, we’ll learn how to use groupby() with transform() and assign() to conduct grouped transformations on a DataFrame.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#learning-objectives",
    "href": "p_untangled_other_grouped_operations.html#learning-objectives",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.2 Learning Objectives",
    "text": "19.2 Learning Objectives\n\nYou can use groupby() with transform() and assign() to conduct grouped operations on a DataFrame.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#required-packages",
    "href": "p_untangled_other_grouped_operations.html#required-packages",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.3 Required Packages",
    "text": "19.3 Required Packages\nThis lesson requires pandas and numpy:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#simple-toy-dataset",
    "href": "p_untangled_other_grouped_operations.html#simple-toy-dataset",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.4 Simple Toy Dataset",
    "text": "19.4 Simple Toy Dataset\nLet’s start with a simple toy dataset to illustrate the concepts:\n\n# Create a simple toy dataset\ntoy_data = pd.DataFrame(\n    {\n        \"year\": [2022, 2022, 2023, 2023],\n        \"month\": [\"Jan\", \"Feb\", \"Jan\", \"Feb\"],\n        \"sales\": [2, 4, 6, 8],\n    }\n)\n\nprint(toy_data)\n\n   year month  sales\n0  2022   Jan      2\n1  2022   Feb      4\n2  2023   Jan      6\n3  2023   Feb      8\n\n\n\n19.4.1 Regular assign() Call\nLet’s add a column with the mean sales for the entire dataset, then calculate the difference between each month’s sales and the mean sales:\n\ntoy_data.assign(\n    mean_sales=lambda x: x.sales.mean(), \n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2022\nJan\n2\n5.0\n-3.0\n\n\n1\n2022\nFeb\n4\n5.0\n-1.0\n\n\n2\n2023\nJan\n6\n5.0\n1.0\n\n\n3\n2023\nFeb\n8\n5.0\n3.0\n\n\n\n\n\n\n\nIn this example, the mean_sales column contains the mean sales value for the entire dataset (5), repeated in each row. The sales_diff column shows the difference between each month’s sales and the overall mean sales.\n\n\n19.4.2 Grouped assign() Call\nBut we might want to see how each month’s sales compare to the mean sales for that year.\n\ntoy_data.assign(\n    mean_sales=lambda x: x.groupby(\"year\").sales.mean(),\n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2022\nJan\n2\nNaN\nNaN\n\n\n1\n2022\nFeb\n4\nNaN\nNaN\n\n\n2\n2023\nJan\n6\nNaN\nNaN\n\n\n3\n2023\nFeb\n8\nNaN\nNaN\n\n\n\n\n\n\n\nNow we have the mean sales for year 2022 as 3 and for year 2023 as 7. and the sales_diff column shows how far each month’s sales are from the mean sales for that year.\nLet’s break down the code, especially the first line:\n\nWithin the assign method, we initiate the mean_sales column\nThen we pass the lambda function to the mean_sales column\nThe lambda function takes the dataframe, denoted x as input, groups it by year (x.groupby(\"year\")), then pulls out the sales column (x.groupby(\"year\").sales) then to compute the mean for each year, we use the transform() method (x.groupby(\"year\").sales.transform(\"mean\")).",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#real-dataset-austin-housing",
    "href": "p_untangled_other_grouped_operations.html#real-dataset-austin-housing",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.5 Real Dataset: Austin Housing",
    "text": "19.5 Real Dataset: Austin Housing\nNow let’s apply these concepts to a real dataset. We’ll use the housing dataset containing housing sales data for Austin, Texas:\n\nhousing = pd.read_csv(\"data/austinhousing.csv\", usecols=[\"year\", \"month\", \"sales\"])\n\n## Display first few rows of the dataset\nhousing\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\n\n\n\n\n0\n2000\n1\n1025\n\n\n1\n2000\n2\n1277\n\n\n2\n2000\n3\n1603\n\n\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n\n\n185\n2015\n6\n3301\n\n\n186\n2015\n7\n3466\n\n\n\n\n187 rows × 3 columns\n\n\n\n\n19.5.1 Regular assign() Call\nLet’s add a column with the mean sales for the entire dataset, then calculate the difference between each month’s sales and the mean sales:\n\nhousing.assign(\n    mean_sales=lambda x: x.sales.mean(), \n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2000\n1\n1025\n1996.68984\n-971.68984\n\n\n1\n2000\n2\n1277\n1996.68984\n-719.68984\n\n\n2\n2000\n3\n1603\n1996.68984\n-393.68984\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n1996.68984\n1002.31016\n\n\n185\n2015\n6\n3301\n1996.68984\n1304.31016\n\n\n186\n2015\n7\n3466\n1996.68984\n1469.31016\n\n\n\n\n187 rows × 5 columns\n\n\n\nIn this example, the mean_sales column contains the mean sales value for the entire dataset, repeated in each row. The sales_diff column shows the difference between each month’s sales and the overall mean sales.\n\n\n19.5.2 Grouped assign() Call\nNow, let’s add a column with the mean sales for each year, and then calculate the difference between each month’s sales and the mean sales for that year:\n\nhousing.assign(\n    mean_sales=lambda x: x.groupby(\"year\").sales.transform(\"mean\"),\n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2000\n1\n1025\n1551.750000\n-526.750000\n\n\n1\n2000\n2\n1277\n1551.750000\n-274.750000\n\n\n2\n2000\n3\n1603\n1551.750000\n51.250000\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n2696.857143\n302.142857\n\n\n185\n2015\n6\n3301\n2696.857143\n604.142857\n\n\n186\n2015\n7\n3466\n2696.857143\n769.142857\n\n\n\n\n187 rows × 5 columns\n\n\n\nHere’s what’s happening: - Within the assign() method, we group by year and use transform(\"mean\") to compute the mean sales for each year. - The mean_sales column now contains the mean sales for each year, repeated in each row of that year. - The sales_diff column shows how far each month’s sales are from the mean sales for that year, allowing us to see which months are high and low performers within each year.\nThis grouped approach allows us to compare sales within each year, accounting for overall trends or differences between years.\n\n\n\n\n\n\nPractice\n\n\n\n19.5.3 Practice Q: Compute Mean Temperature for Each Day\nWith the gibraltar dataframe, group by the date of measurement, then in a new variable called mean_temp, compute the mean temperature for each day. Store the result in a new dataframe called mean_temp_question.\n\n## Complete the code with your answer:\nmean_temp_question = gibraltar.assign(\n    mean_temp=___________________________________-\n)\n\nmean_temp_question\n\n\n\n\n\n19.5.4 More Examples of Grouped Transformations\n\n19.5.4.1 Rank Sales Within Each Year\nNow let’s look at another example. We can add a column that ranks the sales volume for each year.\n\nhousing.assign(sales_rank=housing.groupby(\"year\").sales.rank(ascending=False))\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nsales_rank\n\n\n\n\n0\n2000\n1\n1025\n12.0\n\n\n1\n2000\n2\n1277\n10.0\n\n\n2\n2000\n3\n1603\n5.0\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n3.0\n\n\n185\n2015\n6\n3301\n2.0\n\n\n186\n2015\n7\n3466\n1.0\n\n\n\n\n187 rows × 4 columns\n\n\n\nIn this example, we’re creating a new column sales_rank that ranks the sales column in descending order within each year group. This allows us to see which months had the highest sales for each year.\nThese examples demonstrate how we can use grouped operations to perform more nuanced analyses on our data, allowing us to uncover patterns and insights within specific subgroups of our dataset.\n\n\n19.5.4.2 Cumulative Sales Column\nLet’s add a column that shows the cumulative sales for each year, adding up the sales for each month within the year.\n\nhousing.assign(\n    cumulative_sales=housing.groupby(\"year\").sales.transform(\"cumsum\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\ncumulative_sales\n\n\n\n\n0\n2000\n1\n1025\n1025\n\n\n1\n2000\n2\n1277\n2302\n\n\n2\n2000\n3\n1603\n3905\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n12111\n\n\n185\n2015\n6\n3301\n15412\n\n\n186\n2015\n7\n3466\n18878\n\n\n\n\n187 rows × 4 columns\n\n\n\n\n\n19.5.4.3 Percent of Yearly Sales Done Each Month\nWe can calculate the percent of sales done each month for that year.\n\nhousing.assign(\n    percent_sales=lambda x: 100 * x.sales / x.groupby(\"year\").sales.transform(\"sum\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\npercent_sales\n\n\n\n\n0\n2000\n1\n1025\n5.504538\n\n\n1\n2000\n2\n1277\n6.857849\n\n\n2\n2000\n3\n1603\n8.608560\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n15.886217\n\n\n185\n2015\n6\n3301\n17.485962\n\n\n186\n2015\n7\n3466\n18.359996\n\n\n\n\n187 rows × 4 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.5.5 Practice Q: Cumulative Listings Column\nWith the housing dataframe, group by year, then create a cumulative listings column for each year.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#grouped-filtering-with-query",
    "href": "p_untangled_other_grouped_operations.html#grouped-filtering-with-query",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.6 Grouped Filtering with query()",
    "text": "19.6 Grouped Filtering with query()\nTo query per group, it is usually better to create a relevant column with groupby() and transform(), then use query() on the resulting dataframe to filter data.\n\n19.6.1 Filter for Month with Highest Sales Volume per Year\nFor example, if we want to filter the data for the month with the highest sales volume per year group (the month with the highest sales volume for each year), we can use groupby() with transform() to first create a column with the max sales volume per year, then ungroup and filter.\n\n(\nhousing\n  .assign(max_sales=housing.groupby(\"year\").sales.transform(\"max\"))\n  .query(\"sales == max_sales\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmax_sales\n\n\n\n\n4\n2000\n5\n1980\n1980\n\n\n18\n2001\n7\n1871\n1871\n\n\n28\n2002\n5\n1931\n1931\n\n\n...\n...\n...\n...\n...\n\n\n162\n2013\n7\n3376\n3376\n\n\n173\n2014\n6\n3195\n3195\n\n\n186\n2015\n7\n3466\n3466\n\n\n\n\n16 rows × 4 columns\n\n\n\nWe can drop the max_sales column if we want to at the end.\n\n(\nhousing\n  .assign(max_sales=housing.groupby(\"year\").sales.transform(\"max\"))\n  .query(\"sales == max_sales\")\n  .drop(columns=[\"max_sales\"])\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\n\n\n\n\n4\n2000\n5\n1980\n\n\n18\n2001\n7\n1871\n\n\n28\n2002\n5\n1931\n\n\n...\n...\n...\n...\n\n\n162\n2013\n7\n3376\n\n\n173\n2014\n6\n3195\n\n\n186\n2015\n7\n3466\n\n\n\n\n16 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.6.2 Practice Q: Filter for Time of Day with Highest Wind Speed\nWith the gibraltar dataframe created above, group by the date of measurement, calculate the time of day with the highest recorded wind speed wind_speed for each day, then filter the dataframe to keep only the rows with the highest wind speed for each day.\nNote that for some days, there may be a tie in the highest wind speed.\n\n# Your code here\n\n\n\n\n\n19.6.3 Month with the Most “Typical” Sales\nWe can also use groupby() with transform() to find the month with the most “typical” sales. We’ll use the mean sales volume for each year as the “typical” sales volume, then get the month with the closest sales to that.\n\n(\nhousing\n  .assign(\n    mean_sales=housing.groupby(\"year\").sales.transform(\"mean\"),\n    sales_diff_abs=lambda x: (x.sales - x.mean_sales).abs()\n    )\n  .sort_values(\"sales_diff_abs\")\n  .groupby(\"year\")\n  .first()\n)\n\n\n\n\n\n\n\n\nmonth\nsales\nmean_sales\nsales_diff_abs\n\n\nyear\n\n\n\n\n\n\n\n\n2000\n4\n1556\n1551.750000\n4.250000\n\n\n2001\n3\n1553\n1532.666667\n20.333333\n\n\n2002\n3\n1550\n1559.666667\n9.666667\n\n\n...\n...\n...\n...\n...\n\n\n2013\n9\n2544\n2536.333333\n7.666667\n\n\n2014\n10\n2588\n2579.416667\n8.583333\n\n\n2015\n3\n2677\n2696.857143\n19.857143\n\n\n\n\n16 rows × 4 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n19.6.4 Practice Q: Filter for Times with Above Mean Temperature\nGroup the gibraltar dataframe by date, then use apply(), assign(), and query() as needed to subset to only times when the temperature is above the mean temperature for that day. You should have 698 rows in the output.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#wrap-up",
    "href": "p_untangled_other_grouped_operations.html#wrap-up",
    "title": "19  Grouped Transformations and Filtering",
    "section": "19.7 Wrap Up",
    "text": "19.7 Wrap Up\nIn this lesson, we explored powerful techniques for grouped operations in pandas:\n\nUsing groupby() with transform() and assign() for grouped transformations\nCombining groupby(), transform(), and query() for grouped filtering\n\nThese methods significantly enhance our ability to analyze and manipulate data within specific groups.\nCongratulations on making it through!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html",
    "href": "p_untangled_pivoting.html",
    "title": "20  Reshaping data",
    "section": "",
    "text": "20.1 Intro\nPivoting or reshaping is a data manipulation technique that involves re-orienting the rows and columns of a dataset. This is often required to make data easier to analyze or understand.\nIn this lesson, we will cover how to effectively pivot data using pandas functions.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#learning-objectives",
    "href": "p_untangled_pivoting.html#learning-objectives",
    "title": "20  Reshaping data",
    "section": "20.2 Learning Objectives",
    "text": "20.2 Learning Objectives\n\nUnderstand what wide data format is, and what long data format is.\nLearn how to pivot long data to wide data using melt().\nLearn how to pivot wide data to long data using pivot().",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "href": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "title": "20  Reshaping data",
    "section": "20.3 What do wide and long mean?",
    "text": "20.3 What do wide and long mean?\nThe terms wide and long are best understood in the context of example datasets. Let’s take a look at some now.\nImagine that you have three products for which you collect sales data over three months.\nYou can record the data in a wide format like this:\n\n\n\nProduct\nJan\nFeb\nMar\n\n\n\n\nA\n100\n120\n110\n\n\nB\n90\n95\n100\n\n\nC\n80\n85\n90\n\n\n\n\nOr you could record the data in a long format as so:\n\n\n\nProduct\nMonth\nSales\n\n\n\n\nA\nJan\n100\n\n\nA\nFeb\n120\n\n\nA\nMar\n110\n\n\nB\nJan\n90\n\n\nB\nFeb\n95\n\n\nB\nMar\n100\n\n\nC\nJan\n80\n\n\nC\nFeb\n85\n\n\nC\nMar\n90\n\n\n\nTake a minute to study the two datasets to make sure you understand the relationship between them.\nIn the wide dataset, each observational unit (each product) occupies only one row. And each measurement (sales in Jan, Feb, Mar) is in a separate column.\nIn the long dataset, on the other hand, each observational unit (each product) occupies multiple rows, with one row for each measurement.\n\nHere is another example with mock data, in which the observational units are countries:\nLong format:\n\n\n\nCountry\nYear\nGDP\n\n\n\n\nUSA\n2020\n21433\n\n\nUSA\n2021\n22940\n\n\nChina\n2020\n14723\n\n\nChina\n2021\n17734\n\n\n\nWide format:\n\n\n\nCountry\nGDP_2020\nGDP_2021\n\n\n\n\nUSA\n21433\n22940\n\n\nChina\n14723\n17734\n\n\n\n\nThe examples above are both time-series datasets, because the measurements are repeated across time. But the concepts of long and wide are relevant to other kinds of data too.\nConsider the example below, showing the number of employees in different departments of three companies:\nWide format:\n\n\n\nCompany\nHR\nSales\nIT\n\n\n\n\nA\n10\n20\n15\n\n\nB\n8\n25\n20\n\n\nC\n12\n18\n22\n\n\n\nLong format:\n\n\n\nCompany\nDepartment\nEmployees\n\n\n\n\nA\nHR\n10\n\n\nA\nSales\n20\n\n\nA\nIT\n15\n\n\nB\nHR\n8\n\n\nB\nSales\n25\n\n\nB\nIT\n20\n\n\nC\nHR\n12\n\n\nC\nSales\n18\n\n\nC\nIT\n22\n\n\n\nIn the wide dataset, again, each observational unit (each company) occupies only one row, with the repeated measurements for that unit (number of employees in different departments) spread across multiple columns.\nIn the long dataset, each observational unit is spread over multiple lines.\n\n\n\n\n\n\nVocab\n\n\n\nThe “observational units”, sometimes called “statistical units” of a dataset are the primary entities or items described by the columns in that dataset.\nIn the first example, the observational/statistical units were products; in the second example, countries, and in the third example, companies.\n\n\n\n\n\n\n\n\nPractice\n\n\n\nConsider the mock dataset created below:\n\nimport pandas as pd\n\ntemperatures = pd.DataFrame(\n    {\n        \"country\": [\"Sweden\", \"Denmark\", \"Norway\"],\n        \"avgtemp.1994\": [1, 2, 3],\n        \"avgtemp.1995\": [3, 4, 5],\n        \"avgtemp.1996\": [5, 6, 7],\n    }\n)\ntemperatures\n\n\n\n\n\n\n\n\ncountry\navgtemp.1994\navgtemp.1995\navgtemp.1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7\n\n\n\n\n\n\n\nIs this data in a wide or long format?\n\n# Write your answer here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#when-should-you-use-wide-vs-long-data",
    "href": "p_untangled_pivoting.html#when-should-you-use-wide-vs-long-data",
    "title": "20  Reshaping data",
    "section": "20.4 When should you use wide vs long data?",
    "text": "20.4 When should you use wide vs long data?\nThe truth is: it really depends on what you want to do! The wide format is great for displaying data because it’s easy to visually compare values this way. Long data is best for some data analysis tasks, like grouping and plotting.\nIt will therefore be essential for you to know how to switch from one format to the other easily. Switching from the wide to the long format, or the other way around, is called pivoting.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-wide-to-long",
    "href": "p_untangled_pivoting.html#pivoting-wide-to-long",
    "title": "20  Reshaping data",
    "section": "20.5 Pivoting wide to long",
    "text": "20.5 Pivoting wide to long\nTo practice pivoting from a wide to a long format, we’ll consider data from Our World in Data on fossil fuel consumption per capita. You can find the data here.\nBelow, we read in and view this data on fossil fuel consumption per capita:\n\nfuels_wide = pd.read_csv(\"data/oil_per_capita_wide.csv\")\nfuels_wide\n\n\n\n\n\n\n\n\nEntity\nCode\ny_1970\ny_1980\ny_1990\ny_2000\ny_2010\ny_2020\n\n\n\n\n0\nAlgeria\nDZA\n1764.8470\n3532.7976\n4381.6636\n3351.2180\n5064.9863\n4877.2680\n\n\n1\nArgentina\nARG\n11677.9680\n10598.3990\n7046.2485\n7146.8154\n7966.7827\n6399.2114\n\n\n2\nAustralia\nAUS\n23040.4550\n25007.4380\n23046.9510\n23976.3550\n23584.3070\n20332.4100\n\n\n3\nAustria\nAUT\n14338.8090\n19064.0920\n16595.1930\n18189.0920\n18424.1170\n14934.0650\n\n\n4\nAzerbaijan\nAZE\nNaN\nNaN\n13516.0190\n9119.3470\n4031.9407\n5615.1157\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nUnited States\nUSA\n40813.9530\n42365.6500\n37525.5160\n37730.1600\n31791.3070\n26895.4770\n\n\n77\nUzbekistan\nUZB\nNaN\nNaN\n6324.8677\n3197.1330\n1880.1338\n1859.1548\n\n\n78\nVenezuela\nVEN\n11138.2210\n16234.0960\n12404.5570\n11239.9260\n14948.3070\n4742.6226\n\n\n79\nVietnam\nVNM\n1757.6117\n439.9465\n523.2565\n1280.3065\n2296.7590\n2927.7446\n\n\n80\nWorld\nOWID_WRL\n7217.8340\n8002.0854\n7074.2583\n6990.4272\n6879.6110\n6216.8060\n\n\n\n\n81 rows × 8 columns\n\n\n\nWe observe that each observational unit (each country) occupies only one row, with the repeated measurements of fossil fuel consumption (in Kilowatt-hour equivalents) spread out across multiple columns. Hence this dataset is in a wide format.\nTo convert to a long format, we can use the convenient melt function. Within melt we define which columns we want to pivot:\n\n(fuels_wide\n .melt(id_vars=['Entity', 'Code'], \n       value_vars=fuels_wide.columns[2:])\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nvariable\nvalue\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n1\nArgentina\nARG\ny_1970\n11677.9680\n\n\n2\nAustralia\nAUS\ny_1970\n23040.4550\n\n\n3\nAustria\nAUT\ny_1970\n14338.8090\n\n\n4\nAzerbaijan\nAZE\ny_1970\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n481\nUnited States\nUSA\ny_2020\n26895.4770\n\n\n482\nUzbekistan\nUZB\ny_2020\n1859.1548\n\n\n483\nVenezuela\nVEN\ny_2020\n4742.6226\n\n\n484\nVietnam\nVNM\ny_2020\n2927.7446\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nVery easy!\nLet’s break down the code:\n\nid_vars refers to the column(s) that will not be pivoted. In this case, it’s the country and the code.\nvalue_vars is the column(s) that will be pivoted. In this case, it’s the years columns.\n\nThe years are now indicated in the variable variable, and all the consumption values occupy a single variable, value.We may wish to rename the variable column to year, and the value column to oil_consumption. This can be done directly in the melt function:\n\n(fuels_wide\n .melt(id_vars=['Entity', 'Code'], \n       value_vars=fuels_wide.columns[2:],\n       var_name='year', \n       value_name='oil_consumption')\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n1\nArgentina\nARG\ny_1970\n11677.9680\n\n\n2\nAustralia\nAUS\ny_1970\n23040.4550\n\n\n3\nAustria\nAUT\ny_1970\n14338.8090\n\n\n4\nAzerbaijan\nAZE\ny_1970\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n481\nUnited States\nUSA\ny_2020\n26895.4770\n\n\n482\nUzbekistan\nUZB\ny_2020\n1859.1548\n\n\n483\nVenezuela\nVEN\ny_2020\n4742.6226\n\n\n484\nVietnam\nVNM\ny_2020\n2927.7446\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nYou may also want to remove the y_ in front of each year. This can be achieved with as string operation. We’ll also arrange the data by country and year:\n\n(\n    fuels_wide.melt(\n        id_vars=[\"Entity\", \"Code\"],\n        value_vars=fuels_wide.columns[2:],\n        var_name=\"year\",\n        value_name=\"oil_consumption\",\n    )\n    .assign(year=lambda df: df[\"year\"].str.replace(\"y_\", \"\").astype(int))\n    .sort_values(by=[\"Entity\", \"year\"])\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\n1970\n1764.8470\n\n\n81\nAlgeria\nDZA\n1980\n3532.7976\n\n\n162\nAlgeria\nDZA\n1990\n4381.6636\n\n\n243\nAlgeria\nDZA\n2000\n3351.2180\n\n\n324\nAlgeria\nDZA\n2010\n5064.9863\n\n\n...\n...\n...\n...\n...\n\n\n161\nWorld\nOWID_WRL\n1980\n8002.0854\n\n\n242\nWorld\nOWID_WRL\n1990\n7074.2583\n\n\n323\nWorld\nOWID_WRL\n2000\n6990.4272\n\n\n404\nWorld\nOWID_WRL\n2010\n6879.6110\n\n\n485\nWorld\nOWID_WRL\n2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nHere’s what we added above:\n\nIn the assign function, we used a lambda function to replace the y_ in front of each year with an empty string, and then convert the year to an integer.\nWe used the sort_values function to sort the data by country and year.\n\nNow we have a clean, long dataset; great! For later use, let’s now store this data:\n\nfuels_long = (\n    fuels_wide.melt(\n        id_vars=[\"Entity\", \"Code\"],\n        value_vars=fuels_wide.columns[2:],\n        var_name=\"year\",\n        value_name=\"oil_consumption\",\n    )\n    .assign(year=lambda df: df[\"year\"].str.replace(\"y_\", \"\").astype(int))\n    .sort_values(by=[\"Entity\", \"year\"])\n)\n\n\n\n\n\n\n\nPractice\n\n\n\nFor this practice question, you will use the euro_births_wide dataset from Eurostat. It shows the annual number of births in 50 European countries:\n\neuro_births_wide = pd.read_csv(\"data/euro_births_wide.csv\")\neuro_births_wide\n\n\n\n\n\n\n\n\ncountry\nx2015\nx2016\nx2017\nx2018\nx2019\nx2020\nx2021\n\n\n\n\n0\nBelgium\n122274.0\n121896.0\n119690.0\n118319.0\n117695.0\n114350.0\n118349.0\n\n\n1\nBulgaria\n65950.0\n64984.0\n63955.0\n62197.0\n61538.0\n59086.0\n58678.0\n\n\n2\nCzechia\n110764.0\n112663.0\n114405.0\n114036.0\n112231.0\n110200.0\n111793.0\n\n\n3\nDenmark\n58205.0\n61614.0\n61397.0\n61476.0\n61167.0\n60937.0\n63473.0\n\n\n4\nGermany\n737575.0\n792141.0\n784901.0\n787523.0\n778090.0\n773144.0\n795492.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n45\nUkraine\n411781.0\n397037.0\n363987.0\n235.0\n232.0\nNaN\n212.0\n\n\n46\nArmenia\n41763.0\n40592.0\n37700.0\n335874.0\n308817.0\n293457.0\n271983.0\n\n\n47\nAzerbaijan\n166210.0\n159464.0\n144041.0\n36574.0\n36041.0\n36353.0\nNaN\n\n\n48\nGeorgia\n59249.0\n56569.0\n53293.0\n138982.0\n141179.0\n126571.0\n112284.0\n\n\n49\nNaN\nNaN\nNaN\nNaN\n51138.0\n48296.0\n46520.0\n45946.0\n\n\n\n\n50 rows × 8 columns\n\n\n\nThe data is in a wide format. Convert it to a long format data frame that has the following column names: “country”, “year” and “births_count”\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-long-to-wide",
    "href": "p_untangled_pivoting.html#pivoting-long-to-wide",
    "title": "20  Reshaping data",
    "section": "20.6 Pivoting long to wide",
    "text": "20.6 Pivoting long to wide\nNow you know how to pivot from wide to long with melt. How about going the other way, from long to wide? For this, you can use the pivot function.\nBut before we consider how to use this function to manipulate long data, let’s first consider where you’re likely to run into long data.\nWhile wide data tends to come from external sources (as we have seen above), long data on the other hand, is likely to be created by you while data wrangling, especially in the course of grouped aggregations.\nLet’s see an example of this now.\nWe will use a dataset of contracts granted by the city of Chicago in the years 2020 to 2023. You can find more information about the data here. Below we import the data and do some data processing to prepare it for analysis.\n\ncontracts_raw = pd.read_csv(\"data/chicago_contracts_20_23.csv\")\n\ncontracts = (\n    contracts_raw\n    .assign(year_of_contract=lambda df: pd.to_datetime(df[\"approval_date\"]).dt.year)\n    .reindex(columns=[\"year_of_contract\"] + list(contracts_raw.columns.drop(\"approval_date\")))\n)\n\ncontracts\n\n\n\n\n\n\n\n\nyear_of_contract\ndescription\ncontract_num\nrevision_num\nspecification_num\ncontract_type\nstart_date\nend_date\ndepartment\nvendor_name\nvendor_id\naddress_1\naddress_2\ncity\nstate\nzip\naward_amount\nprocurement_type\ncontract_pdf\n\n\n\n\n0\n2020\nLEASE\n24406\n32\n96136\nPROPERTY LEASE\nNaN\nNaN\nNaN\n8700 BUILDING LLC\n89123305A\n7300 S NARRAGANSETT\nNaN\nBEDFORD PARK\nIllinois\n60638\n321.1\nNaN\nNaN\n\n\n1\n2020\nDFSS-HHS-CS-CEL:\n113798\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nCATHOLIC CHARITIES OF THE ARCHDIOCESE OF CHICAGO\n102484615A\n1 E BANKS ST\nNaN\nCHICAGO\nIllinois\n60670\n17692515.0\nNaN\nNaN\n\n\n2\n2020\nDFSS-HHS-CS-CEL:\n113819\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nKIMBALL DAYCARE CENTER & KINDERGARTEN INC\n105458567Z\n1636-1638 N KIMBALL AVE\nNaN\nCHICAGO\nIllinois\n60647\n11461500.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n3\n2020\nDFSS-HHS-CS-CEL:\n113818\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nJUDAH INTERNATIONAL OUTREACH MINISTRIES, INC\n94219962X\n856 N PULASKI RD\nNaN\nCHICAGO\nIllinois\n60651\n2356515.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n4\n2020\nDFSS-HHS-CS-CEL:\n113820\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nMarillac St. Vincent Family Services Inc DBA S...\n97791861L\n212 S FRANCISCO AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60612\n3666015.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n28823\n2023\nDFSS-CORP-HL-PSH:\n220413\n3\n1221503\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nINNER VOICE INC.\n6231926M\n1621 W WALNUT ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60612\n0.0\nNaN\nNaN\n\n\n28824\n2023\nDFSS-CORP-YS-OST:\n253846\n0\n1247493\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nAFTER-SCHOOL MATTERS, INC.|CLEANED-UP\n72580818P\n66 E RANDOLPH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60601\n32000.0\nNaN\nNaN\n\n\n28825\n2023\nDFSS-IDHS-HL-INTHS:\n253843\n0\n1235949\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nBREAKTHROUGH URBAN MINISTRIES, INC.\n94722896V\n402 N ST LOUIS AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60624\n14400.0\nNaN\nNaN\n\n\n28826\n2023\nCDPH-RW-PA: ESS-HRSA PO 116685 CHICAGO HOUSE A...\n192085\n1\n1095441\nDELEGATE AGENCY\nNaN\nNaN\nDEPARTMENT OF HEALTH\nCHICAGO HOUSE & SOCIAL SERVICE AGENCY\n105470138T\n2229 S MICHIGAN AVE 304 EFT\nNaN\nCHICAGO\nIllinois\n60616\n-32025.2\nNaN\nNaN\n\n\n28827\n2023\nDFSS-HHS-CS-CEL:\n222199\n1\n1070196\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nALLISON'S INFANT & TODDLER CENTER INC\n62751817Z\n234 E 115TH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60628\n141923.0\nNaN\nNaN\n\n\n\n\n28828 rows × 19 columns\n\n\n\nEach row corresponds to one contract, and we have each contract’s id number, the year in which they were granted, the amount of the contract, and the vendor’s name and address, among other variables.\nNow, consider the following grouped summary of the contracts dataset, which shows the number of contracts by state of the vendor in each year:\n\ncontracts_summary = (\n    contracts.groupby([\"state\", \"year_of_contract\"]).size().reset_index(name=\"n\")\n)\n\ncontracts_summary\n\n\n\n\n\n\n\n\nstate\nyear_of_contract\nn\n\n\n\n\n0\nAlabama\n2020\n1\n\n\n1\nAlabama\n2021\n2\n\n\n2\nAlabama\n2022\n1\n\n\n3\nAlabama\n2023\n7\n\n\n4\nArizona\n2020\n3\n\n\n...\n...\n...\n...\n\n\n128\nWashington\n2021\n1\n\n\n129\nWisconsin\n2020\n18\n\n\n130\nWisconsin\n2021\n15\n\n\n131\nWisconsin\n2022\n17\n\n\n132\nWisconsin\n2023\n25\n\n\n\n\n133 rows × 3 columns\n\n\n\nThe output of this grouped operation is a quintessentially “long” dataset! Each observational unit (each contract type) occupies multiple rows (four rows per state, to be exact), with one row for each measurement (each year).\nSo, as you now see, long data often can arrive as an output of grouped summaries, among other data manipulations.\nNow, let’s see how to convert such long data into a wide format with pivot.\nThe code is quite straightforward:\n\n(\n    contracts_summary.pivot(\n        index=\"state\", columns=\"year_of_contract\", values=\"n\"\n    ).reset_index()\n)\n\n\n\n\n\n\n\nyear_of_contract\nstate\n2020\n2021\n2022\n2023\n\n\n\n\n0\nAlabama\n1.0\n2.0\n1.0\n7.0\n\n\n1\nArizona\n3.0\n1.0\n3.0\n2.0\n\n\n2\nArkansas\n1.0\nNaN\n1.0\nNaN\n\n\n3\nBritish Columbia\nNaN\n1.0\nNaN\nNaN\n\n\n4\nCalifornia\n36.0\n42.0\n43.0\n38.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n38\nTexas\n25.0\n24.0\n37.0\n28.0\n\n\n39\nVermont\nNaN\n1.0\nNaN\nNaN\n\n\n40\nVirginia\n4.0\n4.0\n7.0\n9.0\n\n\n41\nWashington\nNaN\n1.0\nNaN\nNaN\n\n\n42\nWisconsin\n18.0\n15.0\n17.0\n25.0\n\n\n\n\n43 rows × 5 columns\n\n\n\nAs you can see, pivot has three important arguments: index, columns, and values.\n\nindex defines which column(s) to use as the new index. In our case, it’s the state, since we want each row to represent one state.\ncolumns identifies which variable to use to define column names in the wide format. In our case, it’s the year of the contract. You can see that the years are now the column names.\nvalues specifies which values will become the core of the wide data format. In our case, it’s the number of contracts. data format.\n\nYou might also want to have the years be your primary observational/statistical unit, with each year occupying one row. This can be carried out similarly to the above example, but with year_of_contract as the index and state as the columns:\n\n(\n    contracts_summary.pivot(\n        index=\"year_of_contract\", columns=\"state\", values=\"n\"\n    ).reset_index()\n)\n\n\n\n\n\n\n\nstate\nyear_of_contract\nAlabama\nArizona\nArkansas\nBritish Columbia\nCalifornia\nCanada\nColorado\nConnecticut\nDelaware\n...\nOregon\nPennsylvania\nRhode Island\nSouth Carolina\nTennessee\nTexas\nVermont\nVirginia\nWashington\nWisconsin\n\n\n\n\n0\n2020\n1.0\n3.0\n1.0\nNaN\n36.0\n1.0\n6.0\n1.0\nNaN\n...\n5.0\n20.0\n1.0\n2.0\n2.0\n25.0\nNaN\n4.0\nNaN\n18.0\n\n\n1\n2021\n2.0\n1.0\nNaN\n1.0\n42.0\n1.0\n2.0\n5.0\nNaN\n...\nNaN\n24.0\nNaN\n3.0\n2.0\n24.0\n1.0\n4.0\n1.0\n15.0\n\n\n2\n2022\n1.0\n3.0\n1.0\nNaN\n43.0\n1.0\n7.0\n3.0\n1.0\n...\nNaN\n31.0\nNaN\n2.0\n3.0\n37.0\nNaN\n7.0\nNaN\n17.0\n\n\n3\n2023\n7.0\n2.0\nNaN\nNaN\n38.0\nNaN\n6.0\n5.0\n2.0\n...\nNaN\n37.0\nNaN\n1.0\n3.0\n28.0\nNaN\n9.0\nNaN\n25.0\n\n\n\n\n4 rows × 44 columns\n\n\n\nHere the unique observation units (our rows) are now the years (2021, 2022, 2023).\n\n\n\n\n\n\nPractice\n\n\n\nThe population dataset shows the populations of 219 countries over time.\nPivot this data into a wide format. Your answer should have 20 columns and 219 rows.\n\npopulation = pd.read_csv(\"data/tidyr_population.csv\")\npopulation\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4055\nZimbabwe\n2009\n12888918\n\n\n4056\nZimbabwe\n2010\n13076978\n\n\n4057\nZimbabwe\n2011\n13358738\n\n\n4058\nZimbabwe\n2012\n13724317\n\n\n4059\nZimbabwe\n2013\n14149648\n\n\n\n\n4060 rows × 3 columns\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-can-be-hard",
    "href": "p_untangled_pivoting.html#pivoting-can-be-hard",
    "title": "20  Reshaping data",
    "section": "20.7 Pivoting can be hard",
    "text": "20.7 Pivoting can be hard\nWe have mostly looked at very simple examples of pivoting here, but in the wild, pivoting can be very difficult to do accurately.\nWhen you run into such cases, we recommend looking at the official documentation of pivoting from the pandas team, as it is quite rich in examples.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#wrap-up",
    "href": "p_untangled_pivoting.html#wrap-up",
    "title": "20  Reshaping data",
    "section": "20.8 Wrap up",
    "text": "20.8 Wrap up\nCongratulations! You’ve mastered the art of reshaping data with pandas.\nYou now understand the differences between wide and long formats, and can skillfully use melt() and pivot() to transform your data as needed.\nRemember, there’s no universally “best” format – it depends on your specific analysis or visualization needs. With these skills, you’re now equipped to handle data in any shape it comes in. Keep practicing with different datasets to reinforce your learning!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  }
]