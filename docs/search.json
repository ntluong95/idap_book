[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with Python",
    "section": "",
    "text": "Introduction\nDRAFT book for the course “Introduction to Data Science with Python”\nThis book is a compilation of lesson notes for a 3-month online course offered by The GRAPH Courses. To access the lesson videos, exercise files, and online quizzes, please visit our website, thegraphcourses.org.\nThe GRAPH Courses is a project of the Global Research and Analyses for Public Health (GRAPH) Network, a non-profit organization dedicated to making code and data skills accessible through affordable live bootcamps and free self-paced courses.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Introduction to Data Science with Python",
    "section": "Contributors",
    "text": "Contributors\nWe are extremely grateful to the following individuals who have contributed to the development of these materials over several years:\nAmanda McKinley, Andree Valle Campos, Aziza Merzouki, Benedict Nguimbis, Bennour Hsin, Camille Beatrice Valera, Daniel Camara, Eduardo Araujo, Elton Mukonda, Guy Wafeu, Imad El Badisy, Imane Bensouda Korachi, Joy Vaz, Kene David Nwosu, Lameck Agasa, Laure Nguemo, Laure Vancauwenberghe, Matteo Franza, Michal Shrestha, Olivia Keiser, Sabina Rodriguez Velasquez, Sara Botero Mesa.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#partners-funders",
    "href": "index.html#partners-funders",
    "title": "Introduction to Data Science with Python",
    "section": "Partners & Funders",
    "text": "Partners & Funders\n\nUniversity of Geneva\nUniversity of Oxford\nWorld Health Organization\nGlobal Fund\nErnst Goehner Foundation",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html",
    "href": "p_foundations_google_colab.html",
    "title": "1  Introduction to Google Colab",
    "section": "",
    "text": "1.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#learning-objectives",
    "href": "p_foundations_google_colab.html#learning-objectives",
    "title": "1  Introduction to Google Colab",
    "section": "",
    "text": "Understand what Google Colab is and its advantages for data science and AI\nLearn how to access and navigate Google Colab\nCreate and manage notebooks in Google Colab\nRun Python code in Colab cells\nUse text cells for explanations and formatting\nImport and use pre-installed libraries for data analysis\nImport and use data to perform analysis\nShare Colab notebooks",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#introduction",
    "href": "p_foundations_google_colab.html#introduction",
    "title": "1  Introduction to Google Colab",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\nGoogle Collaboratory, or Colab for short, is a free online platform that allows you to work with Python or R code in your browser. It’s a great way to get started with Python, as you don’t have to install anything on your computer.\nSome limitations if you’re running heavy workload though. Can get timeout. But for beginner data analysts, it’s perfect and free.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-started-with-colab",
    "href": "p_foundations_google_colab.html#getting-started-with-colab",
    "title": "1  Introduction to Google Colab",
    "section": "1.3 Getting Started with Colab",
    "text": "1.3 Getting Started with Colab\n\nSearch for “Google Colab” in your browser\nUsually the first option. Currently, it’s colab.research.google.com, but it may change in the future.\nSign in with your Google account (create a Gmail account if you don’t have one)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "href": "p_foundations_google_colab.html#creating-and-managing-notebooks",
    "title": "1  Introduction to Google Colab",
    "section": "1.4 Creating and Managing Notebooks",
    "text": "1.4 Creating and Managing Notebooks\n\nNotebooks are the main way to organize work in Colab. They contain code cells and text cells.\nCreate a new notebook: File &gt; New Notebook\nRename your notebook for better organization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#working-with-code-cells",
    "href": "p_foundations_google_colab.html#working-with-code-cells",
    "title": "1  Introduction to Google Colab",
    "section": "1.5 Working with Code Cells",
    "text": "1.5 Working with Code Cells\n\nCode cells are where you write and execute Python code\nType 1 + 1 in a cell then run it\nRun a cell by clicking the play button or using keyboard shortcuts:\n\nCommand + Enter: Run the current cell\nShift + Enter: Run the current cell and create a new one below\n\nTry to get comfortable with keyboard shortcuts\nMay take a while to run the first time. See it’s using Python\nCan change runtime to R actually\nWhen you run a cell, the output is displayed below the cell\nTo see multiple outputs, explicitly print them",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#text-cells",
    "href": "p_foundations_google_colab.html#text-cells",
    "title": "1  Introduction to Google Colab",
    "section": "1.6 Text Cells",
    "text": "1.6 Text Cells\n\nUse text cells for explanations and titles\nThe toolbar makes formatting easy, but pay attention to the generated markdown",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#example-of-working-with-data",
    "href": "p_foundations_google_colab.html#example-of-working-with-data",
    "title": "1  Introduction to Google Colab",
    "section": "1.7 Example of working with data",
    "text": "1.7 Example of working with data\n\nClick on the files tab to see the sample_data folder\nImport the California housing test dataset:\n\nimport pandas\nhousing_data = pandas.read_csv(\"/content/sample_data/california_housing_test.csv\")\n\nView the dataset by typing housing_data in a cell and running it",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#practice",
    "href": "p_foundations_google_colab.html#practice",
    "title": "1  Introduction to Google Colab",
    "section": "1.8 Practice",
    "text": "1.8 Practice\nImport the train dataset and repeat the process",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "href": "p_foundations_google_colab.html#getting-data-from-your-drive",
    "title": "1  Introduction to Google Colab",
    "section": "1.9 Getting data from your Drive",
    "text": "1.9 Getting data from your Drive\n\nSearch for “[your city] housing filetype:csv” on Google\nDownload the file\nIn the files tab, click the button to mount your drive\nCreate a folder and upload the downloaded file\nImport it with pandas as before",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "href": "p_foundations_google_colab.html#where-is-your-notebook-saved",
    "title": "1  Introduction to Google Colab",
    "section": "1.10 Where is your notebook saved?",
    "text": "1.10 Where is your notebook saved?\n\nAll work is automatically saved to your Google Drive\nAccess your notebooks at drive.google.com in the “Colab Notebooks” folder",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#sharing-and-collaborating",
    "href": "p_foundations_google_colab.html#sharing-and-collaborating",
    "title": "1  Introduction to Google Colab",
    "section": "1.11 Sharing and Collaborating",
    "text": "1.11 Sharing and Collaborating\n\nShare notebooks with a link, giving viewer or editor access\nAccess notebooks later from your Google Drive\nDownload notebooks in various formats (ipynb, py)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_google_colab.html#conclusion",
    "href": "p_foundations_google_colab.html#conclusion",
    "title": "1  Introduction to Google Colab",
    "section": "1.12 Conclusion",
    "text": "1.12 Conclusion\nGoogle Colab provides a powerful, accessible platform for data science and AI projects. Its pre-configured environment, free access to hardware accelerators, and easy sharing features make it an excellent choice for beginners and experienced practitioners alike.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Google Colab</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html",
    "href": "p_foundations_coding_basics.html",
    "title": "2  Coding basics",
    "section": "",
    "text": "2.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#learning-objectives",
    "href": "p_foundations_coding_basics.html#learning-objectives",
    "title": "2  Coding basics",
    "section": "",
    "text": "You can write and use comments in Python (single-line and multi-line).\nYou know how to use Python as a calculator for basic arithmetic operations and understand the order of operations.\nYou can use the math library for more complex mathematical operations.\nYou understand how to use proper spacing in Python code to improve readability.\nYou can create, manipulate, and reassign variables of different types (string, int, float).\nYou can get user input and perform calculations with it.\nYou understand the basic rules and best practices for naming variables in Python.\nYou can identify and fix common errors related to variable usage and naming.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#introduction",
    "href": "p_foundations_coding_basics.html#introduction",
    "title": "2  Coding basics",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nIn this lesson, you will learn the basics of using Python.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “coding_basics.py” or “coding_basics.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#comments",
    "href": "p_foundations_coding_basics.html#comments",
    "title": "2  Coding basics",
    "section": "2.3 Comments",
    "text": "2.3 Comments\nComments are text that is ignored by Python. They are used to explain what the code is doing.\nYou use the symbol #, pronounced “hash” or “pound”, to start a comment. Anything after the # on the same line is ignored. For example:\n\n# Addition\n2 + 2\n\n4\n\n\nIf we just tried to write Addition above the code, it would cause an error:\n\nAddition\n2 + 2\n\nNameError: name 'Addition' is not defined\nWe can put the comment on the same line as the code, but it needs to come after the code.\n\n2 + 2  # Addition\n\n4\n\n\nTo write multiple lines of comments, you can either add more # symbols:\n\n# Addition\n# Add two numbers\n2 + 2\n\n4\n\n\nOr you can use triple quotes ''' or \"\"\":\n\n'''\nAddition:\nBelow we add two numbers\n'''\n2 + 2\n\n4\n\n\nOr:\n\n\"\"\"\nAddition:\nBelow we add two numbers\n\"\"\"\n2 + 2\n\n4\n\n\n\n\n\n\n\n\nVocab\n\n\n\nComment: A piece of text in your code that is ignored by Python. Comments are used to explain what the code is doing and are meant for human readers.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.3.1 Q: Commenting in Python\nWhich of the following code chunks are valid ways to comment code in Python?\n# add two numbers\n2 + 2\n2 + 2 # add two numbers\n''' add two numbers\n2 + 2\n# add two numbers 2 + 2\nCheck your answer by trying to run each code chunk.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#python-as-a-calculator",
    "href": "p_foundations_coding_basics.html#python-as-a-calculator",
    "title": "2  Coding basics",
    "section": "2.4 Python as a calculator",
    "text": "2.4 Python as a calculator\nAs you have already seen, Python works as a calculator in standard ways.\nBelow are some other examples of basic arithmetic operations:\n\n2 - 2 # two minus two\n\n0\n\n\n\n2 * 2  # two times two \n\n4\n\n\n\n2 / 2  # two divided by two\n\n1.0\n\n\n\n2 ** 2  # two raised to the power of two\n\n4\n\n\nThere are a few other operators you may come across. For example, % is the modulo operator, which returns the remainder of the division.\n\n10 % 3  # ten modulo three\n\n1\n\n\n// is the floor division operator, which divides then rounds down to the nearest whole number.\n\n10 // 3  # ten floor division three\n\n3\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.4.1 Q: Modulo and floor division\nGuess the result of the following code chunks then run them to check your answer:\n\n5 % 4\n\n\n5 // 4\n\n\n\n\n2.4.2 Order of operations\nPython obeys the standard PEMDAS order of operations (Parentheses, Exponents, Multiplication, Division, Addition, Subtraction).\nFor example, multiplication is evaluated before addition, so below the result is 6.\n\n2 + 2 * 2   \n\n6\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.4.3 Q: Evaluating arithmetic expressions\nWhich, if any, of the following code chunks will evaluate to 10?\n\n2 + 2 * 4\n\n\n6 + 2 ** 2\n\n\n\n\n\n2.4.4 Using the math library\nWe can also use the math library to do more complex mathematical operations. For example, we can use the math.sqrt function to calculate the square root of a number.\n\nimport math\nmath.sqrt(100)  # square root\n\n10.0\n\n\nOr we can use the math.log function to calculate the natural logarithm of a number.\n\nimport math\nmath.log(100)  # logarithm\n\n4.605170185988092\n\n\nmath.sqrt and math.log are examples of Python functions, where an argument (e.g., 100) is passed to the function to perform a calculation.\nWe will learn more about functions later.\n\n\n\n\n\n\nVocab\n\n\n\nFunction: A reusable block of code that performs a specific task. Functions often take inputs (called arguments) and return outputs.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n\n2.4.5 Q: Using the math library\nUsing the math library, calculate the square root of 81.\nWrite your code below and run it to check your answers:\n\n# Your code here\n\n\n\n2.4.6 Q: Describing the use of the random library\nConsider the following code, which generates a random number between 1 and 10:\n\nimport random\nrandom.randint(1, 10)\n\n3\n\n\nIn that code, identify the library, the function, and the argument(s) to the function.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#spacing-in-code",
    "href": "p_foundations_coding_basics.html#spacing-in-code",
    "title": "2  Coding basics",
    "section": "2.5 Spacing in code",
    "text": "2.5 Spacing in code\nGood spacing makes your code easier to read. In Python, two simple spacing practices can greatly improve your code’s readability: using blank lines and adding spaces around operators.\n\n2.5.1 Blank Lines\nUse blank lines to separate different parts of your code:\nFor example, consider the following code chunk:\n\n# Set up numbers\nx = 5\ny = 10\n# Perform calculation\nresult = x + y\n# Display result\nprint(result)\n\n15\n\n\nWe can add blank lines to separate the different parts of the code:\n\n# Set up numbers\nx = 5\ny = 10\n\n# Perform calculation\nresult = x + y\n\n# Display result\nprint(result)\n\n15\n\n\nBlank lines help organize your code into logical sections, similar to paragraphs in writing.\n\n\n2.5.2 Spaces around operators\nAdding spaces around mathematical operators improves readability:\n\n# Hard to read\nx=5+3*2\n\n# Easy to read\nx = 5 + 3 * 2\n\nWhen listing items, add a space after each comma:\n\n# Hard to read\nprint(1,2,3)\n\n# Easy to read\nprint(1, 2, 3)\n\nThis practice follows the convention in written English, where we put a space after a comma. It makes lists of items in your code easier to read.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#variables-in-python",
    "href": "p_foundations_coding_basics.html#variables-in-python",
    "title": "2  Coding basics",
    "section": "2.6 Variables in Python",
    "text": "2.6 Variables in Python\n\n2.6.1 Create a variable\nAs you have seen, to store a value for future use in Python, we assign it to a variable with the assignment operator, =.\n\nmy_var = 2 + 2  # assign the result of `2 + 2 ` to the variable called `my_var`\nprint(my_var)  # print my_var\n\n4\n\n\nNow that you’ve created the variable my_var, Python knows about it and will keep track of it during this Python session.\nYou can open your environment to see what variables you have created. This looks different depending on your IDE.\nSo what exactly is a variable? Think of it as a named container that can hold a value. When you run the code below:\n\nmy_var = 20\n\nyou are telling Python, “store the number 20 in a variable named ‘my_var’”.\nOnce the code is run, we would say, in Python terms, that “the value of variable my_var is 20”.\nTry to come up with a similar sentence for this code chunk:\n\nfirst_name = \"Joanna\"\n\nAfter we run this code, we would say, in Python terms, that “the value of the first_name variable is Joanna”.\n\n\n\n\n\n\nVocab\n\n\n\nA text value like “Joanna” is called a string, while a number like 20 is called an integer. If the number has a decimal point, it is called a float, which is short for “floating-point number”.\n\n\n\n\n\n\n\n\nVocab\n\n\n\nVariable: A named container that can hold a value. In Python, variables can store different types of data, including numbers, strings, and more complex objects.\n\n\n\n\n2.6.2 Reassigning Variables\nReassigning a variable is like changing the contents of a container.\nFor example, previously we ran this code to store the value “Joanna” inside the first_name variable:\n\nfirst_name = \"Joanna\"\n\nTo change this to a different value, simply run a new assignment statement with a new value:\n\nfirst_name = \"Luigi\"\n\nYou can print the variable to observe the change:\n\nfirst_name\n\n'Luigi'\n\n\n\n\n2.6.3 Working with Variables\nMost of your time in Python will be spent manipulating variables. Let’s see some quick examples.\nYou can run simple commands on variables. For example, below we store the value 100 in a variable and then take the square root of the variable:\n\nimport math\n\nmy_number = 100\nmath.sqrt(my_number)\n\n10.0\n\n\nPython “sees” my_number as the number 100, and so is able to evaluate its square root.\n\nYou can also combine existing variables to create new variables. For example, type out the code below to add my_number to itself, and store the result in a new variable called my_sum:\n\nmy_sum = my_number + my_number\nmy_sum\n\n200\n\n\nWhat should be the value of my_sum? First take a guess, then check it by printing it.\n\nPython also allows us to concatenate strings with the + operator. For example, we can concatenate the first_name and last_name variables to create a new variable called full_name:\n\nfirst_name = \"Joanna\"\nlast_name = \"Luigi\"\nfull_name = first_name + \" \" + last_name\nfull_name\n\n'Joanna Luigi'\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.6.4 Q: Variable assignment and manipulation\nConsider the code below. What is the value of the answer variable? Think about it, then run the code to check your answer.\n\neight = 9\nanswer = eight - 8\nanswer\n\n\n\n\n\n2.6.5 Getting User Input\nThough it’s not used often in data analysis, the input() function from Python is a cool Python feature that you should know about. It allows you to get input from the user.\nHere’s a simple example. We can request user input and store it in a variable called name.\n\nname = input()\n\nAnd then we can print a greeting to the user.\n\nprint(\"Hello,\", name)\n\nWe can also include a question for the input prompt:\n\nname = input('What is your name? ')\nprint(\"Hello,\", name)\n\nLet’s see another example. We’ll tell the user how many letters are in their name.\n\nname = input('What is your name? ')\nprint(\"There are\", len(name), \"letters in your name\")\n\nFor instance, if you run this code and enter “Kene”, you might see:\nWhat is your name? Kene\nThere are 4 letters in your name\n\n\n\n\n\n\nPractice\n\n\n\n2.6.6 Q: Using input()\nWrite a short program that asks the user for their favorite color and then prints a message saying “Your favorite color is [color]!”. Test your program by running it and entering a color.\n\n\n\n\n2.6.7 Common Error with Variables\nOne of the most common errors you’ll encounter when working with variables in Python is the NameError. This occurs when you try to use a variable that hasn’t been defined yet. For example:\n\nmy_number = 48  # define `my_number`\nMy_number + 2  # attempt to add 2 to `my_number`\n\nIf you run this code, you’ll get an error message like this:\nNameError: name 'My_number' is not defined\nHere, Python returns an error message because we haven’t created (or defined) the variable My_number yet. Recall that Python is case-sensitive; we defined my_number but tried to use My_number.\nTo fix this, make sure you’re using the correct variable name:\n\nmy_number = 48\nmy_number + 2  # This will work and return 50\n\n50\n\n\nAlways double-check your variable names to avoid this error. Remember, in Python, my_number, My_number, and MY_NUMBER are all different variables.\n\nWhen you first start learning Python, dealing with errors can be frustrating. They’re often difficult to understand.\nBut it’s important to get used to reading and understanding errors, because you’ll get them a lot through your coding career.\nLater, we will show you how to use Large Language Models (LLMs) like ChatGPT to debug errors.\nAt the start though, it’s good to try to spot and fix errors yourself.\n\n\n\n\n\n\nPractice\n\n\n\n2.6.8 Q: Debugging variable errors\nThe code below returns an error. Why? (Look carefully)\n\nmy_1st_name = \"Kene\"\nmy_last_name = \"Nwosu\"\n\nprint(my_Ist_name, my_last_name)\n\nHint: look at the variable names. Are they consistent?\n\n\n\n\n2.6.9 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n— Phil Karlton.\n\nBecause much of your work in Python involves interacting with variables you have created, picking intelligent names for these variables is important.\nNaming variables is difficult because names should be both short (so that you can type them quickly) and informative (so that you can easily remember what the variable contains), and these two goals are often in conflict.\nSo names that are too long, like the one below, are bad because they take forever to type.\n\nsample_of_the_ebola_outbreak_dataset_from_sierra_leone_in_2014\n\nAnd a name like data is bad because it is not informative; the name does not give a good idea of what the variable contains.\nAs you write more Python code, you will learn how to write short and informative names.\n\nFor names with multiple words, there are a few conventions for how to separate the words:\n\nsnake_case = \"Snake case uses underscores\"\ncamelCase = \"Camel case capitalizes new words (but not the first word)\"\nPascalCase = \"Pascal case capitalizes all words including the first\"\n\nWe recommend snake_case, which uses all lower-case words, and separates words with _.\n\nNote too that there are some limitations on variable names:\n\nNames must start with a letter or underscore. So 2014_data is not a valid name (because it starts with a number). Try running the code chunk below to see what error you get.\n\n\n2014_data = \"This is not a valid name\"\n\n\nNames can only contain letters, numbers, and underscores (_). So ebola-data or ebola~data or ebola data with a space are not valid names.\n\n\nebola-data = \"This is not a valid name\"\n\n\nebola~data = \"This is not a valid name\"\n\n\n\n\n\n\n\nSide note\n\n\n\nWhile we recommend snake_case for variable names in Python, you might see other conventions like camelCase or PascalCase, especially when working with code from other languages or certain Python libraries. It’s important to be consistent within your own code and follow the conventions of any project or team you’re working with.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n2.6.10 Q: Valid variable naming conventions\nWhich of the following variable names are valid in Python? Try to determine this without running the code, then check your answers by attempting to run each line.\nThen fix the invalid variable names.\n\n1st_name = \"John\"\nlast_name = \"Doe\"\nfull-name = \"John Doe\"\nage_in_years = 30\ncurrent@job = \"Developer\"\nPhoneNumber = \"555-1234\"\n_secret_code = 42",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_coding_basics.html#wrap-up",
    "href": "p_foundations_coding_basics.html#wrap-up",
    "title": "2  Coding basics",
    "section": "2.7 Wrap-up",
    "text": "2.7 Wrap-up\nIn this lesson, we’ve covered the fundamental building blocks of Python programming:\n\nComments: Using # for single-line and triple quotes for multi-line comments.\nBasic Arithmetic: Using Python as a calculator and understanding order of operations.\nMath Library: Performing complex mathematical operations.\nCode Spacing: Improving readability with proper spacing.\nVariables: Creating, manipulating, and reassigning variables of different types.\nGetting User Input: Using the input() function to get input from the user.\nVariable Naming: Following rules and best practices for naming variables.\nCommon Errors: Identifying and fixing errors related to variables.\n\nThese concepts form the foundation of Python programming. As you continue your journey, you’ll build upon these basics to create more complex and powerful programs. Remember, practice is key to mastering these concepts!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding basics</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html",
    "href": "p_foundations_functions_methods.html",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "3.1 Learning objectives",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#learning-objectives",
    "href": "p_foundations_functions_methods.html#learning-objectives",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "",
    "text": "You understand what functions and methods are in Python.\nYou can identify and use arguments (parameters) in functions and methods.\nYou know how to call built-in functions and methods on objects.\nYou understand what libraries are in Python and how to import them.\nYou know how to install a simple external library and use it in your code.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#introduction",
    "href": "p_foundations_functions_methods.html#introduction",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nIn this lesson, you will learn about functions, methods, and libraries in Python, building on the basics we covered in the previous lesson.\nTo get started, open your preferred Python environment (e.g., Jupyter Notebook, VS Code, or PyCharm), and create a new Python file or notebook.\nNext, save the file with a name like “functions_and_libraries.py” or “functions_and_libraries.ipynb” depending on your environment.\nYou should now type all the code from this lesson into that file.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#functions",
    "href": "p_foundations_functions_methods.html#functions",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.3 Functions",
    "text": "3.3 Functions\nA function is a block of code that performs a specific task. It can take inputs (arguments) and return outputs. Here’s an example of a built-in function with just one argument:\n\n# Using the len() function to get the length of a string\nlen(\"Python\")\n\n6\n\n\nThe round() function takes two arguments: the number to round and the number of decimal places to round to.\n\n# Using the round() function to round a number\nround(3.1415, 2)\n\n3.14\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.3.1 Q: Using built-in functions\nUse the abs() function to get the absolute value of -5.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#arguments-parameters",
    "href": "p_foundations_functions_methods.html#arguments-parameters",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.4 Arguments (Parameters)",
    "text": "3.4 Arguments (Parameters)\nArguments (also called parameters) are the values that you pass to a function (or method) when you call it.\nThere are different ways to pass arguments to a function.\nConsider again the round() function.\nIf we look at the documentation for the round() function, with :\n\nround?\n\nWe see that it takes two arguments:\n\nnumber: The number to round.\nndigits: The number of decimal places to round to.\n\nThere are two main ways to pass arguments to this function.\n\nPositional arguments: Passed in the order they are defined. Since the default order of the arguments is number then ndigits, we can pass the arguments in that order without specifying the argument names, as we did above.\n\n\nround(3.1415, 2)\n\n3.14\n\n\nIf we swap the order of the arguments, we get an error:\n\nround(2, 3.1415)\n\n\nKeyword arguments: Passed by specifying the argument name followed by a = and the argument value.\n\n\nround(number=3.1415, ndigits=2)\n\n3.14\n\n\nWith this method, we can pass the arguments in any order, as long as we use the argument names.\n\nround(ndigits=2, number=3.1415)\n\n3.14\n\n\nSpecifying the keyword is usually recommended, except for simple functions with very few arguments, or when the order of the arguments is obvious from context.\n\n\n\n\n\n\nPractice\n\n\n\n\n3.4.1 Q: Using Positional Arguments with pow()\nUse the pow() function to calculate 2 raised to the power of 5 by passing positional arguments. You may need to consult the documentation for the pow() function to see how it works.\nWrite your code below and run it to check your answer:\n\n# Your code here\n\n\n\n3.4.2 Q: Using Keyword Arguments with round()\nUse the round() function to round the number 9.8765 to 3 decimal places by specifying keyword arguments.\nWrite your code below and run it to check your answer:\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#methods",
    "href": "p_foundations_functions_methods.html#methods",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.5 Methods",
    "text": "3.5 Methods\nMethods are similar to functions, but they are associated with specific objects or data types. They are called using dot notation.\nFor example, every string object comes with a range of built-in methods, like upper() to convert to uppercase, lower() to convert to lowercase, replace() to replace substrings, and many more.\nLet’s see how to use these:\n\nname = \"python\"\nprint(name.upper())\nprint(name.lower())\nprint(name.replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nWe can also call the methods directly on the string object, without assigning it to a variable:\n\n# Using the upper() method on a string\nprint(\"python\".upper())\nprint(\"PYTHON\".lower())\nprint(\"python\".replace(\"p\", \"🐍\"))\n\nPYTHON\npython\n🐍ython\n\n\nSimilarly, numbers in Python come with some built-in methods. For example, the as_integer_ratio() (added in Python 3.8) method converts a decimalnumber to a ratio of two integers.\n\n# Using the as_integer_ratio() method on a float\nexample_decimal = 1.5\nexample_decimal.as_integer_ratio()\n\n(3, 2)\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.5.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nFunction\nMethod\nArgument (Parameter)\nDot Notation\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.5.2 Q: Using methods\n\nCall the replace() method on the string “Helo” to replace the single l with two ls.\nCall the split() method on the string “Hello World” to split the string into a list of words.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#libraries-in-python",
    "href": "p_foundations_functions_methods.html#libraries-in-python",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.6 Libraries in Python",
    "text": "3.6 Libraries in Python\nLibraries are collections of pre-written code that you can use in your programs. They extend the functionality of Python by providing additional functions and tools.\nFor example, the math library provides mathematical functions like sqrt() for square roots and sin() for sine.\nIf we try to use the sqrt() function without importing the math library, we get an error:\n\n# This will cause a NameError\nsqrt(16)\n\nWe can import the math library and use the sqrt() function like this:\n\n# Import the library\nimport math\n\nThen we can use the sqrt() function like this:\n\n# Use the sqrt() function\nmath.sqrt(16)\n\n4.0\n\n\nWe can get help on a function in a similar way, calling both the function and the library it’s in:\n\n# Get help on the sqrt() function\nmath.sqrt?\n\nWe can also import libraries with aliases. For example, we can import the math library with the alias m:\n\n# Import the entire library with an alias\nimport math as m\n# Then we can use the alias to call the function\nm.sqrt(16)\n\n4.0\n\n\nFinally, if you want to skip the alias/library name, you can either import the functions individually:\n\n# Import specific functions from a library\nfrom math import sqrt, sin\n# Then we can use the function directly\nsqrt(16)\nsin(0)\n\n0.0\n\n\nOr import everything from the library:\n\n# Import everything from the library\nfrom math import *\n# Then we can all functions directly, such as sqrt() and sin()\nsqrt(16)\ncos(0)\ntan(0)\nsin(0)\n\n0.0\n\n\nPhew that’s a lot of ways to import libraries! You’ll mostly see the import ... as ... syntax, and sometimes the from ... import ... syntax.\nNote that we typically import all required libraries at the top of the file, in a single code chunk. This is a good practice to follow.\n\n\n\n\n\n\nPractice\n\n\n\n3.6.1 Q: Definitions\nCome up with simple definitions for the following terms that are clear to YOU (even if not technically exactly accurate):\n\nLibrary (Module)\nImport\nAlias\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n3.6.2 Q: Using functions from imported libraries\n\nImport the random library and use the randint() function to generate a random integer between 1 and 10. You can use the ? operator to get help on the function after importing it.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#installing-libraries",
    "href": "p_foundations_functions_methods.html#installing-libraries",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.7 Installing Libraries",
    "text": "3.7 Installing Libraries\nWhile Python comes with many built-in libraries, there are thousands of additional libraries available that you can install to extend Python’s functionality even further. Let’s look at how to install and use a simple external library, with the cowsay library as an example.\nIf we try to import this library without first installing it, we get an error:\n\nimport cowsay\n\nTo install the library, you can use the !pip install command in a code cell in Google Colab. For cowsay, you would run:\n\n!pip install cowsay\n\nPip installs packages from a remote repository called PyPI. Anyone can create and upload a package to PyPI. After a few checks, it’s then available for anyone to install.\n\n\n\n\n\n\nSide-note\n\n\n\nFor those working on local Python instances, you can install cowsay using pip in your terminal:\npip install cowsay\n\n\nOnce installed, we can now import and use the cowsay library:\n\nimport cowsay\n\n# Make the cow say something\ncowsay.cow('Moo!')\n\n  ____\n| Moo! |\n  ====\n    \\\n     \\\n       ^__^\n       (oo)\\_______\n       (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n\n\nThis should display an ASCII art cow saying “Moo!”.\n\n\n\n\n\n\nPractice\n\n\n\n3.7.1 Q: Using the emoji library\n\nInstall the emoji library.\nImport the emoji library.\nConsult the help for the emojize() function in the emoji library.\nUse the emojize() function to display an emoji for “thumbs up”.\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_functions_methods.html#wrap-up",
    "href": "p_foundations_functions_methods.html#wrap-up",
    "title": "3  Functions, Methods, and Libraries in Python",
    "section": "3.8 Wrap-up",
    "text": "3.8 Wrap-up\nIn this lesson, we’ve covered:\n\nFunctions and methods in Python\nArguments (parameters) and how to use them\nImporting and using libraries\nInstalling and using an external library\n\nThese concepts are fundamental to Python programming and will be used extensively as you continue to develop your skills. Practice using different functions, methods, and libraries to become more comfortable with these concepts.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions, Methods, and Libraries in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html",
    "href": "p_foundations_data_structures.html",
    "title": "4  Data Structures in Python",
    "section": "",
    "text": "4.1 Intro\nSo far in our Python explorations, we’ve been working with simple, single values, like numbers and strings. But, as you know, data usually comes in the form of larger structures. The structure most familiar to you is a table, with rows and columns.\nIn this lesson, we’re going to explore the building blocks for organizing data in Python, building up through lists, dictionaries, series, and finally tables, or, more formally,dataframes.\nLet’s dive in!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#learning-objectives",
    "href": "p_foundations_data_structures.html#learning-objectives",
    "title": "4  Data Structures in Python",
    "section": "4.2 Learning objectives",
    "text": "4.2 Learning objectives\n\nCreate and work with Python lists and dictionaries\nUnderstand and use Pandas Series\nExplore Pandas DataFrames for organizing structured data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#imports",
    "href": "p_foundations_data_structures.html#imports",
    "title": "4  Data Structures in Python",
    "section": "4.3 Imports",
    "text": "4.3 Imports\nWe need pandas for this lesson. You can import it like this:\n\nimport pandas as pd\n\nIf you get an error, you probably need to install it. You can do this by running !pip install pandas in a cell.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-lists",
    "href": "p_foundations_data_structures.html#python-lists",
    "title": "4  Data Structures in Python",
    "section": "4.4 Python Lists",
    "text": "4.4 Python Lists\nLists are like ordered containers that can hold different types of information. For example, you might have a list of things to buy:\n\nshopping = [\"apples\", \"bananas\", \"milk\", \"bread\"] \nshopping\n\n['apples', 'bananas', 'milk', 'bread']\n\n\nIn Python, we use something called “zero-based indexing” to access items in a list. This means we start counting positions from 0, not 1.\nLet’s see some examples:\n\nprint(shopping[0])  # First item (remember, we start at 0!)\nprint(shopping[1])  # Second item\nprint(shopping[2])  # Third item\n\napples\nbananas\nmilk\n\n\nIt might seem odd at first, but it’s a common practice in many programming languages. It has to do with how computers store information, and the ease of writing algorithms.\nWe can change the contents of a list after we’ve created it, using the same indexing system.\n\nshopping[1] = \"oranges\"  # Replace the second item (at index 1)\nshopping\n\n['apples', 'oranges', 'milk', 'bread']\n\n\nThere are many methods accessible to lists. For example, we can add elements to a list using the append() method.\n\nshopping.append(\"eggs\")\nshopping\n\n['apples', 'oranges', 'milk', 'bread', 'eggs']\n\n\nIn the initial stages of your Python data journey, you may not work with lists too often, so we’ll keep this intro brief.\n\n\n\n\n\n\nPractice\n\n\n\n4.4.1 Practice: Working with Lists\n\nCreate a list called temperatures with these values: 1,2,3,4\nPrint the first element of the list\nChange the last element to 6\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#python-dictionaries",
    "href": "p_foundations_data_structures.html#python-dictionaries",
    "title": "4  Data Structures in Python",
    "section": "4.5 Python Dictionaries",
    "text": "4.5 Python Dictionaries\nDictionaries are like labeled storage boxes for your data. Each piece of data (value) has a unique label (key). Below, we have a dictionary of grades for some students.\n\ngrades = {\"Alice\": 90, \"Bob\": 85, \"Charlie\": 92}\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92}\n\n\nAs you can see, dictionaries are defined using curly braces {}, with keys and values separated by colons :, and the key-value pairs are separated by commas.\nWe use the key to get the associated value.\n\ngrades[\"Bob\"]\n\n85\n\n\n\n4.5.1 Adding/Modifying Entries\nWe can easily add new information or change existing data in a dictionary.\n\ngrades[\"David\"] = 88  # Add a new student\ngrades\n\n{'Alice': 90, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\ngrades[\"Alice\"] = 95  # Update Alice's grade\ngrades\n\n{'Alice': 95, 'Bob': 85, 'Charlie': 92, 'David': 88}\n\n\n\n\n\n\n\n\nPractice\n\n\n\n4.5.2 Practice: Working with Dictionaries\n\nCreate a dictionary called prices with these pairs: “apple”: 0.50, “banana”: 0.25, “orange”: 0.75\nPrint the price of an orange by using the key\nAdd a new fruit “grape” with a price of 1.5\nChange the price of “banana” to 0.30\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-series",
    "href": "p_foundations_data_structures.html#pandas-series",
    "title": "4  Data Structures in Python",
    "section": "4.6 Pandas Series",
    "text": "4.6 Pandas Series\nPandas provides a data structure called a Series that is similar to a list, but with additional features that are particularly useful for data analysis.\nLet’s create a simple Series:\n\ntemps = pd.Series([1, 2, 3, 4, 5])\ntemps\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\nWe can use built-in Series methods to calculate summary statistics.\n\ntemps.mean()\ntemps.median()\ntemps.std()\n\nnp.float64(1.5811388300841898)\n\n\nAn important feature of Series is that they can have a custom index for intuitive access.\n\ntemps_labeled = pd.Series([1, 2, 3, 4], index=['Mon', 'Tue', 'Wed', 'Thu'])\ntemps_labeled\ntemps_labeled['Wed']\n\nnp.int64(3)\n\n\nThis makes them similar to dictionaries.\n\n\n\n\n\n\nPractice\n\n\n\n4.6.1 Practice: Working with Series\n\nCreate a Series called rainfall with these values: 5, 2, 7, 4, 1\nGet the mean and median rainfall\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#pandas-dataframes",
    "href": "p_foundations_data_structures.html#pandas-dataframes",
    "title": "4  Data Structures in Python",
    "section": "4.7 Pandas DataFrames",
    "text": "4.7 Pandas DataFrames\nNext up, let’s consider Pandas DataFrames, which are like Series but in two dimensions - think spreadsheets or database tables.\nThis is the most important data structure for data analysis.\nA DataFrame is like a spreadsheet in Python. It has rows and columns, making it perfect for organizing structured data.\nMost of the time, you will be importing external data frames, but you should know how to data frames from scratch within Python as well.\nLet’s create three lists first:\n\n# Create three lists\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 28]\ncities = [\"Lagos\", \"London\", \"Lima\"]\n\nThen we combined them into a dictionary, and finally into a dataframe.\n\ndata = {'name': names,\n        'age': ages,\n        'city': cities}\n\npeople_df = pd.DataFrame(data)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nNote that we could have created the dataframe without the intermediate series:\n\npeople_df = pd.DataFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [25, 30, 28],\n        \"city\": [\"Lagos\", \"London\", \"Lima\"],\n    }\n)\npeople_df\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nLagos\n\n\n1\nBob\n30\nLondon\n\n\n2\nCharlie\n28\nLima\n\n\n\n\n\n\n\nWe can select specific columns or rows from our DataFrame.\n\npeople_df[\"city\"]  # Selecting a column. Note that this returns a Series.\npeople_df.loc[0]  # Selecting a row by its label. This also returns a Series.\n\nname    Alice\nage        25\ncity    Lagos\nName: 0, dtype: object\n\n\nWe can call methods on the dataframe.\n\npeople_df.describe() # This is a summary of the numerical columns\npeople_df.info() # This is a summary of the data types\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   name    3 non-null      object\n 1   age     3 non-null      int64 \n 2   city    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n\n\nAnd we can call methods on the Series objects that result from selecting columns.\nFor example, we can get summary statistics on the “city” column.\n\npeople_df[\"city\"].describe()  # This is a summary of the \"city\" column\npeople_df[\"age\"].mean()  # This is the mean of the \"age\" column\n\nnp.float64(27.666666666666668)\n\n\nIn a future series of lessons, we’ll dive deeper into slicing and manipulating DataFrames. Our goal in this lesson is just to get you familiar with the basic syntax and concepts.\n\n\n\n\n\n\nPractice\n\n\n\n4.7.1 Practice: Working with DataFrames\n\nCreate a DataFrame called students with this information:\n\nColumns: “Name”, “Age”, “Grade”\nData:\n\n[“Alice”, 15, “A”]\n[“Bob”, 16, “B”]\n[“Charlie”, 15, “A”]\n\n\nDisplay the entire DataFrame\nShow only the “Grade” column\nDisplay the row for Bob\nCalculate and show the average age of the students\n\n\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_data_structures.html#wrap-up",
    "href": "p_foundations_data_structures.html#wrap-up",
    "title": "4  Data Structures in Python",
    "section": "4.8 Wrap-up",
    "text": "4.8 Wrap-up\nWe’ve explored the main data structures for Python data analysis. From basic lists and dictionaries to Pandas Series and DataFrames, these tools are essential for organizing and analyzing data. They will be the foundation for more advanced data work in future lessons.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html",
    "href": "p_foundations_for_loops.html",
    "title": "5  Intro to Loops in Python",
    "section": "",
    "text": "5.1 Introduction\nAt the heart of programming is the concept of repeating a task multiple times. A for loop is one fundamental way to do that. Loops enable efficient repetition, saving time and effort.\nMastering this concept is essential for writing intelligent Python code.\nLet’s dive in and enhance your coding skills!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#learning-objectives",
    "href": "p_foundations_for_loops.html#learning-objectives",
    "title": "5  Intro to Loops in Python",
    "section": "5.2 Learning Objectives",
    "text": "5.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nUse basic for loops in Python\nUse index variables to iterate through lists in a loop\nFormat output using f-strings within loops\nApply loops to generate multiple plots for data visualization",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#packages",
    "href": "p_foundations_for_loops.html#packages",
    "title": "5  Intro to Loops in Python",
    "section": "5.3 Packages",
    "text": "5.3 Packages\nIn this lesson, we will use the following Python libraries:\n\nimport pandas as pd\nimport plotly.express as px\nfrom vega_datasets import data",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#intro-to-for-loops",
    "href": "p_foundations_for_loops.html#intro-to-for-loops",
    "title": "5  Intro to Loops in Python",
    "section": "5.4 Intro to for Loops",
    "text": "5.4 Intro to for Loops\nLet’s start with a simple example. Suppose we have a list of children’s ages in years, and we want to convert these to months:\n\nages = [7, 8, 9]  # List of ages in years\n\nWe could try to directly multiply the list by 12:\n\nages * 12\n\n[7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9,\n 7,\n 8,\n 9]\n\n\nBut this does not do what we want. It repeats the list 12 times.\nRather, we need to loop through each element in the list and multiply it by 12:\n\nfor age in ages:\n    print(age * 12)\n\n84\n96\n108\n\n\nfor and in are required keywords in the loop. The colon and the indentation on the second line are also required.\nIn this loop, age is a temporary variable that takes the value of each element in ages during each iteration. First, age is 7, then 8, then 9.\nYou can choose any name for this variable:\n\nfor random_name in ages:\n    print(random_name * 12)\n\n84\n96\n108\n\n\nNote that we need the print statement since the loop does not automatically print the result:\n\nfor age in ages:\n    age * 12\n\n\n\n\n\n\n\nPractice\n\n\n\n5.4.1 Hours to Minutes Basic Loop\nTry converting hours to minutes using a for loop. Start with this list of hours:\n\nhours = [3, 4, 5]  # List of hours\n# Your code here",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#printing-with-f-strings",
    "href": "p_foundations_for_loops.html#printing-with-f-strings",
    "title": "5  Intro to Loops in Python",
    "section": "5.5 Printing with f-strings",
    "text": "5.5 Printing with f-strings\nWe might want to print both the result and the original age. We could do this by concatenating strings with the + operator. But we need to convert the age to a string with str().\n\nfor age in ages:\n    print(str(age) + \" years is \" + str(age * 12) + \" months\" )\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nAlternatively, we can use something called an f-string. This is a string that allows us to embed variables directly.\n\nfor age in ages:\n    print(f\"{age} years is {age * 12} months\")\n\n7 years is 84 months\n8 years is 96 months\n9 years is 108 months\n\n\nWithin the f-string, we use curly braces {} to embed the variables.\n\n\n\n\n\n\nPractice\n\n\n\n5.5.1 Practice: F-String\nAgain convert the list of hours below to minutes. Use f-strings to print both the original hours and the converted minutes.\n\nhours = [3, 4, 5]  # List of hours\n# Your code here\n# Example output \"3 hours is 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "href": "p_foundations_for_loops.html#are-for-loops-useful-in-python",
    "title": "5  Intro to Loops in Python",
    "section": "5.6 Are for Loops Useful in Python?",
    "text": "5.6 Are for Loops Useful in Python?\nWhile for loops are useful, in many cases there are more efficient ways to perform operations over collections of data.\nFor example, our initial age conversion could be achieved using pandas Series:\n\nimport pandas as pd\n\nages = pd.Series([7, 8, 9])\nmonths = ages * 12\nprint(months)\n\n0     84\n1     96\n2    108\ndtype: int64\n\n\nBut while libraries like pandas offer powerful ways to work with data, for loops are essential for tasks that can’t be easily vectorized or when you need fine-grained control over the iteration process.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "href": "p_foundations_for_loops.html#looping-with-an-index-and-value",
    "title": "5  Intro to Loops in Python",
    "section": "5.7 Looping with an Index and Value",
    "text": "5.7 Looping with an Index and Value\nSometimes, we want to access both the position (index) and the value of items in a list. The enumerate() function helps us do this easily.\nLet’s look at our ages list again:\n\nages = [7, 8, 9]  # List of ages in years\n\nFirst, let’s see what enumerate() actually does:\n\nfor item in enumerate(ages):\n    print(item)\n\n(0, 7)\n(1, 8)\n(2, 9)\n\n\nAs you can see, enumerate() gives us pairs of (index, value).\nWe can unpack these pairs directly in the for loop:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age}\")\n\nThe person at index 0 is aged 7\nThe person at index 1 is aged 8\nThe person at index 2 is aged 9\n\n\nHere, i is the index, and age is the value at that index.\nNow, let’s create a more detailed output using both the index and value:\n\nfor i, age in enumerate(ages):\n    print(f\"The person at index {i} is aged {age} years which is {age * 12} months\")\n\nThe person at index 0 is aged 7 years which is 84 months\nThe person at index 1 is aged 8 years which is 96 months\nThe person at index 2 is aged 9 years which is 108 months\n\n\nThis is particularly useful when you need both the position and the value in your loop.\n\n\n\n\n\n\nPractice\n\n\n\n5.7.1 Practice: Enumerate with F-strings\nUse enumerate() and f-strings to print a sentence for each hour in the list:\n\nhours = [3, 4, 5]  # List of hours\n\n# Your code here\n# Example output: \"Hour 3 at index 0 is equal to 180 minutes\"",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Intro to Loops in Python</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html",
    "href": "p_data_on_display_data_viz_types.html",
    "title": "6  Data Visualization Types",
    "section": "",
    "text": "7 Univariate Graphs",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric",
    "href": "p_data_on_display_data_viz_types.html#numeric",
    "title": "6  Data Visualization Types",
    "section": "7.1 Numeric",
    "text": "7.1 Numeric\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\n\npx.box(tips, x='tip')\n\n                                                \n\n\n\npx.violin(tips, x='tip', box=True, points=\"all\")",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "href": "p_data_on_display_data_viz_types.html#numeric-vs-numeric",
    "title": "6  Data Visualization Types",
    "section": "9.1 Numeric vs Numeric",
    "text": "9.1 Numeric vs Numeric\n\npx.scatter(tips, x='total_bill', y='tip')",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization Types</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html",
    "href": "p_data_on_display_univariate.html",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "",
    "text": "7.1 Intro\nIn this lesson, you’ll learn how to create univariate graphs using Plotly Express. Univariate graphs are essential for understanding the distribution of a single variable, whether it’s categorical or quantitative.\nLet’s get started!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#learning-objectives",
    "href": "p_data_on_display_univariate.html#learning-objectives",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "7.2 Learning objectives",
    "text": "7.2 Learning objectives\n\nCreate bar charts, pie charts, and treemaps for categorical data using Plotly Express\nGenerate histograms for quantitative data using Plotly Express\nCustomize graph appearance and labels",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#imports",
    "href": "p_data_on_display_univariate.html#imports",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "7.3 Imports",
    "text": "7.3 Imports\nThis lesson requires plotly.express, pandas, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#quantitative-data",
    "href": "p_data_on_display_univariate.html#quantitative-data",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "7.4 Quantitative Data",
    "text": "7.4 Quantitative Data\n\n7.4.1 Histogram\nHistograms are used to visualize the distribution of continuous variables.\nLet’s make a histogram of the tip amounts in the tips dataset.\n\ntips = px.data.tips()\ntips.head() # view the first 5 rows\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nWe can see that the highest bar, corresponding to tips between 1.75 and 2.24, has a frequency of 55. This means that there were 55 tips between 1.75 and 2.24.\n\n\n\n\n\n\nSide-note\n\n\n\nNotice that plotly charts are interactive. You can hover over the bars to see the exact number of tips in each bin.\nTry playing with the buttons at the top right. The button to download the chart as a png is especially useful.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.4.2 Practice: Speed Distribution Histogram\nFollowing the example of the histogram of tips, create a histogram of the speed distribution (Speed_IAS_in_knots) using the birdstrikes dataset.\n\nbirdstrikes = data.birdstrikes()\nbirdstrikes.head()\n# Your code here\n\n\n\n\n\n\n\n\nAirport__Name\nAircraft__Make_Model\nEffect__Amount_of_damage\nFlight_Date\nAircraft__Airline_Operator\nOrigin_State\nWhen__Phase_of_flight\nWildlife__Size\nWildlife__Species\nWhen__Time_of_day\nCost__Other\nCost__Repair\nCost__Total_$\nSpeed_IAS_in_knots\n\n\n\n\n0\nBARKSDALE AIR FORCE BASE ARPT\nT-38A\nNone\n1/8/90 0:00\nMILITARY\nLouisiana\nClimb\nLarge\nTurkey vulture\nDay\n0\n0\n0\n300.0\n\n\n1\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/9/90 0:00\nMILITARY\nLouisiana\nApproach\nMedium\nUnknown bird or bat\nNight\n0\n0\n0\n200.0\n\n\n2\nBARKSDALE AIR FORCE BASE ARPT\nB-52\nNone\n1/11/90 0:00\nMILITARY\nLouisiana\nTake-off run\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n130.0\n\n\n3\nNEW ORLEANS INTL\nB-737-300\nSubstantial\n1/11/90 0:00\nSOUTHWEST AIRLINES\nLouisiana\nTake-off run\nSmall\nRock pigeon\nDay\n0\n0\n0\n140.0\n\n\n4\nBARKSDALE AIR FORCE BASE ARPT\nKC-10A\nNone\n1/12/90 0:00\nMILITARY\nLouisiana\nClimb\nMedium\nUnknown bird or bat\nDay\n0\n0\n0\n160.0\n\n\n\n\n\n\n\n\n\nWe can view the help documentation for the function by typing px.histogram? in a cell and running it.\n\npx.histogram?\n\nFrom the help documentation, we can see that the px.histogram function has many arguments that we can use to customize the graph.\nLet’s make the histogram a bit nicer by adding a title, customizing the x axis label, and changing the color.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"lightseagreen\"]\n)\n\n                                                \n\n\nColor names are based on standard CSS color naming from Mozilla. You can see the full list here.\nAlternatively, you can use hex color codes, like #1f77b4. You can get these easily by using a color picker. Search for “color picker” on Google.\n\npx.histogram(\n    tips,\n    x=\"tip\",\n    labels={\"tip\": \"Tip Amount ($)\"},\n    title=\"Distribution of Tips\", \n    color_discrete_sequence=[\"#6a5acd\"]\n)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.4.3 Practice: Bird Strikes Histogram Custom\nUpdate your birdstrikes histogram to use a hex code color, add a title, and change the x-axis label to “Speed (Nautical Miles Per Hour)”.\n\n# Your code here\n\n\n\n\n\n7.4.4 Counts on bars\nWe can add counts to the bars with the text_auto argument.\n\npx.histogram(tips, x='tip', text_auto= True)\n\n                                                \n\n\n\n7.4.4.1 Bins and bandwidths\nWe can adjust the number of bins or bin width to better represent the data using the nbins argument. Let’s make a histogram with just 10 bins:\n\npx.histogram(tips, x='tip', nbins=10)\n\n                                                \n\n\nNow we have broader tip amount groups.\n\n\n\n\n\n\nPractice\n\n\n\n7.4.5 Practice: Speed Distribution Histogram Custom\nCreate a histogram of the speed distribution (Speed_IAS_in_knots) with 15 bins using the birdstrikes dataset. Add counts to the bars, use a color of your choice, and add an appropriate title.\n\n# Your code here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#categorical-data",
    "href": "p_data_on_display_univariate.html#categorical-data",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "7.5 Categorical Data",
    "text": "7.5 Categorical Data\n\n7.5.1 Bar chart\nBar charts can be used to display the frequency of a single categorical variable.\nPlotly has a px.bar function that we will see later. But for single categorical variables, the function plotly wants you to use is actually px.histogram. (Statisticians everywhere are crying; histograms are supposed to be used for just quantitative data!)\nLet’s create a basic bar chart showing the distribution of sex in the tips dataset:\n\npx.histogram(tips, x='sex')   \n\n                                                \n\n\nLet’s add counts to the bars.\n\npx.histogram(tips, x='sex', text_auto= True)\n\n                                                \n\n\nWe can enhance the chart by adding a color axis, and customizing the labels and title.\n\npx.histogram(tips, x='sex', text_auto=True, color='sex', \n             labels={'sex': 'Gender'},\n             title='Distribution of Customers by Gender')\n\n                                                \n\n\nArguably, in this plot, we do not need the color axis, since the sex variable is already represented by the x axis. But public audiences like colors, so it may still be worth including.\nHowever, we should remove the legend. Let’s also use custom colors.\nFor this, we can first create a figure object, then use the .layout.update method from that object to update the legend.\n\ntips_by_sex = px.histogram(\n    tips,\n    x=\"sex\",\n    text_auto=True,\n    color=\"sex\",\n    labels={\"sex\": \"Gender\"},\n    title=\"Distribution of Customers by Gender\",\n    color_discrete_sequence=[\"#1f77b4\", \"#ff7f0e\"],\n)\n\ntips_by_sex.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.5.2 Practice: Bird Strikes by Phase of Flight\nCreate a bar chart showing the frequency of bird strikes by the phase of flight, When__Phase_of_flight. Add appropriate labels and a title. Use colors of your choice, and remove the legend.\n\n# Your code here\n\n\n\n\n7.5.2.1 Sorting categories\nIt is sometimes useful to dictate a specific order for the categories in a bar chart.\nConsider this bar chart of the election winners by district in the 2013 Montreal mayoral election.\n\nelection = px.data.election()\nelection.head()\n\n\n\n\n\n\n\n\ndistrict\nCoderre\nBergeron\nJoly\ntotal\nwinner\nresult\ndistrict_id\n\n\n\n\n0\n101-Bois-de-Liesse\n2481\n1829\n3024\n7334\nJoly\nplurality\n101\n\n\n1\n102-Cap-Saint-Jacques\n2525\n1163\n2675\n6363\nJoly\nplurality\n102\n\n\n2\n11-Sault-au-Récollet\n3348\n2770\n2532\n8650\nCoderre\nplurality\n11\n\n\n3\n111-Mile-End\n1734\n4782\n2514\n9030\nBergeron\nmajority\n111\n\n\n4\n112-DeLorimier\n1770\n5933\n3044\n10747\nBergeron\nmajority\n112\n\n\n\n\n\n\n\n\npx.histogram(election, x='winner')\n\n                                                \n\n\nLet’s define a custom order for the categories. “Bergeron” will be first, then “Joly” then “Coderre”.\n\ncustom_order = [\"Bergeron\", \"Joly\", \"Coderre\"]\nelection_chart = px.histogram(election, x='winner', category_orders={'winner': custom_order})\nelection_chart\n\n                                                \n\n\nWe can also sort the categories by frequency.\nWe can sort the categories by frequency using the categoryorder attribute of the x axis.\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total descending\")\n\n                                                \n\n\nOr in ascending order:\n\nelection_chart = px.histogram(election, x=\"winner\")\nelection_chart.update_xaxes(categoryorder=\"total ascending\")\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.5.3 Practice: Sorted Origin State Bar Chart\nCreate a sorted bar chart showing the distribution of bird strikes by origin state. Sort the bars in ascending order of frequency.\n\n# Your code here\n\n\n\n\n\n\n7.5.4 Horizontal bar chart\nWhen you have many categories, horizontal bar charts are often easier to read than vertical bar charts. To make a horizontal bar chart, simply use the y axis instead of the x axis.\n\npx.histogram(tips, y='day')\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.5.5 Practice: Horizontal Bar Chart of Origin State\nCreate a horizontal bar chart showing the distribution of bird strikes by origin state.\n\n# Your code here\n\n\n\n\n\n7.5.6 Pie chart\nPie charts are also useful for showing the proportion of categorical variables. They are best used when you have a small number of categories. For larger numbers of categories, pie charts are hard to read.\nLet’s make a pie chart of the distribution of tips by day of the week.\n\npx.pie(tips, names=\"day\")\n\n                                                \n\n\nWe can add labels to the pie chart to make it easier to read.\n\ntips_by_day = px.pie(tips, names=\"day\")\ntips_by_day_with_labels = tips_by_day.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\ntips_by_day_with_labels\n\n                                                \n\n\nThe legend is no longer needed, so we can remove it.\n\ntips_by_day_with_labels.update_layout(showlegend=False)\n\n                                                \n\n\n\n\n\n\n\n\nPro\n\n\n\nIf you forget how to make simple changes like this, don’t hesitate to consult the plotly documentation, Google or ChatGPT.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n7.5.7 Practice: Wildlife Size Pie Chart\nCreate a pie chart showing the distribution of bird strikes by wildlife size. Include percentages and labels inside the pie slices.\n\n# Your code here",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_univariate.html#summary",
    "href": "p_data_on_display_univariate.html#summary",
    "title": "7  Univariate Graphs with Plotly Express",
    "section": "7.6 Summary",
    "text": "7.6 Summary\nIn this lesson, you learned how to create univariate graphs using Plotly Express. You should now feel confident in your ability to create bar charts, pie charts, and histograms. You should also feel comfortable customizing the appearance of your graphs.\nSee you in the next lesson.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Univariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html",
    "href": "p_data_on_display_multivariate.html",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "",
    "text": "8.1 Introduction\nIn this lesson, you’ll learn how to create bivariate and multivariate graphs using Plotly Express. These types of graphs are essential for exploring relationships between two or more variables, whether they are quantitative or categorical. Understanding these relationships can provide deeper insights into your data.\nLet’s dive in!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#learning-objectives",
    "href": "p_data_on_display_multivariate.html#learning-objectives",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.2 Learning Objectives",
    "text": "8.2 Learning Objectives\nBy the end of this lesson, you will be able to:\n\nCreate scatter plots for quantitative vs. quantitative data\nGenerate grouped histograms and violin plots for quantitative vs. categorical data\nCreate grouped, stacked, and percent-stacked bar charts for categorical vs. categorical data\nVisualize time series data using bar charts and line charts\nCreate bubble charts to display relationships between three or more variables\nUse faceting to compare distributions across subsets of data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#imports",
    "href": "p_data_on_display_multivariate.html#imports",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.3 Imports",
    "text": "8.3 Imports\nThis lesson requires plotly.express, pandas, numpy, and vega_datasets. Install them if you haven’t already.\n\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nfrom vega_datasets import data",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-numeric-data",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.4 Numeric vs. Numeric Data",
    "text": "8.4 Numeric vs. Numeric Data\nWhen both variables are quantitative, scatter plots are an excellent way to visualize their relationship.\n\n8.4.1 Scatter Plot\nLet’s create a scatter plot to examine the relationship between total_bill and tip in the tips dataset. The tips dataset is included in Plotly Express and contains information about restaurant bills and tips that were collected by a waiter in a US restaurant.\nFirst, we’ll load the dataset and view the first five rows:\n\ntips = px.data.tips()\ntips\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\nNext, we’ll create a basic scatter plot. We do this with the px.scatter function.\n\npx.scatter(tips, x='total_bill', y='tip')\n\n                                                \n\n\nFrom the scatter plot, we can observe that as the total bill increases, the tip amount tends to increase as well.\nLet’s enhance the scatter plot by adding labels and a title.\n\npx.scatter(\n    tips,\n    x=\"total_bill\",\n    y=\"tip\",\n    labels={\"total_bill\": \"Total Bill ($)\", \"tip\": \"Tip ($)\"},\n    title=\"Relationship Between Total Bill and Tip Amount\",\n)\n\n                                                \n\n\nRecall that you can see additional information about the function by typing px.scatter? in a cell and executing the cell.\n\npx.scatter?\n\n\n\n\n\n\n\nPractice\n\n\n\n8.4.2 Practice: Life Expectancy vs. GDP Per Capita\nUsing the Gapminder dataset (the 2007 subset, g_2007, defined below), create a scatter plot showing the relationship between gdpPercap (GDP per capita) and lifeExp (life expectancy).\nAccording to the plot, what is the relationship between GDP per capita and life expectancy?\n\ngapminder = px.data.gapminder()\ng_2007 = gapminder.query('year == 2007')\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#numeric-vs.-categorical-data",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.5 Numeric vs. Categorical Data",
    "text": "8.5 Numeric vs. Categorical Data\nWhen one variable is quantitative and the other is categorical, we can use grouped histograms, violin plots, or box plots to visualize the distribution of the quantitative variable across different categories.\n\n8.5.1 Grouped Histograms\nFirst, here’s how you can create a regular histogram of all tips:\n\npx.histogram(tips, x='tip')\n\n                                                \n\n\nTo create a grouped histogram, use the color parameter to specify the categorical variable. Here, we’ll color the histogram by sex:\n\npx.histogram(tips, x='tip', color='sex')\n\n                                                \n\n\nBy default, the histograms for each category are stacked. To change this behavior, you can use the barmode parameter. For example, barmode='overlay' will create an overlaid histogram:\n\npx.histogram(tips, x=\"tip\", color=\"sex\", barmode=\"overlay\")\n\n                                                \n\n\nThis creates two semi-transparent histograms overlaid on top of each other, allowing for direct comparison of the distributions.\n\n\n\n\n\n\nPractice\n\n\n\n8.5.2 Practice: Age Distribution by Gender\nUsing the la_riots dataset from vega_datasets, create a grouped histogram of age by gender. Compare the age distributions between different genders.\nAccording to the plot, was the oldest victim male or female?\n\nla_riots = data.la_riots()\nla_riots.head()\n# Your code here\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\ngender\nrace\ndeath_date\naddress\nneighborhood\ntype\nlongitude\nlatitude\n\n\n\n\n0\nCesar A.\nAguilar\n18.0\nMale\nLatino\n1992-04-30\n2009 W. 6th St.\nWestlake\nOfficer-involved shooting\n-118.273976\n34.059281\n\n\n1\nGeorge\nAlvarez\n42.0\nMale\nLatino\n1992-05-01\nMain & College streets\nChinatown\nNot riot-related\n-118.234098\n34.062690\n\n\n2\nWilson\nAlvarez\n40.0\nMale\nLatino\n1992-05-23\n3100 Rosecrans Ave.\nHawthorne\nHomicide\n-118.326816\n33.901662\n\n\n3\nBrian E.\nAndrew\n30.0\nMale\nBlack\n1992-04-30\nRosecrans & Chester avenues\nCompton\nOfficer-involved shooting\n-118.215390\n33.903457\n\n\n4\nVivian\nAustin\n87.0\nFemale\nBlack\n1992-05-03\n1600 W. 60th St.\nHarvard Park\nDeath\n-118.304741\n33.985667\n\n\n\n\n\n\n\n\n\n\n\n8.5.3 Violin & Box Plots\nViolin plots are useful for comparing the distribution of a quantitative variable across different categories. They show the probability density of the data at different values and can include a box plot to summarize key statistics.\nFirst, let’s create a violin plot of all tips:\n\npx.violin(tips, y=\"tip\")\n\n                                                \n\n\nWe can add a box plot to the violin plot by setting the box parameter to True:\n\npx.violin(tips, y=\"tip\", box=True)\n\n                                                \n\n\nFor just the box plot, we can use px.box:\n\npx.box(tips, y=\"tip\")\n\n                                                \n\n\nTo add jitter points to the violin or box plots, we can use the points = 'all' parameter.\n\npx.violin(tips, y=\"tip\", points=\"all\")\n\n                                                \n\n\nNow, to create a violin plot of tips by gender, use the x parameter to specify the categorical variable:\n\npx.violin(tips, y=\"tip\", x=\"sex\", box=True)\n\n                                                \n\n\nWe can also add a color axis to differentiate the violins:\n\npx.violin(tips, y=\"tip\", x=\"sex\", color=\"sex\", box=True)\n\n                                                \n\n\n\n\n\n\n\n\nPractice\n\n\n\n8.5.4 Practice: Life Expectancy by Continent\nUsing the g_2007 dataset, create a violin plot showing the distribution of lifeExp by continent.\nAccording to the plot, which continent has the highest median country life expectancy?\n\ng_2007 = gapminder.query(\"year == 2007\")\ng_2007.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\niso_alpha\niso_num\n\n\n\n\n11\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.580338\nAFG\n4\n\n\n23\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.029526\nALB\n8\n\n\n35\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.367465\nDZA\n12\n\n\n47\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.231267\nAGO\n24\n\n\n59\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.379640\nARG\n32\n\n\n\n\n\n\n\n\n\n\n\n8.5.5 Summary Bar Charts (Mean and Standard Deviation)\nSometimes it’s useful to display the mean and standard deviation of a quantitative variable across different categories. This can be visualized using a bar chart with error bars.\nFirst, let’s calculate the mean and standard deviation of tips for each gender. You have not yet learned how to do this, but you will in a later lesson.\n\n# Calculate the mean and standard deviation\nsummary_df = (\n    tips.groupby(\"sex\")\n    .agg(mean_tip=(\"tip\", \"mean\"), std_tip=(\"tip\", \"std\"))\n    .reset_index()\n)\nsummary_df\n\n\n\n\n\n\n\n\nsex\nmean_tip\nstd_tip\n\n\n\n\n0\nFemale\n2.833448\n1.159495\n\n\n1\nMale\n3.089618\n1.489102\n\n\n\n\n\n\n\nNext, we’ll create a bar chart using px.bar and add error bars using the error_y parameter:\n\n# Create the bar chart\npx.bar(summary_df, x=\"sex\", y=\"mean_tip\", error_y=\"std_tip\")\n\n                                                \n\n\nThis bar chart displays the average tip amount for each gender, with error bars representing the standard deviation.\n\n\n\n\n\n\nPractice\n\n\n\n8.5.6 Practice: Average Total Bill by Day\nUsing the tips dataset, create a bar chart of mean total_bill by day with standard deviation error bars. You should copy and paste the code from the example above and modify it to create this plot.\nAccording to the plot, which day has the highest average total bill?\n\ntips.head()  # View the tips dataset\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSide Note: Difference between px.bar and px.histogram\n\n\n\nNotice that this is the first time we are using the px.bar function. For past plots, we have used px.histogram to make bar charts.\nThe bar chart function generally expects that the numeric variable being plotted is already in it’s own column, while the histogram function does the grouping for you.\nFor example, in the cell below, we use px.histogram to make a bar chart of the sex column. The resulting plot compares the number of male and female customers in the dataset.\n\npx.histogram(tips, x='sex')\n\n                                                \n\n\nTo make the same plot using px.bar, we first need to group by the sex column and count the number of rows for each sex.\n\nsex_counts = tips['sex'].value_counts().reset_index()\nsex_counts\n\n\n\n\n\n\n\n\nsex\ncount\n\n\n\n\n0\nMale\n157\n\n\n1\nFemale\n87\n\n\n\n\n\n\n\nWe can then plot the day column using px.bar:\n\npx.bar(sex_counts, x=\"sex\", y=\"count\")\n\n                                                \n\n\nThis produces a bar chart with one bar for each sex.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "href": "p_data_on_display_multivariate.html#categorical-vs.-categorical-data",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.6 Categorical vs. Categorical Data",
    "text": "8.6 Categorical vs. Categorical Data\nWhen both variables are categorical, bar charts with a color axis are effective for visualizing the frequency distribution across categories. We will focus on three types of bar charts: stacked bar charts, percent-stacked bar charts, and grouped/clustered bar charts.\n\n8.6.1 Stacked Bar Charts\nStacked bar charts show the total counts and the breakdown within each category. To make a stacked bar chart, use the color parameter to specify the categorical variable:\n\npx.histogram(\n    tips,\n    x='day',\n    color='sex'\n)\n\n                                                \n\n\nLet’s add numbers to the bars to show the exact counts, and also improve the color palette with custom colors.\n\npx.histogram(\n    tips,\n    x=\"day\",\n    color=\"sex\",\n    text_auto=True,\n    color_discrete_sequence=[\"#deb221\", \"#2f828a\"],\n)\n\n                                                \n\n\nThis stacked bar chart shows the total number of customers each day, broken down by gender.\n\n\n\n\n\n\nPractice\n\n\n\n8.6.2 Practice: High and Low Income Countries by Continent\nUsing the g_2007_income dataset, create a stacked bar chart showing the count of high and low income countries in each continent.\n\ngap_dat = px.data.gapminder()\n\ng_2007_income = (\n    gap_dat.query(\"year == 2007\")\n    .drop(columns=[\"year\", \"iso_alpha\", \"iso_num\"])\n    .assign(\n        income_group=lambda df: np.where(\n            df.gdpPercap &gt; 15000, \"High Income\", \"Low & Middle Income\"\n        )\n    )\n)\n\ng_2007_income.head()\n# Your code here\n\n\n\n\n\n\n\n\ncountry\ncontinent\nlifeExp\npop\ngdpPercap\nincome_group\n\n\n\n\n11\nAfghanistan\nAsia\n43.828\n31889923\n974.580338\nLow & Middle Income\n\n\n23\nAlbania\nEurope\n76.423\n3600523\n5937.029526\nLow & Middle Income\n\n\n35\nAlgeria\nAfrica\n72.301\n33333216\n6223.367465\nLow & Middle Income\n\n\n47\nAngola\nAfrica\n42.731\n12420476\n4797.231267\nLow & Middle Income\n\n\n59\nArgentina\nAmericas\n75.320\n40301927\n12779.379640\nLow & Middle Income\n\n\n\n\n\n\n\n\n\n\n\n8.6.3 Percent-Stacked Bar Charts\nTo show proportions instead of counts, we can create percent-stacked bar charts by setting the barnorm parameter to 'percent':\n\n# Create the percent-stacked bar chart\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\")\n\n                                                \n\n\nThis chart normalizes the bar heights to represent percentages, showing the proportion of each gender for each day.\nWe can also add text labels to the bars to show the exact percentages:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barnorm=\"percent\", text_auto=\".1f\")\n\n                                                \n\n\nThe symbol .1f in the text_auto parameter formats the text labels to one decimal place.\n\n\n\n\n\n\nPractice\n\n\n\n8.6.4 Practice: Proportion of High and Low Income Countries by Continent\nAgain using the g_2007_income dataset, create a percent-stacked bar chart showing the proportion of high and low income countries in each continent. Add text labels to the bars to show the exact percentages.\nAccording the plot, which continent has the highest proportion of high income countries? Are there any limitations to this plot?\n\n# Your code here\n\n\n\n\n\n8.6.5 Clustered Bar Charts\nFor clustered bar charts, set the barmode parameter to 'group' to place the bars for each category side by side:\n\npx.histogram(tips, x=\"day\", color=\"sex\", barmode=\"group\")\n\n                                                \n\n\nThis layout makes it easier to compare values across categories directly.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#time-series-data",
    "href": "p_data_on_display_multivariate.html#time-series-data",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.7 Time Series Data",
    "text": "8.7 Time Series Data\nTime series data represents observations collected at different points in time. It’s crucial for analyzing trends, patterns, and changes over time. Let’s explore some basic time series visualizations using Nigeria’s population data from the Gapminder dataset.\nFirst, let’s prepare our data:\n\n# Load the Gapminder dataset\ngapminder = px.data.gapminder()\n\n# Subset the data for Nigeria\nnigeria_pop = gapminder.query('country == \"Nigeria\"')[['year', 'pop']]\nnigeria_pop\n\n\n\n\n\n\n\n\nyear\npop\n\n\n\n\n1128\n1952\n33119096\n\n\n1129\n1957\n37173340\n\n\n1130\n1962\n41871351\n\n\n1131\n1967\n47287752\n\n\n1132\n1972\n53740085\n\n\n1133\n1977\n62209173\n\n\n1134\n1982\n73039376\n\n\n1135\n1987\n81551520\n\n\n1136\n1992\n93364244\n\n\n1137\n1997\n106207839\n\n\n1138\n2002\n119901274\n\n\n1139\n2007\n135031164\n\n\n\n\n\n\n\n\n8.7.1 Bar Chart\nA bar chart can be used to plot time series data.\n\n# Bar chart\npx.bar(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThis bar chart gives us a clear view of how Nigeria’s population has changed over the years, with each bar representing the population at a specific year.\n\n\n8.7.2 Line Chart\nA line chart is excellent for showing continuous changes over time:\n\n# Line chart\npx.line(nigeria_pop, x=\"year\", y=\"pop\")\n\n                                                \n\n\nThe line chart connects the population values, making it easier to see the overall trend of population growth.\nAdding markers to a line chart can highlight specific data points:\n\n# Line chart with points\npx.line(nigeria_pop, x='year', y='pop', markers=True)\n\n                                                \n\n\nWe can also compare the population growth of multiple countries by adding a color parameter:\n\nnigeria_ghana = gapminder.query('country in [\"Nigeria\", \"Ghana\"]')\npx.line(nigeria_ghana, x=\"year\", y=\"pop\", color=\"country\", markers=True)\n\n                                                \n\n\nThis chart allows us to compare the population trends of Nigeria and Ghana over time.\n\n\n\n\n\n\nPractice\n\n\n\n8.7.3 Practice: GDP per Capita Time Series\nUsing the Gapminder dataset, create a time series visualization for the GDP per capita of Iraq.\n\n# Your code here\n\nWhat happened to Iraq in the 1980s that might explain the graph shown?",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "href": "p_data_on_display_multivariate.html#plots-with-three-or-more-variables",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.8 Plots with three or more variables",
    "text": "8.8 Plots with three or more variables\nAlthough bivariate visualizations are the most common types of visualizations, plots with three or more variables are also sometimes useful. Let’s explore a few examples.\n\n8.8.1 Bubble Charts\nBubble charts show the relationship between three variables by mapping the size of the points to a third variable. Below, we plot the relationship between gdpPercap and lifeExp with the size of the points representing the population of the country.\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\")\n\n                                                \n\n\nWe can easily spot the largest countries by population, such as China, India, and the United States. We can also add a color axis to differentiate between continents:\n\npx.scatter(g_2007, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", color=\"continent\")\n\n                                                \n\n\nNow we have four different variables being plotted:\n\ngdpPercap on the x-axis\nlifeExp on the y-axis\npop as the size of the points\ncontinent as the color of the points\n\n\n\n\n\n\n\nPractice\n\n\n\n8.8.2 Practice: Tips Bubble Chart\nUsing the tips dataset, create a bubble chart showing the relationship between total_bill and tip with the size of the points representing the size of the party, and the color representing the day of the week.\nUse the plot to answer the question:\n\nThe highest two tip amounts were on which days and what was the table size?\n\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n8.8.3 Facet Plots\nFaceting splits a single plot into multiple plots, with each plot showing a different subset of the data. This is useful for comparing distributions across subsets.\nFor example, we can facet the bubble chart by continent:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n)\n\n                                                \n\n\nWe can change the arrangement of the facets by changing the facet_col_wrap parameter. For example, facet_col_wrap=2 will wrap the facets into two columns:\n\npx.scatter(\n    g_2007,\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    facet_col=\"continent\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nSimilarly, we can facet the violin plots of tips by day of the week:\n\npx.violin(\n    tips,\n    x=\"sex\",\n    y=\"tip\",\n    color=\"sex\",\n    facet_col=\"day\",\n    facet_col_wrap=2,\n)\n\n                                                \n\n\nFaceting allows us to compare distributions across different days, providing more granular insights.\n\n\n\n\n\n\nPractice\n\n\n\n8.8.4 Practice: Tips Facet Plot\nUsing the tips dataset, create a percent-stacked bar chart of the time column, colored by the sex column, and facetted by the day column.\nWhich day-time has the highest proportion of male customers (e.g. Friday Lunch, Saturday Dinner, etc.)?\n\ntips.head()\n# Your code here\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_data_on_display_multivariate.html#summary",
    "href": "p_data_on_display_multivariate.html#summary",
    "title": "8  Bivariate & Multivariate Graphs with Plotly Express",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nIn this lesson, you learned how to create bivariate and multivariate graphs using Plotly Express. Understanding these visualization techniques will help you explore and communicate relationships in your data more effectively.\nSee you in the next lesson!",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bivariate & Multivariate Graphs with Plotly Express</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html",
    "href": "p_tools_installing_python.html",
    "title": "9  Installing Python",
    "section": "",
    "text": "10 Introduction\nSo far in our learning sequence, you’ve been working mostly in Google Colab, a convenient, cloud-based environment for running Python code. Now it’s time to tackle working locally on your own machine. This allows you to work offline and gives you more control over your development environment.\nWe’ll guide you through installing Python 3.12.0 to ensure compatibility with the examples and exercises we’ll cover. Let’s get started!\nBefore we proceed, we’ll split the instructions into two tracks:\nPlease follow the instructions specific to your operating system.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#windows-users",
    "href": "p_tools_installing_python.html#windows-users",
    "title": "9  Installing Python",
    "section": "10.1 Windows Users",
    "text": "10.1 Windows Users\n\n10.1.1 Step 1: Check Your Current Python Version\n\nPress Win + R, type cmd, and press Enter to open Command Prompt.\nType python --version and press Enter.\nIf Python is installed, it will display the version number.\nNote: If you see a version lower than 3.12.0, we’ll proceed to install the correct version.\n\n\n\n10.1.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nScroll down to the bottom of the Python 3.12.0 page to find the “Windows installer (64-bit)”. Then click on the link to download the installer.\n\n\n\n10.1.3 Step 3: Install Python\n\nLocate the downloaded file (usually in your Downloads folder).\nDouble-click the installer to run it.\nImportant: Check the box that says “Add Python 3.12 to PATH” at the bottom of the installer window.\nIf you have the option, also elect to use admin privileges.\nAt the end you may be asked whether to disable path length limit. Click yes to this too.\nWait for the installation to finish.\n\n\n\n10.1.4 Step 4: Verify the Installation\n\nClose your old command prompt window if it is still open.\nIn your start menu, search for cmd to open a new command prompt.\nType python --version and press Enter.\nYou should see Python 3.12.0. (If you see a newer version if you already have Python on your computer. That’s okay. From our IDE, we will be able to select the correct version of Python that we just installed.)\n\n\n\n10.1.5 Step 5: Run Python Locally\n\nIn Command Prompt, type python and press Enter.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Enter.\nYou should see 4.\nType exit() and press Enter.\n\nYay! You’ve successfully installed Python and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_installing_python.html#macos-users",
    "href": "p_tools_installing_python.html#macos-users",
    "title": "9  Installing Python",
    "section": "10.2 macOS Users",
    "text": "10.2 macOS Users\n\n10.2.1 Step 1: Check Your Current Python Version\n\nGo to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nIf Python is installed, it will display the version number.\nYou may get a pop-up asking to install command line developer tools if you do not have them. Go ahead and accept and install that.\nOtherwise, you may get a pop-up asking to install something. Click Cancel and proceed to the next step.\nIf you already have version 3.12.0 installed, you can skip the rest of the steps. If you have any other version (higher or lower), proceed to the next step.\n\n\n\n10.2.2 Step 2: Download Python 3.12.0\n\nOpen your web browser.\nGo to Google and search for “Python 3.12.0”.\nIn the search results, find the link to python.org that mentions Python 3.12.0.\nEnsure the URL is from www.python.org to avoid unofficial sources.\nScroll down to the bottom of the Python 3.12.0 page.\nUnder the “Files” section, find “macOS 64-bit universal2 installer”.\nClick on the link to download the installer.\n\n\n\n10.2.3 Step 3: Install Python 3.12.0\n\nLocate the downloaded .pkg file (usually in your Downloads folder).\nDouble-click the installer to run it.\nClick “Continue” through the prompts.\nRead and agree to the license terms.\nClick “Install”.\nEnter your administrator password when prompted.\nClick “Install Software”.\nWait for the installation to finish.\nClick “Close” once the installation is complete.\n\n\n\n10.2.4 Step 4: Verify the Installation\n\nGo to Applications &gt; Utilities &gt; Terminal.\nType python3 --version and press Return.\nYou should see Python 3.12.0.\n\n\n\n10.2.5 Step 5: Run Python Locally\n\nIn Terminal, type python3 and press Return.\nAt the &gt;&gt;&gt; prompt, type 2 + 2 and press Return.\nYou should see 4.\nType exit() and press Return.\n\nYay! You’ve successfully installed Python 3.12.0 on your Mac and you ran your first local Python command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html",
    "href": "p_tools_using_vscode.html",
    "title": "10  Installing and Using VS Code",
    "section": "",
    "text": "10.1 Introduction\nIn this lesson, we will explore Visual Studio Code (VS Code), a powerful and versatile editor for writing, running, and debugging Python code. VS Code is widely used due to its rich feature set and extensive extension library, which makes coding more efficient and enjoyable.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#learning-objectives",
    "href": "p_tools_using_vscode.html#learning-objectives",
    "title": "10  Installing and Using VS Code",
    "section": "10.2 Learning Objectives",
    "text": "10.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nOpen VS Code and navigate to the editor, terminal, and explorer tabs.\nCreate a new Python file and run it using the Python extension.\nUse the Command Palette to search for and select a color theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-vs-code",
    "href": "p_tools_using_vscode.html#installing-vs-code",
    "title": "10  Installing and Using VS Code",
    "section": "10.3 Installing VS Code",
    "text": "10.3 Installing VS Code\nVS Code is available for Windows, macOS, and Linux. Search for “VS Code” in your favorite search engine and go to the official website, code.visualstudio.com. Download the version for your computer’s operating system.\nAfter installing VS Code, you may need to drag the icon to your applications folder.\nFrom your applications folder, open VS Code.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "href": "p_tools_using_vscode.html#navigating-the-explorer-tab",
    "title": "10  Installing and Using VS Code",
    "section": "10.4 Navigating the Explorer Tab",
    "text": "10.4 Navigating the Explorer Tab\nThe Explorer tab displays the files and folders in your current workspace. When you open VS Code for the first time, it may indicate that you have not yet opened a folder. Most of our work in Python will be organized in folders (also known as workspaces) that contain multiple files.\nLet’s create our first workspace:\n\nOn your computer, navigate to your desktop or another memorable location.\nCreate a new folder called first_python_workspace.\nIn VS Code, click on the “Open Folder” button and locate your newly created folder. Alternatively, you can drag the folder into the VS Code window.\n\nNow that your workspace is open in VS Code, you can create a new file:\n\nIn the Explorer tab, right-click inside the workspace folder.\nSelect “New File”.\nName the file first_script.py. The file will automatically open in the editor.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "href": "p_tools_using_vscode.html#writing-and-saving-python-code",
    "title": "10  Installing and Using VS Code",
    "section": "10.5 Writing and Saving Python Code",
    "text": "10.5 Writing and Saving Python Code\nLet’s write some Python code in your new file:\nprint(2 + 2)\nTo save the file, press Ctrl + S (or Cmd + S on macOS).",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#installing-the-python-extension",
    "href": "p_tools_using_vscode.html#installing-the-python-extension",
    "title": "10  Installing and Using VS Code",
    "section": "10.6 Installing the Python Extension",
    "text": "10.6 Installing the Python Extension\nTo run Python code within VS Code, we need to install the Python extension:\n\nClick on the Extensions tab on the left sidebar (it looks like four squares).\nIn the search bar at the top, type “Python”.\nFind the extension named “Python” published by Microsoft, and click Install.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#running-your-python-script",
    "href": "p_tools_using_vscode.html#running-your-python-script",
    "title": "10  Installing and Using VS Code",
    "section": "10.7 Running Your Python Script",
    "text": "10.7 Running Your Python Script\nWith the Python extension installed, you can now run your script:\n\nOpen first_script.py if it’s not already open.\nClick on the Run icon (a play button) in the top right corner of the editor.\n\nAlternatively, you can right-click inside the editor and select RUn python then “Run Python File in Terminal”.\n\nA terminal window will open at the bottom of the screen, and your code will execute. You should see the output:\n4",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#using-the-command-palette",
    "href": "p_tools_using_vscode.html#using-the-command-palette",
    "title": "10  Installing and Using VS Code",
    "section": "10.8 Using the Command Palette",
    "text": "10.8 Using the Command Palette\nThe Command Palette is a powerful feature in VS Code that allows you to access various commands and settings:\nTo access it, click on the search bar at the top of vscode window, then either press &gt; or select the ‘show and run commands’ option.\nthen type ‘theme’ and select ‘Preferences: Color Theme’.\n\nUse the up and down arrow keys to cycle through the available color themes.\nPress Enter to select your preferred theme.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_using_vscode.html#wrap-up",
    "href": "p_tools_using_vscode.html#wrap-up",
    "title": "10  Installing and Using VS Code",
    "section": "10.9 Wrap Up",
    "text": "10.9 Wrap Up\nIn this lesson, we’ve:\n\nInstalled VS Code and set up a workspace.\nCreated and saved a Python script.\nInstalled the Python extension to enable running and debugging code.\nUsed the Command Palette to customize the editor’s appearance.\n\nHappy coding!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Installing and Using VS Code</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html",
    "href": "p_tools_venv.html",
    "title": "11  Folders and Virtual Environments",
    "section": "",
    "text": "11.1 Introduction\nIn this lesson, we will explore virtual environments in Python using Visual Studio Code (VS Code). We’ll create a new workspace, set up a virtual environment, and install packages specific to our project.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#learning-objectives",
    "href": "p_tools_venv.html#learning-objectives",
    "title": "11  Folders and Virtual Environments",
    "section": "11.2 Learning Objectives",
    "text": "11.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate a new workspace in VS Code\nUnderstand the concept of virtual environments\nCreate and use a virtual environment in VS Code\nInstall and use packages within a virtual environment",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-new-workspace",
    "href": "p_tools_venv.html#creating-a-new-workspace",
    "title": "11  Folders and Virtual Environments",
    "section": "11.3 Creating a New Workspace",
    "text": "11.3 Creating a New Workspace\n\nOn your desktop or in your Documents folder, create a new folder and name it graph_courses_python. This is going to be the main folder for many of the projects for this course so make sure you put it in an easy to reach place.\nOpen VS Code.\nGo to File &gt; Open Folder.\nNavigate to the graph_courses_python folder you just created and select it.\n\nnow create a new script and call it test_cowsay.py.\nType print(2 + 2) in the file then run it with the button to make sure everything is working.\nNext, let’s try to import a package we haven’t installed yet. Create a new Python file named test_cowsay.py and add the following line:\n\nimport cowsay\n\ncowsay.cow(\"Hello, World!\")\n\nThis will not work because we haven’t installed the cowsay package yet. To install it properly, we’ll need to use virtual environments.\n\n11.3.1 Environments & Interpreter\n\nEnvironments are for package management.\nThe interpreter is the Python version you’re using.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#creating-a-virtual-environment",
    "href": "p_tools_venv.html#creating-a-virtual-environment",
    "title": "11  Folders and Virtual Environments",
    "section": "11.4 Creating a Virtual Environment",
    "text": "11.4 Creating a Virtual Environment\n\nOpen the Command Palette (View &gt; Command Palette or Ctrl+Shift+P).\nType Python: Create Environment and select it.\nChoose Venv as the environment type.\nSelect the Python interpreter you want to use (e.g., Python 3.12.0).\n\nYou should now see a new folder called .venv. This is the virtual environment. Inside it is a folder called lib, which contains packages.\nNext, tell VS Code to use this virtual environment:\n\nOpen the Command Palette again.\nType Python: Select Interpreter and choose it.\nSelect the interpreter associated with your virtual environment (it should be listed under .venv).\n\nNow we’ve created and selected our virtual environment. We can install packages without affecting other projects.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-packages",
    "href": "p_tools_venv.html#installing-packages",
    "title": "11  Folders and Virtual Environments",
    "section": "11.5 Installing Packages",
    "text": "11.5 Installing Packages\nLet’s install the cowsay package.\n\nOpen a new terminal in VS Code. You can do this with the terminal file menu option then selecting ‘new terminal’.\nEnsure the terminal is using your virtual environment. You can do this by hovering over the terminal icon in terminal window. It should mention that there is an activated environment for ….venv among other things\nRun the following command:\n\n#| eval: false\npip install cowsay\nSometimes pip disappears off path randomly. Restart vscode, select interpreter again, and it should fix it. Seems to be some bug with VS Code.\nRefresh your environment if necessary. You should now see the cowsay package in the list of installed packages.\nNow we can use the package. Open test_cowsay.py and click the run button to execute the script.\nYou should see a cow saying hello!",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#summary-of-key-steps",
    "href": "p_tools_venv.html#summary-of-key-steps",
    "title": "11  Folders and Virtual Environments",
    "section": "11.6 Summary of Key Steps",
    "text": "11.6 Summary of Key Steps\nCongrats! You have now created a virtual environment and installed a package.\nThese are key steps for any new Python project:\n\nFolder: Create a project folder.\nEnvironment: Set up a virtual environment.\nInterpreter: Select the appropriate Python interpreter.\nLibraries: Install the necessary packages.\n\nRemember the acronym FEIL to help you recall these steps. (If you don’t complete these steps you increase your chances of “FEIL”ure 😅) It’s a bit of a pain to have to set up a virtual environment every time you start a new project, but it’s a good habit to get into.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#demonstrating-environment-isolation",
    "href": "p_tools_venv.html#demonstrating-environment-isolation",
    "title": "11  Folders and Virtual Environments",
    "section": "11.7 Demonstrating Environment Isolation",
    "text": "11.7 Demonstrating Environment Isolation\nLet’s demonstrate that the virtual environment is isolated.\n\nOpen your previous workspace my_first_workspace with the File &gt; Open Folder menu option.\nCreate a Python file and try to use the cowsay package:\n::: {#677ba163 .cell execution_count=2} {.python .cell-code}  import cowsay :::\n\nThis will probably not work because cowsay is not installed in that environment. If it does work, it means you have cowsay installed globally, which is okay.\nNow, let’s return to our main folder/workspace. This is where you’ll conduct most of your analysis for this course.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#installing-course-packages",
    "href": "p_tools_venv.html#installing-course-packages",
    "title": "11  Folders and Virtual Environments",
    "section": "11.8 Installing Course Packages",
    "text": "11.8 Installing Course Packages\nAs a final step, let’s install the packages we’ll need for the course. While we could install each package as we encounter it, it’s more efficient to install them all at once. In the terminal, run the following command. Type these very carefully.\n#| eval: false\npip install plotly pandas jupyter ipykernel kaleido itables\n\npandas: Data manipulation library.\nplotly: Visualization library.\njupyter and ipykernel: Allow us to use Quarto to display our plots.\nkaleido: Library for saving plots in different formats.\nitables: Library for displaying tables in Quarto.\n\nWhen its done installing, your cursor in the terminal should be active again. e.g. you should be able to press enter to start a new command.\nKeep this list of packages handy for future reference, as you’ll likely need them for most projects.\nThis command will install all the required packages in one go. If your installation stops at some point, try rerunning the command. Sometimes network issues may cause the installation to fail. If it freezes for more than 10 minutes, close the terminal and rerun the command.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_venv.html#conclusion",
    "href": "p_tools_venv.html#conclusion",
    "title": "11  Folders and Virtual Environments",
    "section": "11.9 Conclusion",
    "text": "11.9 Conclusion\nYou’ve now learned how to create a workspace, set up a virtual environment, install packages, and use them in your Python projects. Remember that each project should have its own virtual environment to keep dependencies isolated and manageable.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Folders and Virtual Environments</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html",
    "href": "p_tools_quarto.html",
    "title": "12  Using Quarto",
    "section": "",
    "text": "12.1 Introduction\nA significant part of your role as a data analyst involves communicating results to others through reports. Quarto is one of the most powerful and versatile tools for producing such reports. It enables you to generate dynamic documents by combining formatted text and results produced by code. With Quarto, you can create documents in various formats such as HTML, PDF, Word, PowerPoint slides, web dashboards, and many others.\nMost of our documents in the GRAPH courses are actually written in Quarto!\nIn this lesson, we will cover the basics of this powerful tool.\nNote that for this lesson, the video may be easier to understand that the lesson notes, since there are many Graphic-user-interface steps that are hard to describe in writing.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#learning-objectives",
    "href": "p_tools_quarto.html#learning-objectives",
    "title": "12  Using Quarto",
    "section": "12.2 Learning Objectives",
    "text": "12.2 Learning Objectives\nBy the end of this lesson, you should be able to:\n\nCreate and render a Quarto document that includes Python code and narrative text.\nOutput documents in multiple formats, including HTML, PDF, Word, etc.\nUnderstand basic Markdown syntax.\nUse code chunk options to control code execution and output display.\nUse Python packages to display tables and figures in Quarto documents.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#installing-quarto",
    "href": "p_tools_quarto.html#installing-quarto",
    "title": "12  Using Quarto",
    "section": "12.3 Installing Quarto",
    "text": "12.3 Installing Quarto\nTo get started, you first need to install Quarto.\nSearch for quarto download in your favorite search engine. Then follow the instructions for your operating system.\nOn a mac this will mean downloading an .pkg or dmg file then double clicking it to start the installation.\nAfter installing, you can check that it is installed by running the following command in the command line or terminal:\nquarto --version\nNow that Quarto is installed, use it to install the tinytex package, which we will need to compile our PDFs:\nquarto install tinytex\nTo use all the features of Quarto in VSCode, we need to install the Quarto extension and the Jupyter extension. You can install these in the Extensions tab of VSCode.\nThat’s a lot of installations, but fear not—you only have to do these steps once on your computer.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#project-setup",
    "href": "p_tools_quarto.html#project-setup",
    "title": "12  Using Quarto",
    "section": "12.4 Project Setup",
    "text": "12.4 Project Setup\nTo begin, open your graph_courses_python project in VSCode.\nIf you did not watch the previous video explaining project setup, please do so now. After creating your project folder, you need to create a virtual environment, select an interpreter, and install the jupyter package.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#create-a-new-document",
    "href": "p_tools_quarto.html#create-a-new-document",
    "title": "12  Using Quarto",
    "section": "12.5 Create a New Document",
    "text": "12.5 Create a New Document\nA Quarto document is a simple text file with the .qmd extension.\nTo create a new Quarto document, create a new file and save it with a .qmd extension, for example, first_quarto_doc.qmd.\nAdd two sections to your document with the following text:\n# Section 1\n\nHello\n\n---\n\n# Section 2\n\nWorld",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#adding-code-chunks",
    "href": "p_tools_quarto.html#adding-code-chunks",
    "title": "12  Using Quarto",
    "section": "12.6 Adding Code Chunks",
    "text": "12.6 Adding Code Chunks\nYou can add code chunks to your document by using the following syntax:\n\n::: {#5a538691 .cell execution_count=1}\n``` {.python .cell-code}\n# Your code here\n:::\n\nThe shortcut to insert a code chunk is to type `Cmd + Shift + I` (on Mac) or `Ctrl + Shift + I` (on Windows). Alternatively, you can click on the \"...\" button at the top right of the screen.\n\nLet's create a code chunk that adds two numbers together and displays the result:\n\n::: {#aef0075d .cell execution_count=2}\n``` {.python .cell-code}\n2 + 2\n\n4\n\n:::\nYou should see a “Run Cell” button in the toolbar. Click this to run the code chunk.\nVSCode may prompt you to install the ipykernel package if you have not yet installed it in your current environment. Go ahead and install it. (or rewatch our video on setting up a virtual environment if you are not sure how to do this).\nNow practice adding one more code chunk at the end of the document that multiplies 3 by 3.\nAs you add these, you should see the buttons “Run Next Cell” and “Run Above” also appear.\nThere are two shortcuts you should also get used to:\n\nCmd + Enter (Mac) or Ctrl + Enter (Windows/Linux) to run the code chunk\noption + Enter (Mac) or Alt + Enter (Windows/Linux) to run the current line or the highlighted section of code.\n\nto test these, add multiple lines of code to one code chunk then pracyice ruinning the whole thing with the first shortcut, then line by line with the second.\nSo as you can see we can use Quarto as an interactive document similar to a Jupyter notebook or Google Colab. But what makes it really shine is its ability to output to many formats.\nLet’s see how to do this.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#quarto-document-header-yaml",
    "href": "p_tools_quarto.html#quarto-document-header-yaml",
    "title": "12  Using Quarto",
    "section": "12.7 Quarto Document Header (YAML)",
    "text": "12.7 Quarto Document Header (YAML)\nAt the top of the document, let’s add a YAML section. This is where we can specify details about the document, such as its title, author, and format.\n---\ntitle: \"My First Quarto Document\"\nauthor: \"Your Name\"\nformat: html\n---\nThe format is where we can specify the output format of the document. For now, we are trying to keep things simple and just use HTML.\nFor things to render properly, you need the jupyter package. Watch our video on setting up a virtual environment if you have not done this yet and are not sure how to install packages.\nender the document by clicking on the “Render” button\nYou should see a new tab open in your vscode.\nIf you go to the explorer, you should see a new file called first_quarto_doc.html.\nSo now we have the main elements of a Quarto document:\n\nThe YAML header\nSection headers\nText\nCode chunks\nThe outputs of those code chunks\n\nThese things together make a very powerful tool for reporting.\nSomething something\nformat: html: embed-resources: true\nAnother powerful feature of Quarto is its ability to output to many formats.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#output-formats",
    "href": "p_tools_quarto.html#output-formats",
    "title": "12  Using Quarto",
    "section": "12.8 Output Formats",
    "text": "12.8 Output Formats\nYou can change the format value in the YAML header to experiment with other formats.\nTry the following formats:\n\nhtml: Renders the document as an HTML webpage.\npdf: Renders the document as a PDF. You will need to have LaTeX (or tinytex) installed on your computer to use this format.\ndocx: Renders the document as a Microsoft Word document.\npptx: Renders the document as a PowerPoint presentation.\nrevealjs: Renders the document as an HTML slideshow.\ndashboard: Renders the document as an interactive dashboard.\n\nThere is a chance some of these may not work on your computer due to different operating systems or versions of software.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#markdown",
    "href": "p_tools_quarto.html#markdown",
    "title": "12  Using Quarto",
    "section": "12.9 Markdown",
    "text": "12.9 Markdown\nThe text inside Quarto documents is written in Markdown.\nMarkdown is a simple set of conventions for adding formatting to plain text. For example, to italicize text, you wrap it in asterisks *text here*, and to start a new header, you use the pound sign #. We will learn these in detail below.\nIn your editor, you can refer to the “Markdown Quick Reference” or “Cheatsheet” to learn more about Markdown syntax.\nYou can define titles of different levels by starting a line with one or more #:\n# Level 1 Title\n\n## Level 2 Title\n\n### Level 3 Title\nThe body of the document consists of text that follows the Markdown syntax. A Markdown file is a text file that contains lightweight markup to help set heading levels or format text. For example, the following text:\nThis is text with *italics* and **bold**.\n\nYou can define bulleted lists:\n\n- First element\n- Second element\nWill generate the following formatted text:\n\nThis is text with italics and bold.\nYou can define bulleted lists:\n\nFirst element\nSecond element\n\n\nNote that you need spaces before and after lists, as well as keeping the listed items on separate lines. Otherwise, they will all crunch together rather than making a list.\nWe see that words placed between asterisks are italicized, and lines that begin with a dash are transformed into a bulleted list.\nThe Markdown syntax allows for other formatting, such as the ability to insert links or images. For example, the following code:\n[Example Link](https://example.com)\n… will give the following link:\n\nExample Link\n\nWe can also embed images. In your document, you can type:\n![Alt text](images/picture_name.jpg)\nReplace “Alt text” with a description of the image (it can also be blank), “images” with the name of the image folder in your project, and “picture_name.jpg” with the name of the image you want to use. Alternatively, in some editors, you can drag and drop the image into your document.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#code-chunk-options",
    "href": "p_tools_quarto.html#code-chunk-options",
    "title": "12  Using Quarto",
    "section": "12.10 Code Chunk Options",
    "text": "12.10 Code Chunk Options\nIt is possible to pass options to each code chunk to modify its behavior.\nFor example, a code chunk looks like this:\n\n# Your code here\nx = 2 + 2\nprint(x)\n\n4\n\n\nBut you can add options to control the code chunk’s execution and display:\n\n\n4\n\n\nIn this example, the echo: false option tells Quarto not to display the code in the rendered document, only the output.\n\n12.10.1 Global Options\nYou may want to apply options globally to all code chunks in your document. You can set default code execution options in the YAML header under the execute key.\nFor example:\n---\ntitle: \"Quarto Document\"\nformat: html\nexecute:\n  echo: false\n---\nThis will set echo: false for all code chunks.\n\n\n12.10.2 Other Code Chunk Options\nHere is a comprehensive list of options you can pass to code chunks:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\neval\nEvaluate the code chunk (if false, just echoes the code into the output).\n\n\necho\nInclude the source code in output (true or false).\n\n\noutput\nInclude the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown).\n\n\nwarning\nInclude warnings in the output (true or false).\n\n\nerror\nInclude errors in the output (note that this implies that errors executing code will not halt processing of the document).\n\n\ninclude\nCatch-all for preventing any output (code or results) from being included (e.g., include: false suppresses all output from the code block).",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-tables",
    "href": "p_tools_quarto.html#displaying-tables",
    "title": "12  Using Quarto",
    "section": "12.11 Displaying Tables",
    "text": "12.11 Displaying Tables\nBy default, pandas DataFrames display adequately in Quarto. However, to make them look nicer, we can use the itables package.\nMake sure you have the itables package installed. If not, install it:\n\n!pip install itables\n\nThen, you can run something like:\n\nimport plotly.express as px\nfrom itables import show\n\ntips = px.data.tips()\nshow(tips)\n\n\n\n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.2 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nOf course, interactive tables will only work in HTML formats. We’ll look at tables for other formats later.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#displaying-plots",
    "href": "p_tools_quarto.html#displaying-plots",
    "title": "12  Using Quarto",
    "section": "12.12 Displaying Plots",
    "text": "12.12 Displaying Plots\nFor interactive plots, we can use the plotly package.\n\ntips = px.data.tips()\ntips_sex = px.violin(tips, x=\"day\", y=\"total_bill\", color=\"sex\")\ntips_sex.show()\n\n                                                \n\n\nThis will display an interactive Plotly plot in the HTML output.\nFor static outputs like PDFs and Word documents, we need to write the image to a file and then include it in the document.\nFirst, save the plot as an image:\n\ntips_sex.write_image(\"tips_sex_plot.png\")\n\nThis will create a static image file in the same folder as your document. We can then include it in the document like this:\n![Violin plot of total bill by day and sex](tips_sex_plot.png)\nThis will display the image in the output.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_tools_quarto.html#wrapping-up",
    "href": "p_tools_quarto.html#wrapping-up",
    "title": "12  Using Quarto",
    "section": "12.13 Wrapping Up",
    "text": "12.13 Wrapping Up\nIn this lesson, we learned how to create and render Quarto documents, add formatting, and include code chunks.\nWe also learned how to use code chunk options to control the behavior of our documents.\nWe experimented with different output formats and how to customize the display of our documents.\nWith these tools, you can create dynamic and interactive reports that are easily shareable in various formats. Quarto’s flexibility and integration with Python make it an excellent choice for data analysts and researchers alike.",
    "crumbs": [
      "Tools for Local Python",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html",
    "href": "p_untangled_subset_columns.html",
    "title": "13  Subsetting columns",
    "section": "",
    "text": "13.1 Introduction\nToday we will begin our exploration of pandas for data manipulation!\nOur first focus will be on selecting and renaming columns. Often your dataset comes with many columns that you do not need, and you would like to narrow it down to just a few. Pandas makes this easy. Let’s see how.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#learning-objectives",
    "href": "p_untangled_subset_columns.html#learning-objectives",
    "title": "13  Subsetting columns",
    "section": "13.2 Learning objectives",
    "text": "13.2 Learning objectives\n\nYou can keep or drop columns from a DataFrame using pandas methods like loc[], filter(), and drop().\nYou can select columns based on regex patterns with filter().\nYou can use rename() to change column names.\nYou can use regex to clean column names.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#about-pandas",
    "href": "p_untangled_subset_columns.html#about-pandas",
    "title": "13  Subsetting columns",
    "section": "13.3 About pandas",
    "text": "13.3 About pandas\nPandas is a popular library for data manipulation and analysis. It is designed to make it easy to work with tabular data in Python.\nInstall pandas with the following command in your terminal if it is not already installed:\n\npip install pandas \n\nThen import pandas with the following command in your script:\n\nimport pandas as pd",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_subset_columns.html#the-yaounde-covid-19-dataset",
    "title": "13  Subsetting columns",
    "section": "13.4 The Yaounde COVID-19 dataset",
    "text": "13.4 The Yaounde COVID-19 dataset\nIn this lesson, we analyse results from a COVID-19 survey conducted in Yaounde, Cameroon in late 2020. The survey estimated how many people had been infected with COVID-19 in the region, by testing for antibodies.\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nLet’s load and examine the dataset:\n\nyao = pd.read_csv(\"data/yaounde_data.csv\")\nyao\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "href": "p_untangled_subset_columns.html#selecting-columns-with-square-brackets",
    "title": "13  Subsetting columns",
    "section": "13.5 Selecting columns with square brackets []",
    "text": "13.5 Selecting columns with square brackets []\nIn pandas, the most common way to select a column is simply to use square brackets [] and the column name. For example, to select the age and sex columns, we type:\n\nyao[[\"age\", \"sex\"]]\n\n\n\n\n\n\n\n\nage\nsex\n\n\n\n\n0\n45\nFemale\n\n\n1\n55\nMale\n\n\n...\n...\n...\n\n\n969\n31\nFemale\n\n\n970\n17\nFemale\n\n\n\n\n971 rows × 2 columns\n\n\n\nNote the double square brackets [[]]. Without it, you will get an error:\n\nyao[\"age\", \"sex\"]\n\nKeyError: ('age', 'sex')\nIf you want to select a single column, you may omit the double square brackets, but your output will no longer be a DataFrame. Compare the following:\n\nyao[\"age\"] # does not return a DataFrame\n\n0      45\n1      55\n       ..\n969    31\n970    17\nName: age, Length: 971, dtype: int64\n\n\n\nyao[[\"age\"]]  # returns a DataFrame\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\n\n\n\n\n\n\nKey Point\n\n\n\n13.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n13.6.1 Practice Q: Select Columns with []\n\nUse the [] operator to select the “weight_kg” and “height_cm” variables in the yao DataFrame. Assign the result to a new DataFrame called yao_weight_height. Then print this new DataFrame.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro tip\n\n\n\nThere are many ways to select columns in pandas. In your free time, you may choose to explore the .loc[] and .take() methods, which provide additional functionality.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#storing-data-subsets",
    "href": "p_untangled_subset_columns.html#storing-data-subsets",
    "title": "13  Subsetting columns",
    "section": "13.6 Storing data subsets",
    "text": "13.6 Storing data subsets\nNote that these selections are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset with only three columns:\n\nyao_subset = yao[[\"age\", \"sex\", \"igg_result\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\nsex\nigg_result\n\n\n\n\n0\n45\nFemale\nNegative\n\n\n1\n55\nMale\nPositive\n\n\n...\n...\n...\n...\n\n\n969\n31\nFemale\nNegative\n\n\n970\n17\nFemale\nNegative\n\n\n\n\n971 rows × 3 columns\n\n\n\nAnd if we want to overwrite a DataFrame, we can assign the subset back to the original DataFrame. Let’s overwrite the yao_subset DataFrame to have only the age column:\n\nyao_subset = yao_subset[[\"age\"]]\nyao_subset\n\n\n\n\n\n\n\n\nage\n\n\n\n\n0\n45\n\n\n1\n55\n\n\n...\n...\n\n\n969\n31\n\n\n970\n17\n\n\n\n\n971 rows × 1 columns\n\n\n\nThe yao_subset DataFrame has gone from having 3 columns to having 1 column.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "href": "p_untangled_subset_columns.html#excluding-columns-with-drop",
    "title": "13  Subsetting columns",
    "section": "13.7 Excluding columns with drop()",
    "text": "13.7 Excluding columns with drop()\nSometimes it is more useful to drop columns you do not need than to explicitly select the ones that you do need.\nTo drop columns, we can use the drop() method with the columns argument. To drop the age column, we type:\n\nyao.drop(columns=[\"age\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nsex\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\nNon-smoker\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\nEx-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\nNon-smoker\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 52 columns\n\n\n\nTo drop several columns:\n\nyao.drop(columns=[\"age\", \"sex\"])\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\nAgain, note that this is not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset age and sex dropped:\n\nyao_subset = yao.drop(columns=[\"age\", \"sex\"])\nyao_subset\n\n\n\n\n\n\n\n\nid\ndate_surveyed\nage_category\nage_category_3\nhighest_education\noccupation\nweight_kg\nheight_cm\nis_smoker\nis_pregnant\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45 - 64\nAdult\nSecondary\nInformal worker\n95\n169\nNon-smoker\nNo\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n45 - 64\nAdult\nUniversity\nSalaried worker\n96\n185\nEx-smoker\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n30 - 44\nAdult\nSecondary\nUnemployed\n66\n169\nNon-smoker\nNo\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n15 - 29\nChild\nSecondary\nUnemployed\n67\n162\nNon-smoker\nNo response\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 51 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n13.7.1 Practice Q: Drop Columns with drop()\n\nFrom the yao DataFrame, remove the columns highest_education and consultation. Assign the result to a new DataFrame called yao_no_education_consultation. Print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "href": "p_untangled_subset_columns.html#using-filter-to-select-columns-by-regex",
    "title": "13  Subsetting columns",
    "section": "13.8 Using filter() to select columns by regex",
    "text": "13.8 Using filter() to select columns by regex\nThe filter() method and its regex argument offer a powerful way to select columns based on patterns in their names. As an example, to select columns containing the string “ig”, we can write:\n\nyao.filter(regex=\"ig\")\n\n\n\n\n\n\n\n\nhighest_education\nweight_kg\nheight_cm\nneighborhood\nigg_result\nigm_result\nsymp_fatigue\n\n\n\n\n0\nSecondary\n95\n169\nBriqueterie\nNegative\nNegative\nNo\n\n\n1\nUniversity\n96\n185\nBriqueterie\nPositive\nNegative\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nSecondary\n66\n169\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n970\nSecondary\n67\n162\nTsinga Oliga\nNegative\nNegative\nNo\n\n\n\n\n971 rows × 7 columns\n\n\n\nThe argument regex specifies the pattern to match. Regex stands for regular expression and refers to a sequence of characters that define a search pattern.\nTo select columns starting with the string “ig”, we write:\n\nyao.filter(regex=\"^ig\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe symbol ^ is a regex character that matches the beginning of the string.\nTo select columns ending with the string “result”, we can write:\n\nyao.filter(regex=\"result$\")\n\n\n\n\n\n\n\n\nigg_result\nigm_result\n\n\n\n\n0\nNegative\nNegative\n\n\n1\nPositive\nNegative\n\n\n...\n...\n...\n\n\n969\nNegative\nNegative\n\n\n970\nNegative\nNegative\n\n\n\n\n971 rows × 2 columns\n\n\n\nThe character $ is regex that matches the end of the string.\n\n\n\n\n\n\nPractice\n\n\n\n13.8.1 Practice Q: Select Columns with Regex\n\nSelect all columns in the yao DataFrame that start with “is”. Assign the result to a new DataFrame called yao_is_columns. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "href": "p_untangled_subset_columns.html#change-column-names-with-rename",
    "title": "13  Subsetting columns",
    "section": "13.9 Change column names with rename()",
    "text": "13.9 Change column names with rename()\nWe can use the rename() method to change column names:\n\nyao.rename(columns={\"age\": \"patient_age\", \"sex\": \"patient_sex\"})\n\n\n\n\n\n\n\n\nid\ndate_surveyed\npatient_age\nage_category\nage_category_3\npatient_sex\nhighest_education\noccupation\nweight_kg\nheight_cm\n...\nis_drug_antibio\nis_drug_hydrocortisone\nis_drug_other_anti_inflam\nis_drug_antiviral\nis_drug_chloro\nis_drug_tradn\nis_drug_oxygen\nis_drug_other\nis_drug_no_resp\nis_drug_none\n\n\n\n\n0\nBRIQUETERIE_000_0001\n2020-10-22\n45\n45 - 64\nAdult\nFemale\nSecondary\nInformal worker\n95\n169\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nBRIQUETERIE_000_0002\n2020-10-24\n55\n45 - 64\nAdult\nMale\nUniversity\nSalaried worker\n96\n185\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\nTSINGAOLIGA_026_0002\n2020-11-11\n31\n30 - 44\nAdult\nFemale\nSecondary\nUnemployed\n66\n169\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n970\nTSINGAOLIGA_026_0003\n2020-11-11\n17\n15 - 29\nChild\nFemale\nSecondary\nUnemployed\n67\n162\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n971 rows × 53 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n13.9.1 Practice Q: Rename Columns with rename()\n\nRename the age_category column in the yao DataFrame to age_cat. Assign the result to a new DataFrame called yao_age_cat. Then print this new DataFrame.\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "href": "p_untangled_subset_columns.html#cleaning-messy-column-names",
    "title": "13  Subsetting columns",
    "section": "13.10 Cleaning messy column names",
    "text": "13.10 Cleaning messy column names\nFor automatic cleaning of column names, you can use regular expressions with the str.replace() method in pandas. This will allow you to replace everything that is not a letter or a number with an underscore (‘_’).\nHere’s how you can do it on a test DataFrame with messy column names. Messy column names are names with spaces, special characters, or other non-alphanumeric characters.\n\ntest_df = pd.DataFrame({\"good_name\": [1], \"bad name\": [2], \"bad*@name*2\": [3]})\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad name\nbad*@name*2\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n\n\n\nSuch column names are not ideal because, for example, we cannot select them with the dot operator the way we can for clean names:\n\ntest_df.good_name  # this works\n\n0    1\nName: good_name, dtype: int64\n\n\nBut this does not work:\n\ntest_df.bad name\n\n      test_df.bad name\n                 ^\nSyntaxError: invalid syntax\nWe can automatically clean such names using the str.replace() method along with regular expressions.\n\nclean_names = test_df.columns.str.replace(r'[^a-zA-Z0-9]', '_', regex=True)\n\nThe regular expression r'[^a-zA-Z0-9]' matches any character that is not a letter (either uppercase or lowercase) or a digit. The str.replace() method replaces these characters with an underscore (‘_’) to make the column names more legible and usable in dot notation.\nNow we can replace the column names in the DataFrame with the cleaned names:\n\ntest_df.columns = clean_names\ntest_df\n\n\n\n\n\n\n\n\ngood_name\nbad_name\nbad__name_2\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n13.10.1 Practice Q: Clean Column Names with Regex\n\nConsider the data frame defined below with messy column names. Use the str.replace() method to clean the column names.\n\n\ncleaning_practice = pd.DataFrame(\n    {\"Aloha\": range(3), \"Bell Chart\": range(3), \"Animals@#$%^\": range(3)}\n)\ncleaning_practice\n\n\n\n\n\n\n\n\nAloha\nBell Chart\nAnimals@#$%^\n\n\n\n\n0\n0\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n2\n2\n2",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_subset_columns.html#wrap-up",
    "href": "p_untangled_subset_columns.html#wrap-up",
    "title": "13  Subsetting columns",
    "section": "13.11 Wrap up",
    "text": "13.11 Wrap up\nHopefully this lesson has shown you how intuitive and useful pandas is for data manipulation!\nThis is the first of a series of basic data wrangling techniques: see you in the next lesson to learn more.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Subsetting columns</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html",
    "href": "p_untangled_query_rows.html",
    "title": "14  Subsetting rows",
    "section": "",
    "text": "14.1 Intro\nIn our previous lesson we saw how to select variables (columns). In this lesson we will see how to keep or drop data entries.\nDropping abnormal data entries or keeping subsets of your data points is another essential aspect of data wrangling.\nLet’s get started!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#learning-objectives",
    "href": "p_untangled_query_rows.html#learning-objectives",
    "title": "14  Subsetting rows",
    "section": "14.2 Learning objectives",
    "text": "14.2 Learning objectives\n\nYou can use the query() method to keep or drop rows from a DataFrame.\nYou can specify conditions using relational operators like greater than (&gt;), less than (&lt;), equal to (==), and not equal to (!=).\nYou can combine conditions with & and |.\nYou can negate conditions with ~.\nYou can use the isna() and notna() methods.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_query_rows.html#the-yaounde-covid-19-dataset",
    "title": "14  Subsetting rows",
    "section": "14.3 The Yaounde COVID-19 dataset",
    "text": "14.3 The Yaounde COVID-19 dataset\nIn this lesson, we will again use the data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0\nLet’s load the data into a pandas DataFrame.\n\nimport pandas as pd\n\nyaounde = pd.read_csv(\"data/yaounde_data.csv\")\n# a smaller subset of variables\nyao = yaounde[\n    [\n        \"age\",\n        \"sex\",\n        \"weight_kg\",\n        \"highest_education\",\n        \"neighborhood\",\n        \"occupation\",\n        \"symptoms\",\n        \"is_smoker\",\n        \"is_pregnant\",\n        \"igg_result\",\n        \"igm_result\",\n    ]\n]\nyao.head()\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nUniversity\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nSecondary\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n4\n55\nFemale\n67\nPrimary\nBriqueterie\nTrader--Farmer\nNo symptoms\nNon-smoker\nNo\nPositive\nNegative",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#introducing-query",
    "href": "p_untangled_query_rows.html#introducing-query",
    "title": "14  Subsetting rows",
    "section": "14.4 Introducing query()",
    "text": "14.4 Introducing query()\nWe can use the query() method to keep rows that satisfy a set of conditions. Let’s take a look at a simple example. If we want to keep just the male records, we run:\n\nyao.query('sex == \"Male\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nUniversity\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nSecondary\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nUniversity\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 11 columns\n\n\n\nAs you can see, the query() syntax is quite simple. It may be a bit surprising to have to put code in quotes, but it is quite readable.\nNote the use of double equals (==) instead of single equals (=) there. The == sign tests for equality, while the single equals sign assigns a value. This is a common source of errors when you are a beginner, so watch out for it.\nWe can chain query() with shape[0] to count the number of male respondents.\n\nyao.query('sex == \"Male\"').shape[0]\n\n422\n\n\n\n\n\n\n\n\nReminder\n\n\n\nRecall that the shape property returns the number of rows and columns in a DataFrame. The first element, shape[0], is the number of rows\n\n\n\n\n\n\n\n\nKey Point\n\n\n\nNote that these subsets are not modifying the DataFrame itself. If we want a modified version, we create a new DataFrame to store the subset. For example, below we create a subset of male respondents:\n\nyao_male = yao.query('sex == \"Male\"')\nyao_male\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nUniversity\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nSecondary\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nUniversity\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 11 columns\n\n\n\nBut for ease of explanation, in the examples below, we are simply printing the result, without storing it in a variable.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.4.1 Practice Q: Subset for Pregnant Respondents\nSubset the yao data frame to respondents who were pregnant during the survey. Assign the result to a new DataFrame called yao_pregnant. Then print this new DataFrame. There should be 24 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#relational-operators",
    "href": "p_untangled_query_rows.html#relational-operators",
    "title": "14  Subsetting rows",
    "section": "14.5 Relational operators",
    "text": "14.5 Relational operators\nThe == operator introduced above is an example of a “relational” operator, as it tests the relation between two values. Here is a list of some more of these operators. You will use these often when you are querying rows in your data.\n\n\n\nOperator\nis True if\n\n\nA &lt; B\nA is less than B\n\n\nA &lt;= B\nA is less than or equal to B\n\n\nA &gt; B\nA is greater than B\n\n\nA &gt;= B\nA is greater than or equal to B\n\n\nA == B\nA is equal to B\n\n\nA != B\nA is not equal to B\n\n\nA.isin(B)\nA is an element of B\n\n\n\nLet’s see how to use these with query():\n\nyao.query('sex == \"Female\"')  # keep rows where `sex` is female\nyao.query('sex != \"Male\"')  # keep rows where `sex` is not \"Male\"\nyao.query(\"age &lt; 6\")  # keep respondents under 6\nyao.query(\"age &gt;= 70\")  # keep respondents aged at least 70\n\n# keep respondents whose highest education is \"Primary\" or \"Secondary\"\nyao.query('highest_education.isin([\"Primary\", \"Secondary\"])')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nSecondary\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n751 rows × 11 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.5.1 Practice Q: Subset for Children\n\nFrom yao, keep only respondents who were children (under 18). Assign the result to a new DataFrame called yao_children. There should be 291 rows.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.5.2 Practice Q: Subset for Tsinga and Messa\n\nWith isin(), keep only respondents who live in the “Tsinga” or “Messa” neighborhoods. Assign the result to a new DataFrame called yao_tsinga_messa. There should be 129 rows\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "href": "p_untangled_query_rows.html#accessing-external-variables-in-query",
    "title": "14  Subsetting rows",
    "section": "14.6 Accessing external variables in query()",
    "text": "14.6 Accessing external variables in query()\nThe query() method allows you to access variables outside the DataFrame using the @ symbol. This is useful when you want to use dynamic values in your query conditions.\nFor example, say you have the variable min_age that you want to use in your query. You can do this as follows:\n\nmin_age = 25\n\n# Query using external variables\nyao.query('age &gt;= @min_age')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nUniversity\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n524 rows × 11 columns\n\n\n\nThis feature is helpful when you need to filter data based on values that may change or are determined at runtime.\n\n\n\n\n\n\nPractice\n\n\n\n14.6.1 Practice Q: Subset for Young Respondents\n\nFrom yao, keep respondents who are less than or equal to the variable max_age, defined below. Assign the result to a new DataFrame called yao_young. There should be 590 rows.\n\n\nmax_age = 30\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#combining-conditions-with-and",
    "href": "p_untangled_query_rows.html#combining-conditions-with-and",
    "title": "14  Subsetting rows",
    "section": "14.7 Combining conditions with & and |",
    "text": "14.7 Combining conditions with & and |\nWe can pass multiple conditions to query() using & (the “ampersand” symbol) for AND and | (the “vertical bar” or “pipe” symbol) for OR.\nFor example, to keep respondents who are either younger than 18 OR older than 65, we can write:\n\nyao.query(\"age &lt; 18 | age &gt; 65\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nSecondary\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nSecondary\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nSecondary\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n331 rows × 11 columns\n\n\n\nTo keep respondents who are pregnant AND are ex-smokers, we write:\n\nyao.query('is_pregnant == \"Yes\" & is_smoker == \"Ex-smoker\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n273\n25\nFemale\n90\nSecondary\nCarriere\nHome-maker\nCough--Rhinitis--Sneezing\nEx-smoker\nYes\nPositive\nNegative\n\n\n\n\n\n\n\nThere is only one person who is both pregnant and ex-smoker.\n\n\n\n\n\n\nPractice\n\n\n\n14.7.1 Practice Q: Subset for IgG Positive Men\nSubset yao to only keep men who tested IgG positive. Assign the result to a new DataFrame called yao_igg_positive_men. There should be 148 rows after your filter. Think carefully about whether to use & or |.\n\n# Your code here\n\nSubset yao to keep both children (under 18) and anyone whose highest education is primary school. Assign the result to a new DataFrame called yao_children_primary. There should be 444 rows after your filter. Think carefully about & vs |. It may be counterintuitive!\n\n# Your code here\n\n\n\nNote that we can also chain queries together instead of using & or |. For example, to keep respondents who are above 18, who smoke and who live in Tsinga, we could write:\n\nyao.query('age &gt; 18 & is_smoker == \"Ex-smoker\" & neighborhood == \"Tsinga\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n827\n24\nMale\n85\nSecondary\nTsinga\nStudent--Informal worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nPositive\n\n\n835\n22\nMale\n70\nUniversity\nTsinga\nStudent\nFever--Headache--Fatigue\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n899\n29\nFemale\n64\nDoctorate\nTsinga\nInformal worker\nFever--Headache--Anosmia or ageusia\nEx-smoker\nNo\nPositive\nNegative\n\n\n903\n28\nMale\n69\nSecondary\nTsinga\nTrader\nSneezing--Headache\nEx-smoker\nNaN\nNegative\nNegative\n\n\n\n\n12 rows × 11 columns\n\n\n\nBut we could also write:\n\n(yao\n.query('age &gt; 18')\n.query('is_smoker == \"Ex-smoker\"')\n.query('neighborhood == \"Tsinga\"'))\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n827\n24\nMale\n85\nSecondary\nTsinga\nStudent--Informal worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nPositive\n\n\n835\n22\nMale\n70\nUniversity\nTsinga\nStudent\nFever--Headache--Fatigue\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n899\n29\nFemale\n64\nDoctorate\nTsinga\nInformal worker\nFever--Headache--Anosmia or ageusia\nEx-smoker\nNo\nPositive\nNegative\n\n\n903\n28\nMale\n69\nSecondary\nTsinga\nTrader\nSneezing--Headache\nEx-smoker\nNaN\nNegative\nNegative\n\n\n\n\n12 rows × 11 columns\n\n\n\nWhen you have a very long sequence of conditions, this kind of chaining can be more readable than using & multiple times. The example above is not that long, so it may not be a good candidate for splitting the query up, but now you know how to do it in case you need to.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "href": "p_untangled_query_rows.html#negating-conditions-with-the-operator",
    "title": "14  Subsetting rows",
    "section": "14.8 Negating conditions with the ~ operator",
    "text": "14.8 Negating conditions with the ~ operator\nTo negate conditions in query(), we use the ~ operator (pronounced “tilde”).\nLet’s use this to drop respondents who are students:\n\nyao.query('~ (occupation == \"Student\")')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n588 rows × 11 columns\n\n\n\nNotice that we have to enclose the condition in parentheses.\nOf course, in this case, we could more simply use != to drop students, so this is not a great use case for not.\n\nyao.query('occupation != \"Student\"')\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n588 rows × 11 columns\n\n\n\nThe ~ operator is more useful when we have more complex conditions that we want to negate.\nImagine we want to give out a drug, but respondents below 18 (children) or who weigh less than 30kg (too light) are not eligible. The code below selects the children and these light respondents:\n\nyao.query(\"age &lt; 18 | weight_kg &lt; 30\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n5\n17\nFemale\n65\nSecondary\nBriqueterie\nStudent\nFever--Cough--Rhinitis--Nausea or vomiting--Di...\nNon-smoker\nNo\nNegative\nNegative\n\n\n6\n13\nFemale\n65\nSecondary\nBriqueterie\nStudent\nSneezing\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n962\n15\nMale\n44\nSecondary\nTsinga Oliga\nStudent\nFever--Cough--Rhinitis\nNon-smoker\nNaN\nPositive\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n291 rows × 11 columns\n\n\n\nNow to drop these individuals, we can negate the condition with ~:\n\nyao.query(\"~ (age &lt; 18 | weight_kg &lt; 30)\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nMale\n77\nUniversity\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n680 rows × 11 columns\n\n\n\nNote how we enclosed the conditions in parentheses. This allows the not operator to ‘act’ on the whole condition, rather than just the first part of it.\n\n\n\n\n\n\nPractice\n\n\n\n14.8.1 Practice Q: Drop Smokers and drop those over 50\nFrom yao, drop respondents that are either above 50 or who are smokers. Assign the result to a new DataFrame called yao_dropped. Your output should have 810 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#nan-values",
    "href": "p_untangled_query_rows.html#nan-values",
    "title": "14  Subsetting rows",
    "section": "14.9 NaN values",
    "text": "14.9 NaN values\nThe relational operators introduced so far do not work with null values like NaN.\nFor example, the is_pregnant column contains missing values for some respondents. To keep the rows with missing is_pregnant values, we could try writing:\n\nyao.query(\"is_pregnant == NaN\")  # does not work\n\nBut this will not work. This is because NaN is a non-existent value. So the system cannot evaluate whether it is “equal to” or “not equal to” anything.\nInstead, we can use the isna() method to select rows with missing values:\n\nyao.query(\"is_pregnant.isna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n1\n55\nMale\n96\nUniversity\nBriqueterie\nSalaried worker\nNo symptoms\nEx-smoker\nNaN\nPositive\nNegative\n\n\n2\n23\nMale\n74\nUniversity\nBriqueterie\nStudent\nNo symptoms\nSmoker\nNaN\nNegative\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n966\n32\nMale\n54\nSecondary\nTsinga Oliga\nInformal worker\nRhinitis--Sneezing--Diarrhoea\nSmoker\nNaN\nNegative\nNegative\n\n\n968\n35\nMale\n77\nUniversity\nTsinga Oliga\nInformal worker\nHeadache\nSmoker\nNaN\nPositive\nNegative\n\n\n\n\n422 rows × 11 columns\n\n\n\nOr we can select rows that are not missing with notna():\n\nyao.query(\"is_pregnant.notna()\")\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nSecondary\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n549 rows × 11 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n14.9.1 Practice Q: Keep Missing Smoking Status\nFrom the yao dataset, keep all the respondents who had missing records for the report of their smoking status.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "href": "p_untangled_query_rows.html#querying-based-on-string-patterns",
    "title": "14  Subsetting rows",
    "section": "14.10 Querying Based on String Patterns",
    "text": "14.10 Querying Based on String Patterns\nSometimes, we need to filter our data based on whether a string column contains a certain substring. This is particularly useful when dealing with multi-answer type variables, where responses may contain multiple values separated by delimiters. Let’s explore this using the occupation column in our dataset.\nFirst, let’s take a look at the unique values in the occupation column:\n\noccup_df = yao[[\"occupation\", \"sex\"]]\noccup_df.value_counts().to_dict()\n\n{('Student', 'Female'): 203,\n ('Student', 'Male'): 180,\n ('Informal worker', 'Male'): 112,\n ('Informal worker', 'Female'): 77,\n ('Trader', 'Female'): 69,\n ('Home-maker', 'Female'): 65,\n ('Unemployed', 'Female'): 50,\n ('Trader', 'Male'): 42,\n ('Salaried worker', 'Female'): 34,\n ('Salaried worker', 'Male'): 20,\n ('Unemployed', 'Male'): 18,\n ('Retired', 'Male'): 14,\n ('Retired', 'Female'): 13,\n ('Student--Informal worker', 'Male'): 11,\n ('No response', 'Female'): 8,\n ('Other', 'Male'): 7,\n ('Other', 'Female'): 6,\n ('Trader--Farmer', 'Female'): 4,\n ('Student--Trader', 'Male'): 4,\n ('Informal worker--Trader', 'Male'): 3,\n ('Home-maker--Trader', 'Female'): 3,\n ('Home-maker--Informal worker', 'Female'): 3,\n ('Farmer', 'Female'): 3,\n ('Informal worker--Other', 'Male'): 2,\n ('Farmer', 'Male'): 2,\n ('Home-maker--Farmer', 'Female'): 2,\n ('Student--Informal worker', 'Female'): 2,\n ('Retired--Informal worker', 'Female'): 2,\n ('Farmer--Other', 'Male'): 1,\n ('Trader--Unemployed', 'Female'): 1,\n ('Informal worker--Unemployed', 'Male'): 1,\n ('No response', 'Male'): 1,\n ('Retired--Other', 'Male'): 1,\n ('Student--Other', 'Female'): 1,\n ('Retired--Trader', 'Female'): 1,\n ('Home-maker--Informal worker--Farmer', 'Female'): 1,\n ('Retired--Informal worker', 'Male'): 1,\n ('Informal worker--Trader--Farmer--Other', 'Male'): 1,\n ('Informal worker--Trader', 'Female'): 1,\n ('Student--Informal worker--Other', 'Male'): 1}\n\n\nAs we can see, some respondents have multiple occupations, separated by “–”. To query based on string containment, we can use the str.contains() method within our query().\n\n14.10.1 Basic String Containment\nTo find all respondents who are students (either solely or in combination with other occupations), we can use:\n\noccup_df.query(\"occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\noccupation\nsex\n\n\n\n\n2\nStudent\nMale\n\n\n3\nStudent\nFemale\n\n\n...\n...\n...\n\n\n963\nStudent\nFemale\n\n\n964\nStudent--Informal worker\nMale\n\n\n\n\n402 rows × 2 columns\n\n\n\nThis query will return all rows where the occupation column contains the word “Student”, regardless of whether it’s the only occupation or part of a multiple-occupation entry.\n\n\n14.10.2 Combining with Other Conditions\nYou can combine string containment queries with other conditions. For example, to find female students:\n\noccup_df.query(\"occupation.str.contains('Student') & sex == 'Female'\")\n\n\n\n\n\n\n\n\noccupation\nsex\n\n\n\n\n3\nStudent\nFemale\n\n\n5\nStudent\nFemale\n\n\n...\n...\n...\n\n\n949\nStudent\nFemale\n\n\n963\nStudent\nFemale\n\n\n\n\n206 rows × 2 columns\n\n\n\n\n\n14.10.3 Negating String Containment\nTo find respondents who are not students (i.e., their occupation does not contain “Student”), you can use the ~ operator:\n\noccup_df.query(\"~occupation.str.contains('Student')\")\n\n\n\n\n\n\n\n\noccupation\nsex\n\n\n\n\n0\nInformal worker\nFemale\n\n\n1\nSalaried worker\nMale\n\n\n...\n...\n...\n\n\n969\nUnemployed\nFemale\n\n\n970\nUnemployed\nFemale\n\n\n\n\n569 rows × 2 columns\n\n\n\n\n\n14.10.4 Search for non-alphabetic characters\nIf you want to find respondents who have multiple occupations, you can search for the delimiter:\n\noccup_df.query(\"occupation.str.contains('--')\")\n\n\n\n\n\n\n\n\noccupation\nsex\n\n\n\n\n4\nTrader--Farmer\nFemale\n\n\n45\nRetired--Informal worker\nMale\n\n\n...\n...\n...\n\n\n964\nStudent--Informal worker\nMale\n\n\n967\nInformal worker--Trader\nFemale\n\n\n\n\n47 rows × 2 columns\n\n\n\nThis will return all rows where the occupation contains “–”, indicating multiple occupations.\n\n\n\n\n\n\nPractice\n\n\n\n14.10.5 Practice Q: Symptoms\nThe symptoms column contains a list of symptoms that respondents reported.\nQuery yao to find respondents who reported “Cough” or “Fever” as symptoms. Your answer should have 219 rows.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#subsetting-with-square-brackets",
    "href": "p_untangled_query_rows.html#subsetting-with-square-brackets",
    "title": "14  Subsetting rows",
    "section": "14.11 Subsetting with square brackets, []",
    "text": "14.11 Subsetting with square brackets, []\nAs a final note, you can also use logical subsetting with [] (square brackets) to filter rows.\nWe don’t recommend this for filtering DataFrames, as it’s difficult to use properly in method chains, but we will use it at other points in our Python journey, so it’s good to know about.\nLet’s see an example using a pandas Series.\n\ns = pd.Series([1, 2, 3, 4, 5])\ns\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\nWe can use logical subsetting to filter this series:\n\ns[s &gt; 3]  # subset s to just the values greater than 3\n\n3    4\n4    5\ndtype: int64\n\n\nWe obtain a new Series with the values that are greater than 3. Series cannot be filtered with query(), so they are a good use case for [].\nWe can do the same with DataFrames. Let’s filter our yao DataFrame to keep only females:\n\nyao[yao.sex == 'Female']\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n3\n20\nFemale\n70\nSecondary\nBriqueterie\nStudent\nRhinitis--Sneezing--Anosmia or ageusia\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n970\n17\nFemale\n67\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo response\nNegative\nNegative\n\n\n\n\n549 rows × 11 columns\n\n\n\nThis returns a DataFrame with only female respondents.\nWe can combine conditions using & for AND and | for OR. For example, to keep females who are over 30:\n\nyao[(yao.sex == 'Female') & (yao.age &gt; 30)]\n\n\n\n\n\n\n\n\nage\nsex\nweight_kg\nhighest_education\nneighborhood\noccupation\nsymptoms\nis_smoker\nis_pregnant\nigg_result\nigm_result\n\n\n\n\n0\n45\nFemale\n95\nSecondary\nBriqueterie\nInformal worker\nMuscle pain\nNon-smoker\nNo\nNegative\nNegative\n\n\n4\n55\nFemale\n67\nPrimary\nBriqueterie\nTrader--Farmer\nNo symptoms\nNon-smoker\nNo\nPositive\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n960\n48\nFemale\n46\nPrimary\nTsinga Oliga\nTrader--Farmer\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n969\n31\nFemale\n66\nSecondary\nTsinga Oliga\nUnemployed\nNo symptoms\nNon-smoker\nNo\nNegative\nNegative\n\n\n\n\n224 rows × 11 columns\n\n\n\nNote the use of parentheses around each condition.\n\n\n\n\n\n\nPractice\n\n\n\n14.11.1 Practice Q: Heights subset\nCreate a Series called heights with the following values: [160, 175, 182, 168, 190, 173]. Then, use logical subsetting to create a new Series called tall_people that only includes heights greater than 180 cm.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_query_rows.html#wrap-up",
    "href": "p_untangled_query_rows.html#wrap-up",
    "title": "14  Subsetting rows",
    "section": "14.12 Wrap up",
    "text": "14.12 Wrap up\nGreat job! You’ve learned how to select specific columns and filter rows based on various conditions.\nThese skills allow you to focus on relevant data and create targeted subsets for analysis.\nNext, we’ll explore how to modify and transform your data, further expanding your data wrangling toolkit. See you in the next lesson!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Subsetting rows</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html",
    "href": "p_untangled_assign.html",
    "title": "15  Creating and modifying variables with assign()",
    "section": "",
    "text": "15.1 Intro\nToday you will learn how to modify existing variables or create new ones, using the assign() method from pandas. This is an essential step in most data analysis projects.\nLet’s get started!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#learning-objective",
    "href": "p_untangled_assign.html#learning-objective",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.2 Learning objective",
    "text": "15.2 Learning objective\n\nYou can use the assign() method from pandas to create new variables or modify existing variables.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#imports",
    "href": "p_untangled_assign.html#imports",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.3 Imports",
    "text": "15.3 Imports\nThis lesson will require the pandas package. You can import it with the following code:\n\nimport pandas as pd",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#datasets",
    "href": "p_untangled_assign.html#datasets",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.4 Datasets",
    "text": "15.4 Datasets\nIn this lesson, we will use a dataset of counties of the United States with demographic and economic data.\n\ncounties = pd.read_csv(\"data/us_counties_data.csv\")\ncounties\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\n58877.0\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nPR\nYabucoa, PR\n30364.0\n55.214614\nNaN\nNaN\nNaN\nNaN\n-19.807069\n0.1\n\n\n3225\nPR\nYauco, PR\n34062.0\n67.711484\nNaN\nNaN\nNaN\nNaN\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 10 columns\n\n\n\nThe variables in the dataset are:\n\nstate, US state\ncounty, US county\npop_20, population estimate for 2020\narea_sq_miles, area in square miles\nhh_inc_21, median household income for 2021\necon_type, economic type of the county\npop_change_2010_2020, population change between 2010 and 2020\nunemp_20, unemployment rate for 2020\npct_emp_change_2010_2021, percentage change in employment between 2010 and 2021\n\nThe variables are collected from a range of sources, including the US Census Bureau, the Bureau of Labor Statistics, and the American Community Survey.\nLet’s create a small subset of the variables, with just area and population, for illustration.\n\n## small subset for illustration\narea_pop = counties[[\"county\", \"area_sq_miles\", \"pop_20\"]]\narea_pop\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n\n\n\n\n3226 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#introducing-assign-and-lambda-functions",
    "href": "p_untangled_assign.html#introducing-assign-and-lambda-functions",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.5 Introducing assign() and lambda functions",
    "text": "15.5 Introducing assign() and lambda functions\nWe use the assign() method from pandas to create new variables or modify existing variables.\nLet’s see a quick example.\nThe area_pop dataset shows the area of each county in square miles. We want to create a new variable with this converted to square kilometers so we must multiply the area_sq_miles variable by 2.59. With assign(), we can write:\n\narea_pop.assign(area_sq_km=lambda x: x.area_sq_miles * 2.59)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n\n\n\n\n3226 rows × 4 columns\n\n\n\nNice! The syntax may appear confusing, so let’s break it down.\nWe can read lambda x: x.area_sq_miles * 2.59 as, “define a function that takes a DataFrame x, then multiplies the area_sq_miles variable in that DataFrame by 2.59.”\nA lambda is a term to refer to a small function defined without a name. This part of the code is essential.\nBut the symbol called x above can be anything you want; it’s simply a placeholder for the data frame. For example, we could call it dat:\n\narea_pop.assign(area_sq_km=lambda dat: dat.area_sq_miles * 2.59)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n\n\n\n\n3226 rows × 4 columns\n\n\n\nLet’s see another example. We’ll add a variable showing the area in hectares by multiplying area_sq_miles by 259. We’ll also store the new DataFrame in a variable called area_pop_converted. (Remember that if we don’t store the new DataFrame in a variable, as in our example above, it will not be saved.)\n\narea_pop_converted = area_pop.assign(\n    area_sq_km=lambda x: x.area_sq_miles * 2.59,\n    area_hectares=lambda x: x.area_sq_miles * 259,\n)\narea_pop_converted  # view the new DataFrame\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.5.1 Practice Q: Area in acres\n\nUsing the area_pop dataset, create a new column called area_acres by multiplying the area_sq_miles variable by 640. Store the new DataFrame in an object called conversion_question and print it.\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#overwriting-variables",
    "href": "p_untangled_assign.html#overwriting-variables",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.6 Overwriting variables",
    "text": "15.6 Overwriting variables\nWe can also use assign() to overwrite existing variables. For example, to round the area_sq_km variable to one decimal place, we can write:\n\narea_pop_converted.assign(area_sq_km=lambda x: round(x.area_sq_miles * 2.59, 1))\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.6\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.7\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.0\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.4\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n15.6.1 Practice Q: Area in acres rounded\n\nUsing the conversion_question dataset you created above, round the area_acres variable to one decimal place. Store the new DataFrame in an object called conversion_question_rounded and print it.\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#more-complex-assignments",
    "href": "p_untangled_assign.html#more-complex-assignments",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.7 More complex assignments",
    "text": "15.7 More complex assignments\nTo get further practice with the assign() method, let’s look at creating new columns that combine multiple existing variables.\nFor example, to calculate the population density per square kilometer, we can write:\n\narea_pop_converted.assign(pop_per_sq_km=lambda x: x.pop_20 / x.area_sq_km)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n38.240725\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n56.619326\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n212.326977\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n194.226306\n\n\n\n\n3226 rows × 6 columns\n\n\n\nWe can also round the pop_per_sq_km variable to one decimal place within the assign() method, either on the same line:\n\narea_pop_converted.assign(\n    pop_per_sq_km=lambda x: round(x.pop_20 / x.area_sq_km, 1)\n)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n194.2\n\n\n\n\n3226 rows × 6 columns\n\n\n\nOr on a new line like so:\n\narea_pop_converted.assign(\n    pop_per_sq_km=lambda x: x.pop_20 / x.area_sq_km,\n    pop_per_sq_km_rounded=lambda x: round(x.pop_per_sq_km, 1),\n)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\npop_per_sq_km_rounded\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n38.240725\n38.2\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n56.619326\n56.6\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n212.326977\n212.3\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n194.226306\n194.2\n\n\n\n\n3226 rows × 7 columns\n\n\n\nAs a final step, let’s practice method chaining by arranging the dataset by the pop_per_sq_km variable in descending order, and store this in a new variable called area_pop_converted_sorted.\n\narea_pop_converted_sorted = area_pop_converted.assign(\n    pop_per_sq_km=lambda x: x.pop_20 / x.area_sq_km,\n    pop_per_sq_km_rounded=lambda x: round(x.pop_per_sq_km, 1),\n).sort_values(\"pop_per_sq_km\", ascending=False)\n\narea_pop_converted_sorted\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\npop_per_sq_km_rounded\n\n\n\n\n1863\nNew York, NY\n22.656266\n1687834.0\n58.679729\n5867.972888\n28763.493499\n28763.5\n\n\n1856\nKings, NY\n69.376570\n2727393.0\n179.685318\n17968.531752\n15178.719317\n15178.7\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n98\nWrangell-Petersburg, AK\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2921\nBedford, VA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n3226 rows × 7 columns\n\n\n\nWe see that New York county has the highest population density in the country.\n\n\n\n\n\n\nPractice\n\n\n\n15.7.1 Practice Q: Rounding all variables\n\nConsider the sample dataset created below. Use assign() to round all variables to 1 decimal place. Your final dataframe should be called sample_df_rounded and should still have three columns, a, b, and c.\n\n\nsample_df = pd.DataFrame(\n    {\n        \"a\": [1.111, 2.222, 3.333],\n        \"b\": [4.444, 5.555, 6.666],\n        \"c\": [7.777, 8.888, 9.999],\n    }\n)\nsample_df\n\n\n\n\n\n\n\n\na\nb\nc\n\n\n\n\n0\n1.111\n4.444\n7.777\n\n\n1\n2.222\n5.555\n8.888\n\n\n2\n3.333\n6.666\n9.999\n\n\n\n\n\n\n\n\n# your code here\n\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\n15.7.2 Why use lambda functions in assign()?\nYou may be wondering whether the lambda function is really necessary within assign. After all, we could run code like this:\n\narea_pop.assign(\n    area_sq_km=area_pop.area_sq_miles * 2.59,\n    area_hectares=area_pop.area_sq_miles * 259,\n)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n\n\n\n\n3226 rows × 5 columns\n\n\n\nHere, we are referring back to the area_pop DataFrame within the assign() method.\nThe problem with this is we cannot access variables created in the same assign() call or created in the same method chain.\nFor example, below, if you try to use the area_sq_km variable to calculate the population density, you will get an error:\n\narea_pop.assign(\n    area_sq_km=area_pop.area_sq_miles * 2.59,\n    area_hectares=area_pop.area_sq_miles * 259,\n    pop_per_sq_km=area_pop.pop_20 / area_pop.area_sq_km,\n)\n\nAttributeError: 'DataFrame' object has no attribute 'area_sq_km'\nPython cannot find the area_sq_km variable because it is created in the same assign() call. So the area_pop DataFrame is does not yet have that variable!\nLambda functions in assign() allow you to create new columns based on intermediate results within the same call. So the code below works:\n\narea_pop.assign(\n    area_sq_km=lambda x: x.area_sq_miles * 2.59,\n    area_hectares=lambda x: x.area_sq_miles * 259,\n    # area_sq_km is created in the previous line, but is already available here\n    pop_per_sq_km=lambda x: x.pop_20 / x.area_sq_km,\n)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop_20\narea_sq_km\narea_hectares\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n1539.641317\n153964.131747\n38.240725\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n4117.675277\n411767.527703\n56.619326\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n143.005851\n14300.585058\n212.326977\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n175.372743\n17537.274254\n194.226306\n\n\n\n\n3226 rows × 6 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#creating-boolean-variables",
    "href": "p_untangled_assign.html#creating-boolean-variables",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.8 Creating Boolean variables",
    "text": "15.8 Creating Boolean variables\nYou can use assign() to create a Boolean variable to categorize part of your dataset.\nConsider the pop_change_2010_2020 variable in the counties dataset, which shows the percentage change in population between 2010 and 2020. This is shown below in our changes_df subset.\n\nchanges_df = counties[\n    [\"county\", \"pop_change_2010_2020\", \"pct_emp_change_2010_2021\"]\n]  # Make dataset subset\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\n\n\n1\nBaldwin, AL\n27.159356\n28.2\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 3 columns\n\n\n\nBelow we create a Boolean variable, pop_increased, that is True if the growth rate is greater than 0 and False if it is not.\n\nchanges_df.assign(pop_increased=lambda x: x.pop_change_2010_2020 &gt; 0)\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increased\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\n\n\n\n\n3226 rows × 4 columns\n\n\n\nThe code x['pop_change_2010_2020'] &gt; 0 evaluates whether each growth rate is greater than 0. Growth rates that match that condition (growth rates greater than 0) are True and those that fail the condition are False.\nLet’s do the same for the employment change variable and store the results in our dataset.\n\nchanges_df = changes_df.assign(\n    pop_increased=lambda x: x.pop_change_2010_2020 &gt; 0,\n    emp_increased=lambda x: x.pct_emp_change_2010_2021 &gt; 0,\n)\nchanges_df\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increased\nemp_increased\n\n\n\n\n0\nAutauga, AL\n7.758700\n9.0\nTrue\nTrue\n\n\n1\nBaldwin, AL\n27.159356\n28.2\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n-19.807069\n0.1\nFalse\nTrue\n\n\n3225\nYauco, PR\n-18.721309\n-5.3\nFalse\nFalse\n\n\n\n\n3226 rows × 5 columns\n\n\n\nWe can now query the dataset to, for example, see which counties had a population increase but an employment decrease. From a policy perspective, this would be a concern, since employment is not able to keep up with population growth.\n\nchanges_df.query(\"pop_increased == True & emp_increased == False\")\n# Or more succintly:\nchanges_df.query(\"pop_increased & not emp_increased\")\n\n\n\n\n\n\n\n\ncounty\npop_change_2010_2020\npct_emp_change_2010_2021\npop_increased\nemp_increased\n\n\n\n\n71\nBethel, AK\n9.716099\n-0.7\nTrue\nFalse\n\n\n75\nDillingham, AK\n0.206313\n-16.1\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n3127\nCampbell, WY\n1.935708\n-14.8\nTrue\nFalse\n\n\n3137\nNatrona, WY\n5.970842\n-0.2\nTrue\nFalse\n\n\n\n\n242 rows × 5 columns\n\n\n\nThere are 242 such concerning counties.\n\n\n\n\n\n\nPractice\n\n\n\n15.8.1 Practice Q: Foreign-born residents\n\nUse the foreign_born_num variable and the population estimate to calculate the percentage of foreign-born residents in each county. Then create a Boolean variable called foreign_born_pct_gt_30 that is True if the percentage of foreign-born residents is greater than 30% and False if it is not. Store the new DataFrame in a variable called foreign_born_df_question.\n\n\n# your code here\n\n\nUse .query() on the foreign_born_df_question DataFrame to return only the counties where the foreign_born_pct_gt_30 variable is True. You should get 24 rows.\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\n15.8.2 Accessing variables with [] inside a lambda function\nWe can also use the square brackets [] to access the variables. This is useful in two cases:\n\nWhen the variable name has special characters.\nWhen the variable name is a reserved word or method name in Python.\n\nIf the population variable were called pop 20, with a space, we would not be able to access it with dot notation.\n\ndemo_df = area_pop.rename(columns={\"pop_20\": \"pop 20\"})\ndemo_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop 20\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n\n\n\n\n3226 rows × 3 columns\n\n\n\n\ndemo_df.assign(pop_per_sq_km=lambda x: x.pop 20 / x.area_sq_miles) # gives error\n\narea_pop.assign(pop_per_sq_km=lambda x: x.pop 20 / x.area_sq_miles)\n                                            ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\nSo we would need to use square brackets to access the variable.\n\ndemo_df.assign(pop_per_sq_km=lambda x: x[\"pop 20\"] / x.area_sq_miles)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop 20\npop_per_sq_km\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n99.043477\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n146.644055\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n549.926871\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n503.046133\n\n\n\n\n3226 rows × 4 columns\n\n\n\nIn reality, we should probably just rename such a variable though!\n\nThe square brackets are also needed if our variable name is a reserved word or method name in Python.\nFor example, if our population variable was called pop, we would get an error when accessing it with dot notation in the assign() method, because pop is a method name.\n\ndemo_df = area_pop.rename(columns={\"pop_20\": \"pop\"})\ndemo_df\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n\n\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n\n\n\n\n3226 rows × 3 columns\n\n\n\n\ndemo_df.assign(pop_per_sq_mile=lambda x: x.pop / x.area_sq_miles)\n\nTypeError: unsupported operand type(s) for /: 'method' and 'float'\nIn such a case, we can use the square brackets [] to access the pop variable.\n\ndemo_df.assign(pop_per_sq_mile=lambda x: x[\"pop\"] / x.area_sq_miles)\n\n\n\n\n\n\n\n\ncounty\narea_sq_miles\npop\npop_per_sq_mile\n\n\n\n\n0\nAutauga, AL\n594.456107\n58877.0\n99.043477\n\n\n1\nBaldwin, AL\n1589.836014\n233140.0\n146.644055\n\n\n...\n...\n...\n...\n...\n\n\n3224\nYabucoa, PR\n55.214614\n30364.0\n549.926871\n\n\n3225\nYauco, PR\n67.711484\n34062.0\n503.046133\n\n\n\n\n3226 rows × 4 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_assign.html#wrap-up",
    "href": "p_untangled_assign.html#wrap-up",
    "title": "15  Creating and modifying variables with assign()",
    "section": "15.9 Wrap up",
    "text": "15.9 Wrap up\nAs you can imagine, transforming data is an essential step in any data analysis workflow. It is often required to clean data and to prepare it for further statistical analysis or for making plots. And as you have seen, it is quite simple to transform data with pandas’ assign() method.\nCongrats on making it through.\nBut your data wrangling journey isn’t over yet! In our next lessons, we will learn how to create complex data summaries and how to create and work with data frame groups. Intrigued? See you in the next lesson.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Creating and modifying variables with `assign()`</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html",
    "href": "p_untangled_case_when.html",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "",
    "text": "16.1 Introduction\nIn the previous lesson, you learned the basics of data transformation using pandas’ assign() method.\nIn that lesson, we looked at global transformations; that is, transformations that did the same thing to an entire variable. In this lesson, we will look at how to conditionally manipulate certain rows based on whether or not they meet defined criteria.\nFor this, we will mostly use the powerful pandascase_when() method. Note that this was introduced recently, in pandas 2.2, so if you are using an older version of pandas, you will need to upgrade.\nLet’s get started.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#learning-objectives",
    "href": "p_untangled_case_when.html#learning-objectives",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.2 Learning objectives",
    "text": "16.2 Learning objectives\n\nYou can transform or create new variables based on conditions using case_when()\nYou know how to use a default condition in case_when() to match unmatched cases.\nYou can handle NaN values in case_when() transformations.\nYou understand case_when() conditions priority order.\nYou can use np.where() for binary conditional assignment.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#packages",
    "href": "p_untangled_case_when.html#packages",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.3 Packages",
    "text": "16.3 Packages\nThis lesson will require pandas and numpy:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#datasets",
    "href": "p_untangled_case_when.html#datasets",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.4 Datasets",
    "text": "16.4 Datasets\nIn this lesson, we will use a dataset of counties of the United States with demographic and economic data.\nBelow we import it, then add some NAs to the pop_20 column, to simulate missing data.\n\n# Import and view the dataset\ncounties = pd.read_csv(\"data/us_counties_data.csv\")\n\n# Add NAs to the population column\ncounties.loc[counties.index % 5 == 0, \"pop_20\"] = np.nan\n\ncounties\n\n\n\n\n\n\n\n\nstate\ncounty\npop_20\narea_sq_miles\nhh_inc_21\necon_type\nunemp_20\nforeign_born_num\npop_change_2010_2020\npct_emp_change_2010_2021\n\n\n\n\n0\nAL\nAutauga, AL\nNaN\n594.456107\n66444.0\nNonspecialized\n5.4\n1241.0\n7.758700\n9.0\n\n\n1\nAL\nBaldwin, AL\n233140.0\n1589.836014\n65658.0\nRecreation\n6.2\n7938.0\n27.159356\n28.2\n\n\n2\nAL\nBarbour, AL\n25180.0\n885.008019\n38649.0\nManufacturing\n7.8\n659.0\n-8.136359\n-13.9\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3223\nPR\nVillalba, PR\n22044.0\n35.636678\nNaN\nNaN\nNaN\nNaN\n-15.264833\n-1.2\n\n\n3224\nPR\nYabucoa, PR\n30364.0\n55.214614\nNaN\nNaN\nNaN\nNaN\n-19.807069\n0.1\n\n\n3225\nPR\nYauco, PR\nNaN\n67.711484\nNaN\nNaN\nNaN\nNaN\n-18.721309\n-5.3\n\n\n\n\n3226 rows × 10 columns\n\n\n\nThe variables in the dataset are:\n\nstate, US state\ncounty, US county\npop_20, population estimate for 2020\narea_sq_miles, area in square miles\nhh_inc_21, median household income for 2021\necon_type, economic type of the county\npop_change_2010_2020, population change between 2010 and 2020\nunemp_20, unemployment rate for 2020\npct_emp_change_2010_2021, percentage change in employment between 2010 and 2021\n\nThe variables are collected from a range of sources, including the US Census Bureau, the Bureau of Labor Statistics, and the American Community Survey.\nLet’s also make some subsets of the data to work with.\n\ncounties_pop = counties[[\"pop_20\"]]\ncounties_income = counties[[\"hh_inc_21\"]]",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#introduction-to-case_when",
    "href": "p_untangled_case_when.html#introduction-to-case_when",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.5 Introduction to case_when()",
    "text": "16.5 Introduction to case_when()\nTo get familiar with case_when(), let’s begin with a simple conditional transformation on the pop_20 column of the counties dataset.\nWe will make a new column, called “pop_class”, that has the value “Smaller” if the population is below 50,000, and “Larger” if the population is 50,000 or more.\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: x.pop_20.case_when(\n        [\n            (x.pop_20 &lt; 50000, \"Smaller\"),\n            (x.pop_20 &gt;= 50000, \"Larger\"),\n        ]\n    )\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n233140.0\nLarger\n\n\n2\n25180.0\nSmaller\n\n\n...\n...\n...\n\n\n3223\n22044.0\nSmaller\n\n\n3224\n30364.0\nSmaller\n\n\n3225\nNaN\nNaN\n\n\n\n\n3226 rows × 2 columns\n\n\n\nThe statement case_when([(x.pop_20 &lt; 50000, \"Smaller\"), (x.pop_20 &gt;= 50000, \"Larger\")]) can be read as: “if pop_20 is below 50,000, input ‘Smaller’, else if pop_20 is greater than or equal to 50,000, input ‘Larger’”.\nThe case_when() syntax may seem a bit foreign. So let’s break it down:\n\nFirst, we use the assign() method to create a new column, pop_class.\nThen we use a lambda function to define the pop_class column. Hopefullyou you recall how to write these.\nThen, we use the case_when() method to create the new column.\nInside the case_when() method, we use a list of tuples.\nRecall that lists are created with square brackets [].\nTuples are created with parentheses (itemA, itemB), with each element in the tuple separated by a comma.\nSo the list of tuples is [(conditionA, valueA), (conditionB, valueB)].\n\n\nAfter creating a new variable with case_when(), you shoul inspect it to make sure the transformation worked as intended.\nLet’s use the value_counts() method to ensure that the numbers and proportions “make sense”:\n\ncounties_pop[\"pop_class\"].value_counts() # numbers\n\npop_class\nSmaller    1773\nLarger      801\nName: count, dtype: int64\n\n\n\ncounties_pop[\"pop_class\"].value_counts(normalize=True) # proportions\n\npop_class\nSmaller    0.688811\nLarger     0.311189\nName: proportion, dtype: float64\n\n\nDo these seem reasonable based on the population size of US counties? (Yes!)\nLet’s make it a bit more complex with a third condition for medium-sized counties. Counties with populations from 30,000 to below 100,000 will be labeled “Medium”, and counties with populations of 100,000 and up will be labeled “Large”.\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: x.pop_20.case_when(\n        [\n            (x.pop_20 &lt; 30000, \"Small\"),\n            ((x.pop_20 &gt;= 30000) & (x.pop_20 &lt; 100000), \"Medium\"),\n            (x.pop_20 &gt;= 100000, \"Large\"),\n        ]\n    )\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n233140.0\nLarge\n\n\n2\n25180.0\nSmall\n\n\n...\n...\n...\n\n\n3223\n22044.0\nSmall\n\n\n3224\n30364.0\nMedium\n\n\n3225\nNaN\nNaN\n\n\n\n\n3226 rows × 2 columns\n\n\n\nNote that when we combine multiple conditions with &, we must use parentheses around each condition.\n\n\n\n\n\n\nPractice\n\n\n\n16.5.1 Practice Q: Recode income class\nThe counties_income dataset has a column called hh_inc_21 with the median household income in 2021. Make a new column, called income_group, with the following three groups:\n\n“Below 30k” for counties with income under $30,000\n“30k to 60k” for counties with income between $30,000 and $60,000 (exclusive of 60k)\n“60k and above” for counties with income of $60,000 and up\n\n\n# Complete the code with your answer:\nQ_income_group = counties_income.assign(\n    income_group=lambda x: x.hh_inc_21.case_when(\n        [\n            (x.hh_inc_21 &lt; 30000, \"Below 30k\"),\n            _________________________________\n            _________________________________\n        ]\n    )\n)\nQ_income_group[\"income_group\"].value_counts(normalize=True)\n\n\nUse the value_counts() method to count the proportions of each income group. If you did it correctly, the proportion of counties with median household income below $60,000 should be approximately 60%.\n\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\n16.6 Denoting intervals\nWhen categorizing variables, it’s important to clearly communicate whether each category includes or excludes its boundaries. Here are two common approaches:\n\nMathematical notation:\n\nUse square brackets [] for inclusive bounds and parentheses () for exclusive bounds.\nExample:\n\n[0, 60,000) : This means under $60,000\n[60,000, 100,000) : This means $60,000 to $99,999\n[100,000, ∞) : This means $100,000 and up. The ∞ symbol stands for infinity.\n\n\nCode-friendly notation:\n\nUse &gt;= and &lt;= for inclusive bounds.\nUse &gt; and &lt; for exclusive bounds.\nExample:\n\nx &lt; $60,000 : This means under $60,000\n$60,000 &lt; x &lt; $100,000 : This means $60,000 to $99,999.99…\nx &gt;= $100,000 : This means $100,000 and up\n\n\nRounded integer ranges (more reader-friendly):\n\nRound values to integers and use descriptive language.\nExample:\n\n$0 to $59,999\n$60,000 to $99,999\n$100,000 and up\n\n\n\nChoose the method that best suits your audience and context.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#denoting-intervals",
    "href": "p_untangled_case_when.html#denoting-intervals",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.6 Denoting intervals",
    "text": "16.6 Denoting intervals\nWhen categorizing variables, it’s important to clearly communicate whether each category includes or excludes its boundaries. Here are two common approaches:\n\nMathematical notation:\n\nUse square brackets [] for inclusive bounds and parentheses () for exclusive bounds.\nExample:\n\n[0, 60,000) : This means under $60,000\n[60,000, 100,000) : This means $60,000 to $99,999\n[100,000, ∞) : This means $100,000 and up. The ∞ symbol stands for infinity.\n\n\nCode-friendly notation:\n\nUse &gt;= and &lt;= for inclusive bounds.\nUse &gt; and &lt; for exclusive bounds.\nExample:\n\nx &lt; $60,000 : This means under $60,000\n$60,000 &lt; x &lt; $100,000 : This means $60,000 to $99,999.99…\nx &gt;= $100,000 : This means $100,000 and up\n\n\nRounded integer ranges (more reader-friendly):\n\nRound values to integers and use descriptive language.\nExample:\n\n$0 to $59,999\n$60,000 to $99,999\n$100,000 and up\n\n\n\nChoose the method that best suits your audience and context.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#a-catch-all-condition",
    "href": "p_untangled_case_when.html#a-catch-all-condition",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.7 A catch-all condition",
    "text": "16.7 A catch-all condition\nIn a case_when() statement, you sometimes want to catch all rows not matched with provided conditions. You can do this with a catch-all condition. This should be something that always returns True. Unfortunately, simply using True as the condition will not work, since we need a sequence of True values.\nYou could use pd.Series(True, index=df.index) to create a sequence of True values, but perhaps an easier solution is to test whether the values are equal to themselves, using, for example x.pop_20 == x.pop_20:\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: x.pop_20.case_when(\n        [\n            ((x.pop_20 &gt;= 30000) & (x.pop_20 &lt; 100000), \"Medium\"),\n            (x.pop_20 == x.pop_20, \"Other\"),\n        ]\n    )\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n233140.0\nOther\n\n\n2\n25180.0\nOther\n\n\n...\n...\n...\n\n\n3223\n22044.0\nOther\n\n\n3224\n30364.0\nMedium\n\n\n3225\nNaN\nNaN\n\n\n\n\n3226 rows × 2 columns\n\n\n\nThis x.pop_20 == x.pop_20 condition can be read as “for everything else…”.\n\n\n\n\n\n\nWatch Out\n\n\n\n16.7.1 Order of conditions\nIt is important to use x.pop_20 == x.pop_20 as the final condition in case_when(). If you use it as the first condition, it will take precedence over all others, as seen here:\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: x.pop_20.case_when(\n        [\n            (x.pop_20 == x.pop_20, \"Other\"),\n            ((x.pop_20 &gt;= 30000) & (x.pop_20 &lt; 100000), \"Medium\"),\n        ]\n    )\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nNaN\n\n\n1\n233140.0\nOther\n\n\n2\n25180.0\nOther\n\n\n...\n...\n...\n\n\n3223\n22044.0\nOther\n\n\n3224\n30364.0\nOther\n\n\n3225\nNaN\nNaN\n\n\n\n\n3226 rows × 2 columns\n\n\n\nAs you can observe, all non-NaN counties are now coded with “Other”, because the x.pop_20 == x.pop_20 condition was placed first, and therefore took precedence.\nStatements with case_when are evaluated from top to bottom, so conditions that should be evaluated first should be placed higher in the list.\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.7.2 Practice Q: Recode income class 2\nUsing the counties_income data, and the hh_inc_21 column, create a new column called income_class that has the value “Medium income” for counties with median household income from $60,000 to below $100,000 and “Other” for all other counties. (Use a catch-all condition.) Your new dataframe should be called Q_income_class.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#counting-counties-by-name-characteristics",
    "href": "p_untangled_case_when.html#counting-counties-by-name-characteristics",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.8 Counting counties by name characteristics",
    "text": "16.8 Counting counties by name characteristics\nTo get more practice with case_when(), let’s explore a fun application of it by analyzing county names.\nWe’ll look for counties that have the words “River” or “Rio” (spanish for river) in their names. This may be interesting to know, as these often indicate counties with significant water features.\nFirst, we’ll use case_when() to create a new column that flags these counties:\n\ncounties_names = counties[[\"county\"]]\n\ncounties_names = counties_names.assign(\n    water_name=lambda x: x.county.case_when(\n        [\n            (x.county.str.contains(\"River|Rio\"), \"River in name\"),\n            (x.county == x.county, \"Other\"),\n        ]\n    )\n)\ncounties_names\n\n\n\n\n\n\n\n\ncounty\nwater_name\n\n\n\n\n0\nAutauga, AL\nOther\n\n\n1\nBaldwin, AL\nOther\n\n\n2\nBarbour, AL\nOther\n\n\n...\n...\n...\n\n\n3223\nVillalba, PR\nOther\n\n\n3224\nYabucoa, PR\nOther\n\n\n3225\nYauco, PR\nOther\n\n\n\n\n3226 rows × 2 columns\n\n\n\nIn this code, we’re using a regular expression \"River|Rio\" to match either “River” or “Rio” in the county name. The | symbol in regex means “or”. We’re also using our catch-all condition x.county == x.county to label all other counties as “Other”.\nLet’s see how many counties we’ve identified:\n\ncounties_names[\"water_name\"].value_counts(normalize=True)\n\nwater_name\nOther            0.99597\nRiver in name    0.00403\nName: proportion, dtype: float64\n\n\nInteresting! It looks like about X% of counties have “River” or “Rio” in their name.\nNow, let’s use .query() to keep just those water-related counties:\n\nwater_counties = counties_names.query(\"water_name == 'Water in name'\")\nwater_counties\n\n\n\n\n\n\n\n\ncounty\nwater_name\n\n\n\n\n\n\n\n\n\nCool! Do you recognize any of these counties?\n\n\n\n\n\n\nPractice\n\n\n\n16.8.1 Practice Q: Find lake or bay counties\nIn a similar way to what we did with “River” or “Rio”, find counties with “Lake” or “Bay” in their names. Create a new column called lake_bay_name and then use .query() to create a dataframe called lake_bay_counties with only these counties.\nStart with the counties_names dataframe.\n\n# Your code here\n\nHow does the proportion of “Lake” or “Bay” counties compare to “River” or “Rio” counties?\n\n\nThis example shows how we can use case_when() along with string methods and regular expressions to create meaningful categories based on text data. It’s a powerful way to explore patterns in your dataset!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#matching-nans-with-isna",
    "href": "p_untangled_case_when.html#matching-nans-with-isna",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.9 Matching NaN’s with isna()",
    "text": "16.9 Matching NaN’s with isna()\nAs you may have noticed, the pop_20 column contains NaN values, and these are not matched with any of the conditions we provided.\nWe can match missing values manually with isna(). Below we match NaN populations with isna() and set their population size to “Missing population”:\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: x.pop_20.case_when(\n        [\n            (x.pop_20 &lt; 50000, \"Smaller\"),\n            (x.pop_20 &gt;= 50000, \"Larger\"),\n            (x.pop_20.isna(), \"Missing\"),\n        ]\n    )\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nMissing\n\n\n1\n233140.0\nLarger\n\n\n2\n25180.0\nSmaller\n\n\n...\n...\n...\n\n\n3223\n22044.0\nSmaller\n\n\n3224\n30364.0\nSmaller\n\n\n3225\nNaN\nMissing\n\n\n\n\n3226 rows × 2 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n16.9.1 Practice Q: Recode economic type\nThe econ_type column of the counties dataset describes the primary economic activity of the county:\n\npractice_econ = counties[[\"econ_type\"]]\npractice_econ.value_counts(dropna=False)\n\necon_type               \nNonspecialized              1237\nManufacturing                501\nFarming                      444\nFederal/State Government     407\nRecreation                   333\nMining                       221\nNaN                           83\nName: count, dtype: int64\n\n\nImplement the following recoding, storing the new data in a column called econ_type_recode:\n\nFarming, Mining and Manufacturing to “Industry”\nFederal/State Government to “Government”\nRecreation and Nonspecialized to “Other”\nNaNs to “Missing”\n\nTo get you started, here is how to recode “Farming, Mining and Manufacturing” to “Industry”:\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#binary-conditions-np.where",
    "href": "p_untangled_case_when.html#binary-conditions-np.where",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.10 Binary conditions: np.where()",
    "text": "16.10 Binary conditions: np.where()\nThere is another function similar to case_when() for when we want to apply a binary condition to a variable: np.where(). A binary condition is either True or False.\nLet’s test it out with a simple Series:\n\nx = pd.Series([1, 2, 3, 4, 5])\nx\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n\nnp.where(x &gt; 3, \"Above 3\", \"3 or below\")\n\narray(['3 or below', '3 or below', '3 or below', 'Above 3', 'Above 3'],\n      dtype='&lt;U10')\n\n\nAs you can see, np.where() returns a new array with the values “Above 3” for all values greater than 3, and “3 or below” for all values less than or equal to 3.\nThis is useful when you want to make a quick modification to a variable, and don’t want to type up a whole case_when() statement.\nLet’s try it out on our dataset to classify counties as “Larger” if their population is 50000 or more, and “Smaller” if their population is less than 50000.\n\ncounties_pop = counties_pop.assign(\n    pop_class=lambda x: np.where(x.pop_20 &gt;= 50000, \"Larger\", \"Smaller\")\n)\ncounties_pop\n\n\n\n\n\n\n\n\npop_20\npop_class\n\n\n\n\n0\nNaN\nSmaller\n\n\n1\n233140.0\nLarger\n\n\n2\n25180.0\nSmaller\n\n\n...\n...\n...\n\n\n3223\n22044.0\nSmaller\n\n\n3224\n30364.0\nSmaller\n\n\n3225\nNaN\nSmaller\n\n\n\n\n3226 rows × 2 columns\n\n\n\nNice and easy!\nSince it is only a binary condition, it’s utility is limited, but it can be useful for quick transformations.\n\n\n\n\n\n\nPractice\n\n\n\n16.10.1 Practice Q: Recode income class with np.where()\nWith the counties_income data, make a new column, called income_class, that has the value “Medium income” for counties with median household income between $60,000 and $100,000 (exclusive of 100k) and “Other” for all other counties. Use the np.where() function.\n\n# Complete the code with your answer:\nQ_income_class = counties_income.assign(\n    income_class=lambda x: np.where(\n        # Your code here\n    )\n)",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_case_when.html#wrap-up",
    "href": "p_untangled_case_when.html#wrap-up",
    "title": "16  Conditional assignment with case_when() and np.where()",
    "section": "16.11 Wrap up",
    "text": "16.11 Wrap up\nChanging or constructing your variables based on conditions on other variables is one of the most repeated data wrangling tasks.\nI hope now that you will feel comfortable using case_when() and np.where() within assign().\nSoon we will learn more complex pandas operations such as grouping variables and summarizing them.\nSee you next time!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Conditional assignment with case_when() and np.where()</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html",
    "href": "p_untangled_groupby_agg.html",
    "title": "17  Grouping and summarizing data",
    "section": "",
    "text": "17.1 Introduction\nIn this lesson, we’ll explore two powerful pandas methods: agg() and groupby(). These tools will enable you to extract summary statistics and perform operations on grouped data effortlessly.\nLet’s dive in and discover how to unlock deeper insights from your data!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#learning-objectives",
    "href": "p_untangled_groupby_agg.html#learning-objectives",
    "title": "17  Grouping and summarizing data",
    "section": "17.2 Learning objectives",
    "text": "17.2 Learning objectives\n\nYou can use pandas.DataFrame.agg() to extract summary statistics from datasets.\nYou can use pandas.DataFrame.groupby() to group data by one or more variables before performing operations on them.\nYou can use sum() together with groupby()-agg() to count rows that meet a condition.\nYou can pass custom functions to agg() to compute summary statistics.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#libraries",
    "href": "p_untangled_groupby_agg.html#libraries",
    "title": "17  Grouping and summarizing data",
    "section": "17.3 Libraries",
    "text": "17.3 Libraries\nRun the following lines to import the necessary libraries:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "href": "p_untangled_groupby_agg.html#the-yaounde-covid-19-dataset",
    "title": "17  Grouping and summarizing data",
    "section": "17.4 The Yaounde COVID-19 dataset",
    "text": "17.4 The Yaounde COVID-19 dataset\nIn this lesson, we will again use data from the COVID-19 serological survey conducted in Yaounde, Cameroon.\n\nyaounde = pd.read_csv('data/yaounde_data.csv')\n\n## A smaller subset of variables\nyao = yaounde[['age', 'age_category_3', 'sex', 'weight_kg', 'height_cm',\n               'neighborhood', 'is_smoker', 'is_pregnant', 'occupation',\n               'treatment_combinations', 'symptoms', 'n_days_miss_work', 'n_bedridden_days',\n               'highest_education', 'igg_result']]\n\nyao\n\n\n\n\n\n\n\n\nage\nage_category_3\nsex\nweight_kg\nheight_cm\nneighborhood\nis_smoker\nis_pregnant\noccupation\ntreatment_combinations\nsymptoms\nn_days_miss_work\nn_bedridden_days\nhighest_education\nigg_result\n\n\n\n\n0\n45\nAdult\nFemale\n95\n169\nBriqueterie\nNon-smoker\nNo\nInformal worker\nParacetamol\nMuscle pain\n0.0\n0.0\nSecondary\nNegative\n\n\n1\n55\nAdult\nMale\n96\n185\nBriqueterie\nEx-smoker\nNaN\nSalaried worker\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nPositive\n\n\n2\n23\nAdult\nMale\n74\n180\nBriqueterie\nSmoker\nNaN\nStudent\nNaN\nNo symptoms\nNaN\nNaN\nUniversity\nNegative\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n968\n35\nAdult\nMale\n77\n168\nTsinga Oliga\nSmoker\nNaN\nInformal worker\nParacetamol\nHeadache\n0.0\n0.0\nUniversity\nPositive\n\n\n969\n31\nAdult\nFemale\n66\n169\nTsinga Oliga\nNon-smoker\nNo\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n970\n17\nChild\nFemale\n67\n162\nTsinga Oliga\nNon-smoker\nNo response\nUnemployed\nNaN\nNo symptoms\nNaN\nNaN\nSecondary\nNegative\n\n\n\n\n971 rows × 15 columns\n\n\n\nYou can find out more about this dataset here: https://www.nature.com/articles/s41467-021-25946-0",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#what-are-summary-statistics",
    "href": "p_untangled_groupby_agg.html#what-are-summary-statistics",
    "title": "17  Grouping and summarizing data",
    "section": "17.5 What are summary statistics?",
    "text": "17.5 What are summary statistics?\nA summary statistic is a single value (such as a mean or median) that describes a sequence of values (typically a column in your dataset).\nComputing summary statistics is a very common operation in most data analysis workflows, so it will be important to become fluent in extracting them from your datasets. And for this task, there is no better tool than the pandas method agg()! So let’s see how to use this powerful method.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "href": "p_untangled_groupby_agg.html#introducing-pandas.dataframe.agg",
    "title": "17  Grouping and summarizing data",
    "section": "17.6 Introducing pandas.DataFrame.agg()",
    "text": "17.6 Introducing pandas.DataFrame.agg()\nTo get started, let’s consider how to get simple summary statistics without using agg(), then we will consider why you should actually use agg().\nImagine you were asked to find the mean age of respondents in the yao data frame. You can do this by calling the mean() method on the age column of the yao data frame:\n\nyao[[\"age\"]].mean()\n\nage    29.017508\ndtype: float64\n\n\nAnd that’s it! You now have a simple summary statistic. Extremely easy, right?\nSo why do we need agg() to get summary statistics if the process is already so simple without it? We’ll come back to the why question soon. First let’s see how to obtain summary statistics with agg().\nGoing back to the previous example, the correct syntax to get the mean age with agg() would be:\n\nyao.agg(mean_age=('age', 'mean'))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\n\n\n\n\n\nThe anatomy of this syntax is:\n\ndataframe.agg(new_column_name=(\"COLUMN_TO_SUMMARIZE\", \"SUMMARY_FUNCTION\"))\n\n\nYou can also compute multiple summary statistics in a single agg() statement. For example, if you wanted both the mean and the median age, you could run:\n\nyao.agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n\n\n\n\n\n\n\n\nage\n\n\n\n\nmean_age\n29.017508\n\n\nmedian_age\n26.000000\n\n\n\n\n\n\n\nNice! Try your hand at the practice question below.\n\n\n\n\n\n\nPractice\n\n\n\n17.6.1 Practice Q: Mean and median weight\nUse agg() and the relevant summary functions to obtain the mean and median of respondent weights from the weight_kg variable of the yao data frame.\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.6.2 Practice Q: Min and max height\nUse agg() and the relevant summary functions to obtain the minimum and maximum respondent heights from the height_cm variable of the yao data frame. You may need to use a search engine to find out the minimum and maximum functions in Python if you don’t remember them.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "href": "p_untangled_groupby_agg.html#grouped-summaries-with-pandas.dataframe.groupby",
    "title": "17  Grouping and summarizing data",
    "section": "17.7 Grouped summaries with pandas.DataFrame.groupby()",
    "text": "17.7 Grouped summaries with pandas.DataFrame.groupby()\nNow let’s see how to use groupby() to obtain grouped summaries, the primary reason for using agg() in the first place.\nAs its name suggests, pandas.DataFrame.groupby() lets you group a data frame by the values in a variable (e.g. male vs female sex). You can then perform operations that are split according to these groups.\nLet’s try to group the yao data frame by sex and observe the effect:\n\nyao.groupby(\"sex\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x113c67350&gt;\n\n\nHmm. Apparently nothing happened. We just get a GroupBy object.\nBut watch what happens when we chain the groupby() with the agg() call we used in the previous section:\n\n(\n    yao.groupby(\"sex\")\n    .agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n)\n\n\n\n\n\n\n\n\nmean_age\nmedian_age\n\n\nsex\n\n\n\n\n\n\nFemale\n29.495446\n26.0\n\n\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\nNow we get a different statistic for each group! The mean age for female respondents is about 29.5, while for male respondents it’s about 28.4.\nAs was mentioned earlier, this kind of grouped summary is the primary reason the agg() function is so useful.\nYou may notice that there are two header rows. This is because the output has a hierarchical index (called a MultiIndex in pandas). While this can be useful in some cases, it often makes further data manipulation more difficult. We can reset the index to convert the group labels back to a regular column with the reset_index() method.\n\n(\n    yao.groupby(\"sex\")\n    .agg(mean_age=(\"age\", \"mean\"), median_age=(\"age\", \"median\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nmean_age\nmedian_age\n\n\n\n\n0\nFemale\n29.495446\n26.0\n\n\n1\nMale\n28.395735\n25.0\n\n\n\n\n\n\n\n\nLet’s see some more examples.\nSuppose you were asked to obtain the maximum and minimum weights for individuals in different neighborhoods, and also present the number of individuals in each neighborhood. We can write:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        max_weight=(\"weight_kg\", \"max\"),\n        min_weight=(\"weight_kg\", \"min\"),\n        count=(\"neighborhood\", \"size\"),  # the size function counts rows per group\n    )\n    .reset_index()\n) \n\n\n\n\n\n\n\n\nneighborhood\nmax_weight\nmin_weight\ncount\n\n\n\n\n0\nBriqueterie\n128\n20\n106\n\n\n1\nCarriere\n129\n14\n236\n\n\n2\nCité Verte\n118\n16\n72\n\n\n...\n...\n...\n...\n...\n\n\n6\nNkomkana\n161\n15\n75\n\n\n7\nTsinga\n105\n15\n81\n\n\n8\nTsinga Oliga\n100\n17\n67\n\n\n\n\n9 rows × 4 columns\n\n\n\nGreat! With just a few code lines you are able to extract quite a lot of information.\n\nLet’s see one more example for good measure. The variable n_days_miss_work tells us the number of days that respondents missed work due to COVID-like symptoms. Individuals who reported no COVID-like symptoms have an NA for this variable:\n\nyao[['n_days_miss_work']]\n\n\n\n\n\n\n\n\nn_days_miss_work\n\n\n\n\n0\n0.0\n\n\n1\nNaN\n\n\n2\nNaN\n\n\n...\n...\n\n\n968\n0.0\n\n\n969\nNaN\n\n\n970\nNaN\n\n\n\n\n971 rows × 1 columns\n\n\n\nTo count the total number of work days missed for each sex group, we can write:\n\n(\n    yao.groupby(\"sex\")\n    .agg(total_days_missed=(\"n_days_miss_work\", \"sum\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\ntotal_days_missed\n\n\n\n\n0\nFemale\n256.0\n\n\n1\nMale\n272.0\n\n\n\n\n\n\n\nThe output tells us that across all women in the sample, 256 work days were missed due to COVID-like symptoms, and across all men, 272 days.\n\nHopefully now you see why agg() is so powerful. In combination with groupby(), it lets you obtain highly informative grouped summaries of your datasets with very few lines of code.\nProducing such summaries is a very important part of most data analysis workflows, so this skill is likely to come in handy soon!\n\n\n\n\n\n\nPractice\n\n\n\n17.7.1 Practice Q: Min and max height per sex\nUse groupby(), agg(), and the relevant summary functions to obtain the minimum and maximum heights for each sex in the yao data frame, as well as the number of individuals in each sex group.\nYour output should be a DataFrame that looks like this:\n\n\n\nsex\nmin_height_cm\nmax_height_cm\ncount\n\n\n\n\nFemale\n\n\n\n\n\nMale\n\n\n\n\n\n\n\n# Your code here\n\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\nGroupby and agg are not always needed\nSometimes you just want a quick summary of a variable, not per group.\nRemember that there are quick ways to get such statistics. So you do not always need to use groupby() and agg().\n\nyao['age'].describe() # summary stats for a numeric variable\n\ncount    971.000000\nmean      29.017508\nstd       17.340397\n            ...    \n50%       26.000000\n75%       39.000000\nmax       79.000000\nName: age, Length: 8, dtype: float64\n\n\n\nyao['sex'].describe() # summary stats for a categorical variable\n\ncount        971\nunique         2\ntop       Female\nfreq         549\nName: sex, dtype: object\n\n\n\nyao['sex'].value_counts() # count of each category\n\nsex\nFemale    549\nMale      422\nName: count, dtype: int64\n\n\n\nyao.select_dtypes(object).describe() # summary stats for categorical variables\n\n\n\n\n\n\n\n\nage_category_3\nsex\nneighborhood\nis_smoker\nis_pregnant\noccupation\ntreatment_combinations\nsymptoms\nhighest_education\nigg_result\n\n\n\n\ncount\n971\n971\n971\n969\n549\n971\n262\n971\n971\n971\n\n\nunique\n3\n2\n9\n3\n3\n28\n31\n122\n7\n2\n\n\ntop\nAdult\nFemale\nCarriere\nNon-smoker\nNo\nStudent\nTraditional meds.\nNo symptoms\nSecondary\nNegative\n\n\nfreq\n635\n549\n236\n859\n464\n383\n57\n675\n433\n669\n\n\n\n\n\n\n\n\nyao.select_dtypes(np.number).describe() # summary stats for numeric variables\n\n\n\n\n\n\n\n\nage\nweight_kg\nheight_cm\nn_days_miss_work\nn_bedridden_days\n\n\n\n\ncount\n971.000000\n971.000000\n971.000000\n296.000000\n295.000000\n\n\nmean\n29.017508\n64.405767\n158.944387\n1.783784\n1.145763\n\n\nstd\n17.340397\n23.053370\n18.776356\n4.583006\n2.525201\n\n\n...\n...\n...\n...\n...\n...\n\n\n50%\n26.000000\n66.000000\n164.000000\n0.000000\n0.000000\n\n\n75%\n39.000000\n78.000000\n170.000000\n2.000000\n2.000000\n\n\nmax\n79.000000\n162.000000\n196.000000\n45.000000\n30.000000\n\n\n\n\n8 rows × 5 columns\n\n\n\n\n\n\n\n\n\n\n\nWatch-out\n\n\n\n17.8 NaN values in agg()\nWhen using agg() to compute grouped summary statistics, pay attention to whether your group of interest contains NaN values.\nFor example, to get mean weight by smoking status, we can write:\n\n(\n    yao.groupby(\"is_smoker\")\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n\n\n\n\n\nBut this actually excludes some rows with NaN smoking status from the summary table.\nWe can include these individuals in the summary table by setting dropna=False with the groupby() function.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n3\nNaN\n73.000000\n\n\n\n\n\n\n\nAlso recall that you can see how many individuals are in each smoking status group by using the size() function. It is often useful to include this information in your summary table, so that you know how many individuals are behind each summary statistic.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"), \n         count=(\"is_smoker\", \"size\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\ncount\n\n\n\n\n0\nEx-smoker\n76.366197\n71\n\n\n1\nNon-smoker\n63.033760\n859\n\n\n2\nSmoker\n72.410256\n39\n\n\n3\nNaN\n73.000000\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.8.1 Practice Q: Mean weight by pregnancy status\nUse groupby(), agg(), and the mean() function to obtain the mean weight (kg) by pregnancy status in the yao data frame. Include individuals with NaN pregnancy status in the summary table.\nThe output data frame should look something like this:\n\n\n\nis_pregnant\nweight_mean\n\n\n\n\nNo\n\n\n\nNo response\n\n\n\nYes\n\n\n\nNaN\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "href": "p_untangled_groupby_agg.html#nan-values-in-agg",
    "title": "17  Grouping and summarizing data",
    "section": "17.8 NaN values in agg()",
    "text": "17.8 NaN values in agg()\nWhen using agg() to compute grouped summary statistics, pay attention to whether your group of interest contains NaN values.\nFor example, to get mean weight by smoking status, we can write:\n\n(\n    yao.groupby(\"is_smoker\")\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n\n\n\n\n\nBut this actually excludes some rows with NaN smoking status from the summary table.\nWe can include these individuals in the summary table by setting dropna=False with the groupby() function.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\n\n\n\n\n0\nEx-smoker\n76.366197\n\n\n1\nNon-smoker\n63.033760\n\n\n2\nSmoker\n72.410256\n\n\n3\nNaN\n73.000000\n\n\n\n\n\n\n\nAlso recall that you can see how many individuals are in each smoking status group by using the size() function. It is often useful to include this information in your summary table, so that you know how many individuals are behind each summary statistic.\n\n(\n    yao.groupby(\"is_smoker\", dropna=False)\n    .agg(weight_mean=(\"weight_kg\", \"mean\"), \n         count=(\"is_smoker\", \"size\"))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nis_smoker\nweight_mean\ncount\n\n\n\n\n0\nEx-smoker\n76.366197\n71\n\n\n1\nNon-smoker\n63.033760\n859\n\n\n2\nSmoker\n72.410256\n39\n\n\n3\nNaN\n73.000000\n2",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "href": "p_untangled_groupby_agg.html#grouping-by-multiple-variables-nested-grouping",
    "title": "17  Grouping and summarizing data",
    "section": "17.9 Grouping by multiple variables (nested grouping)",
    "text": "17.9 Grouping by multiple variables (nested grouping)\nIt is possible to group a data frame by more than one variable. This is sometimes called “nested” grouping.\nSuppose you want to know the mean age of men and women in each neighbourhood, you could put both sex and neighborhood in the groupby() statement:\n\n(\n    yao\n    .groupby(['sex', 'neighborhood'])\n    .agg(mean_age=('age', 'mean'))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nsex\nneighborhood\nmean_age\n\n\n\n\n0\nFemale\nBriqueterie\n31.622951\n\n\n1\nFemale\nCarriere\n28.164286\n\n\n2\nFemale\nCité Verte\n31.750000\n\n\n...\n...\n...\n...\n\n\n15\nMale\nNkomkana\n29.812500\n\n\n16\nMale\nTsinga\n28.820513\n\n\n17\nMale\nTsinga Oliga\n24.297297\n\n\n\n\n18 rows × 3 columns\n\n\n\nFrom this output data frame you can tell that, for example, women from Briqueterie have a mean age of 31.6 years.\nThe order of the columns listed in groupby() is interchangeable. So if you run groupby(['neighborhood', 'sex']) instead of groupby(['sex', 'neighborhood']), you’ll get the same result, although it will be ordered differently:\n\n(\n    yao\n    .groupby(['neighborhood', 'sex'])\n    .agg(mean_age=('age', 'mean'))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nsex\nmean_age\n\n\n\n\n0\nBriqueterie\nFemale\n31.622951\n\n\n1\nBriqueterie\nMale\n33.711111\n\n\n2\nCarriere\nFemale\n28.164286\n\n\n...\n...\n...\n...\n\n\n15\nTsinga\nMale\n28.820513\n\n\n16\nTsinga Oliga\nFemale\n24.266667\n\n\n17\nTsinga Oliga\nMale\n24.297297\n\n\n\n\n18 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.9.1 Practice Q: Total bedridden days per age-sex group\nUse groupby(), agg(), and the sum() function to calculate the total number of bedridden days (from the n_bedridden_days variable) reported by respondents of each age-sex group. For age group, use the age_category_3 variable.\nYour output should be a data frame with three columns named as shown below:\n\n\n\nage_category_3\nsex\ntotal_bedridden_days\n\n\n\n\nAdult\nFemale\n\n\n\nAdult\nMale\n\n\n\nChild\nFemale\n\n\n\nChild\nMale\n\n\n\nSenior\nFemale\n\n\n\nSenior\nMale\n\n\n\n\n\n# your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#custom-summary-statistics",
    "href": "p_untangled_groupby_agg.html#custom-summary-statistics",
    "title": "17  Grouping and summarizing data",
    "section": "17.10 Custom summary statistics",
    "text": "17.10 Custom summary statistics\nSometimes, you may want to apply custom summary statistics that aren’t available as built-in functions. In these cases, you can use lambda functions or define your own functions to use with agg().\nFor example, let’s say we want to calculate the range (difference between maximum and minimum) of weights in each neighborhood:\n\n(\n    yao\n    .groupby('neighborhood')\n    .agg(weight_range=('weight_kg', lambda x: x.max() - x.min()))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nweight_range\n\n\n\n\n0\nBriqueterie\n108\n\n\n1\nCarriere\n115\n\n\n2\nCité Verte\n102\n\n\n...\n...\n...\n\n\n6\nNkomkana\n146\n\n\n7\nTsinga\n90\n\n\n8\nTsinga Oliga\n83\n\n\n\n\n9 rows × 2 columns\n\n\n\nNotice that we still provide a tuple to the agg() function, ('weight_kg', lambda x: x.max() - x.min()), but the second element of the tuple is a lambda function.\nThis lambda function operates on the column provided in the tuple, weight_kg.\n(Unlike the lambda functions we saw in the assign() method, we do not need to access the column within the lambda with x.weight_kg.max() - x.weight_kg.min(). We simply use x.max() - x.min().)\n\n\n\n\n\n\nPractice\n\n\n\n17.10.1 Practice Q: IQR of age by neighborhood\nFind the interquartile range (IQR) of the age variable for each neighborhood. The IQR is the difference between the 75th and 25th percentiles. Your lambda will look like this: lambda x: x.quantile(0.75) - x.quantile(0.25)\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#counting-rows-that-meet-a-condition",
    "href": "p_untangled_groupby_agg.html#counting-rows-that-meet-a-condition",
    "title": "17  Grouping and summarizing data",
    "section": "17.11 Counting rows that meet a condition",
    "text": "17.11 Counting rows that meet a condition\nIt is sometimes useful to count the rows that meet specific conditions within a group. This can be done with the groupby and agg functions.\nFor example, to count the number of people under 18 in each neighborhood, we can write:\n\n(\n    yao\n    .groupby('neighborhood')\n    .agg(num_children=('age', lambda x: (x &lt; 18).sum()))\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nnum_children\n\n\n\n\n0\nBriqueterie\n28\n\n\n1\nCarriere\n58\n\n\n2\nCité Verte\n19\n\n\n...\n...\n...\n\n\n6\nNkomkana\n22\n\n\n7\nTsinga\n23\n\n\n8\nTsinga Oliga\n25\n\n\n\n\n9 rows × 2 columns\n\n\n\nThe lambda function (x &lt; 18).sum() counts the number of values in the x series that are less than 18. The x series is the age variable for each neighborhood.\nLet’s broaden our scope and add the number of seniors (65+) to the summary table.\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        num_children=(\"age\", lambda x: (x &lt; 18).sum()),\n        num_seniors=(\"age\", lambda x: (x &gt;= 65).sum())\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nnum_children\nnum_seniors\n\n\n\n\n0\nBriqueterie\n28\n9\n\n\n1\nCarriere\n58\n9\n\n\n2\nCité Verte\n19\n4\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n22\n6\n\n\n7\nTsinga\n23\n5\n\n\n8\nTsinga Oliga\n25\n0\n\n\n\n\n9 rows × 3 columns",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#calculate-percentage-shares",
    "href": "p_untangled_groupby_agg.html#calculate-percentage-shares",
    "title": "17  Grouping and summarizing data",
    "section": "17.12 Calculate percentage shares",
    "text": "17.12 Calculate percentage shares\nWe can expand our previous example to calculate the percentage of children and seniors in each neighborhood.\nCalculating such percentage shares is a common task in data analysis.\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        children_percent=(\"age\", lambda x: (x &lt; 18).sum() / len(x) * 100),\n        seniors_percent=(\"age\", lambda x: (x &gt;= 65).sum() / len(x) * 100),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nchildren_percent\nseniors_percent\n\n\n\n\n0\nBriqueterie\n26.415094\n8.490566\n\n\n1\nCarriere\n24.576271\n3.813559\n\n\n2\nCité Verte\n26.388889\n5.555556\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n29.333333\n8.000000\n\n\n7\nTsinga\n28.395062\n6.172840\n\n\n8\nTsinga Oliga\n37.313433\n0.000000\n\n\n\n\n9 rows × 3 columns\n\n\n\nHopefully the code above is not too confusing. To calculate the percentage of children, we divide the number of children (x &lt; 18).sum() by the total number of individuals in the series, len(x), and multiply by 100. Likewise for seniors.\nA more concise approach utilizes the fact that the mean of a boolean series represents the proportion of True values:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        children_percent=(\"age\", lambda x: (x &lt; 18).mean() * 100),\n        seniors_percent=(\"age\", lambda x: (x &gt;= 65).mean() * 100),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\nchildren_percent\nseniors_percent\n\n\n\n\n0\nBriqueterie\n26.415094\n8.490566\n\n\n1\nCarriere\n24.576271\n3.813559\n\n\n2\nCité Verte\n26.388889\n5.555556\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n29.333333\n8.000000\n\n\n7\nTsinga\n28.395062\n6.172840\n\n\n8\nTsinga Oliga\n37.313433\n0.000000\n\n\n\n\n9 rows × 3 columns\n\n\n\nWhile this method is more efficient, it may be less intuitive. If you find it confusing, feel free to use the more explicit calculation shown earlier.\n\nAs a final example, to count the number and percentage of people with doctorate degrees in each neighborhood, we can write:\n\n(\n    yao.groupby(\"neighborhood\")\n    .agg(\n        count_w_doctorates=(\"highest_education\", lambda x: (x == \"Doctorate\").sum()),\n        pct_in_neighborhood_w_doctorates=(\n            \"highest_education\",\n            lambda x: (x == \"Doctorate\").mean() * 100,\n        ),\n    )\n    .reset_index()\n)\n\n\n\n\n\n\n\n\nneighborhood\ncount_w_doctorates\npct_in_neighborhood_w_doctorates\n\n\n\n\n0\nBriqueterie\n2\n1.886792\n\n\n1\nCarriere\n1\n0.423729\n\n\n2\nCité Verte\n1\n1.388889\n\n\n...\n...\n...\n...\n\n\n6\nNkomkana\n4\n5.333333\n\n\n7\nTsinga\n3\n3.703704\n\n\n8\nTsinga Oliga\n3\n4.477612\n\n\n\n\n9 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n17.12.1 Practice Q: Smoker rate by sex and age category\nCalculate the percentage of people with are smokers (use the is_smoker variable) for each combination of sex and age category (use age_category_3). The output should look like this:\n\n\n\nsex\nage_category_3\nsmoker_rate\n\n\n\n\nFemale\nAdult\n\n\n\nFemale\nChild\n\n\n\nFemale\nSenior\n\n\n\nMale\nAdult\n\n\n\nMale\nChild\n\n\n\nMale\nSenior\n\n\n\n\nYour summary table should show that male adults have the highest smoker rate, followed by male seniors.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_groupby_agg.html#wrap-up",
    "href": "p_untangled_groupby_agg.html#wrap-up",
    "title": "17  Grouping and summarizing data",
    "section": "17.13 Wrap up",
    "text": "17.13 Wrap up\nIn this lesson, you’ve learned how to obtain quick summary statistics from your data using agg(), group your data using groupby(), and combine groupby() with agg() for powerful data summarization.\nThese skills are essential for both exploratory data analysis and preparing data for presentation or plotting. The combination of groupby() and agg() is one of the most common and useful data manipulation techniques in pandas.\nIn our next lesson, we’ll explore ways to combine groupby() with other pandas methods.\nSee you there!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Grouping and summarizing data</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html",
    "href": "p_untangled_other_grouped_operations.html",
    "title": "18  Grouped Transformations and Filtering",
    "section": "",
    "text": "18.1 Introduction to Grouped Operations\nData wrangling often involves applying the same operations separately to different groups within the data. In this lesson, we’ll learn how to use groupby() with transform() and assign() to conduct grouped transformations on a DataFrame.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#learning-objectives",
    "href": "p_untangled_other_grouped_operations.html#learning-objectives",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.2 Learning Objectives",
    "text": "18.2 Learning Objectives\n\nYou can use groupby() with transform() and assign() to conduct grouped operations on a DataFrame.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#required-packages",
    "href": "p_untangled_other_grouped_operations.html#required-packages",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.3 Required Packages",
    "text": "18.3 Required Packages\nThis lesson requires pandas and numpy:\n\nimport pandas as pd\nimport numpy as np",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#simple-toy-dataset",
    "href": "p_untangled_other_grouped_operations.html#simple-toy-dataset",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.4 Simple Toy Dataset",
    "text": "18.4 Simple Toy Dataset\nLet’s start with a simple toy dataset to illustrate the concepts:\n\n# Create a simple toy dataset\ntoy_data = pd.DataFrame(\n    {\n        \"year\": [2022, 2022, 2023, 2023],\n        \"month\": [\"Jan\", \"Feb\", \"Jan\", \"Feb\"],\n        \"sales\": [2, 4, 6, 8],\n    }\n)\n\nprint(toy_data)\n\n   year month  sales\n0  2022   Jan      2\n1  2022   Feb      4\n2  2023   Jan      6\n3  2023   Feb      8\n\n\n\n18.4.1 Regular assign() Call\nLet’s add a column with the mean sales for the entire dataset, then calculate the difference between each month’s sales and the mean sales:\n\ntoy_data.assign(\n    mean_sales=lambda x: x.sales.mean(), \n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2022\nJan\n2\n5.0\n-3.0\n\n\n1\n2022\nFeb\n4\n5.0\n-1.0\n\n\n2\n2023\nJan\n6\n5.0\n1.0\n\n\n3\n2023\nFeb\n8\n5.0\n3.0\n\n\n\n\n\n\n\nIn this example, the mean_sales column contains the mean sales value for the entire dataset (5), repeated in each row. The sales_diff column shows the difference between each month’s sales and the overall mean sales.\n\n\n18.4.2 Grouped assign() Call\nBut we might want to see how each month’s sales compare to the mean sales for that year.\n\ntoy_data.assign(\n    mean_sales=lambda x: x.groupby(\"year\").sales.mean(),\n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2022\nJan\n2\nNaN\nNaN\n\n\n1\n2022\nFeb\n4\nNaN\nNaN\n\n\n2\n2023\nJan\n6\nNaN\nNaN\n\n\n3\n2023\nFeb\n8\nNaN\nNaN\n\n\n\n\n\n\n\nNow we have the mean sales for year 2022 as 3 and for year 2023 as 7. and the sales_diff column shows how far each month’s sales are from the mean sales for that year.\nLet’s break down the code, especially the first line:\n\nWithin the assign method, we initiate the mean_sales column\nThen we pass the lambda function to the mean_sales column\nThe lambda function takes the dataframe, denoted x as input, groups it by year (x.groupby(\"year\")), then pulls out the sales column (x.groupby(\"year\").sales) then to compute the mean for each year, we use the transform() method (x.groupby(\"year\").sales.transform(\"mean\")).",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#real-dataset-austin-housing",
    "href": "p_untangled_other_grouped_operations.html#real-dataset-austin-housing",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.5 Real Dataset: Austin Housing",
    "text": "18.5 Real Dataset: Austin Housing\nNow let’s apply these concepts to a real dataset. We’ll use the housing dataset containing housing sales data for Austin, Texas:\n\nhousing = pd.read_csv(\"data/austinhousing.csv\", usecols=[\"year\", \"month\", \"sales\"])\n\n## Display first few rows of the dataset\nhousing\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\n\n\n\n\n0\n2000\n1\n1025\n\n\n1\n2000\n2\n1277\n\n\n2\n2000\n3\n1603\n\n\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n\n\n185\n2015\n6\n3301\n\n\n186\n2015\n7\n3466\n\n\n\n\n187 rows × 3 columns\n\n\n\n\n18.5.1 Regular assign() Call\nLet’s add a column with the mean sales for the entire dataset, then calculate the difference between each month’s sales and the mean sales:\n\nhousing.assign(\n    mean_sales=lambda x: x.sales.mean(), \n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2000\n1\n1025\n1996.68984\n-971.68984\n\n\n1\n2000\n2\n1277\n1996.68984\n-719.68984\n\n\n2\n2000\n3\n1603\n1996.68984\n-393.68984\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n1996.68984\n1002.31016\n\n\n185\n2015\n6\n3301\n1996.68984\n1304.31016\n\n\n186\n2015\n7\n3466\n1996.68984\n1469.31016\n\n\n\n\n187 rows × 5 columns\n\n\n\nIn this example, the mean_sales column contains the mean sales value for the entire dataset, repeated in each row. The sales_diff column shows the difference between each month’s sales and the overall mean sales.\n\n\n18.5.2 Grouped assign() Call\nNow, let’s add a column with the mean sales for each year, and then calculate the difference between each month’s sales and the mean sales for that year:\n\nhousing.assign(\n    mean_sales=lambda x: x.groupby(\"year\").sales.transform(\"mean\"),\n    sales_diff=lambda x: x.sales - x.mean_sales\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmean_sales\nsales_diff\n\n\n\n\n0\n2000\n1\n1025\n1551.750000\n-526.750000\n\n\n1\n2000\n2\n1277\n1551.750000\n-274.750000\n\n\n2\n2000\n3\n1603\n1551.750000\n51.250000\n\n\n...\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n2696.857143\n302.142857\n\n\n185\n2015\n6\n3301\n2696.857143\n604.142857\n\n\n186\n2015\n7\n3466\n2696.857143\n769.142857\n\n\n\n\n187 rows × 5 columns\n\n\n\nHere’s what’s happening: - Within the assign() method, we group by year and use transform(\"mean\") to compute the mean sales for each year. - The mean_sales column now contains the mean sales for each year, repeated in each row of that year. - The sales_diff column shows how far each month’s sales are from the mean sales for that year, allowing us to see which months are high and low performers within each year.\nThis grouped approach allows us to compare sales within each year, accounting for overall trends or differences between years.\n\n\n\n\n\n\nPractice\n\n\n\n18.5.3 Practice Q: Compute Mean Temperature for Each Day\nWith the gibraltar dataframe, group by the date of measurement, then in a new variable called mean_temp, compute the mean temperature for each day. Store the result in a new dataframe called mean_temp_question.\n\n## Complete the code with your answer:\nmean_temp_question = gibraltar.assign(\n    mean_temp=___________________________________-\n)\n\nmean_temp_question\n\n\n\n\n\n18.5.4 More Examples of Grouped Transformations\n\n18.5.4.1 Rank Sales Within Each Year\nNow let’s look at another example. We can add a column that ranks the sales volume for each year.\n\nhousing.assign(sales_rank=housing.groupby(\"year\").sales.rank(ascending=False))\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nsales_rank\n\n\n\n\n0\n2000\n1\n1025\n12.0\n\n\n1\n2000\n2\n1277\n10.0\n\n\n2\n2000\n3\n1603\n5.0\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n3.0\n\n\n185\n2015\n6\n3301\n2.0\n\n\n186\n2015\n7\n3466\n1.0\n\n\n\n\n187 rows × 4 columns\n\n\n\nIn this example, we’re creating a new column sales_rank that ranks the sales column in descending order within each year group. This allows us to see which months had the highest sales for each year.\nThese examples demonstrate how we can use grouped operations to perform more nuanced analyses on our data, allowing us to uncover patterns and insights within specific subgroups of our dataset.\n\n\n18.5.4.2 Cumulative Sales Column\nLet’s add a column that shows the cumulative sales for each year, adding up the sales for each month within the year.\n\nhousing.assign(\n    cumulative_sales=housing.groupby(\"year\").sales.transform(\"cumsum\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\ncumulative_sales\n\n\n\n\n0\n2000\n1\n1025\n1025\n\n\n1\n2000\n2\n1277\n2302\n\n\n2\n2000\n3\n1603\n3905\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n12111\n\n\n185\n2015\n6\n3301\n15412\n\n\n186\n2015\n7\n3466\n18878\n\n\n\n\n187 rows × 4 columns\n\n\n\n\n\n18.5.4.3 Percent of Yearly Sales Done Each Month\nWe can calculate the percent of sales done each month for that year.\n\nhousing.assign(\n    percent_sales=lambda x: 100 * x.sales / x.groupby(\"year\").sales.transform(\"sum\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\npercent_sales\n\n\n\n\n0\n2000\n1\n1025\n5.504538\n\n\n1\n2000\n2\n1277\n6.857849\n\n\n2\n2000\n3\n1603\n8.608560\n\n\n...\n...\n...\n...\n...\n\n\n184\n2015\n5\n2999\n15.886217\n\n\n185\n2015\n6\n3301\n17.485962\n\n\n186\n2015\n7\n3466\n18.359996\n\n\n\n\n187 rows × 4 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.5.5 Practice Q: Cumulative Listings Column\nWith the housing dataframe, group by year, then create a cumulative listings column for each year.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#grouped-filtering-with-query",
    "href": "p_untangled_other_grouped_operations.html#grouped-filtering-with-query",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.6 Grouped Filtering with query()",
    "text": "18.6 Grouped Filtering with query()\nTo query per group, it is usually better to create a relevant column with groupby() and transform(), then use query() on the resulting dataframe to filter data.\n\n18.6.1 Filter for Month with Highest Sales Volume per Year\nFor example, if we want to filter the data for the month with the highest sales volume per year group (the month with the highest sales volume for each year), we can use groupby() with transform() to first create a column with the max sales volume per year, then ungroup and filter.\n\n(\nhousing\n  .assign(max_sales=housing.groupby(\"year\").sales.transform(\"max\"))\n  .query(\"sales == max_sales\")\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\nmax_sales\n\n\n\n\n4\n2000\n5\n1980\n1980\n\n\n18\n2001\n7\n1871\n1871\n\n\n28\n2002\n5\n1931\n1931\n\n\n...\n...\n...\n...\n...\n\n\n162\n2013\n7\n3376\n3376\n\n\n173\n2014\n6\n3195\n3195\n\n\n186\n2015\n7\n3466\n3466\n\n\n\n\n16 rows × 4 columns\n\n\n\nWe can drop the max_sales column if we want to at the end.\n\n(\nhousing\n  .assign(max_sales=housing.groupby(\"year\").sales.transform(\"max\"))\n  .query(\"sales == max_sales\")\n  .drop(columns=[\"max_sales\"])\n)\n\n\n\n\n\n\n\n\nyear\nmonth\nsales\n\n\n\n\n4\n2000\n5\n1980\n\n\n18\n2001\n7\n1871\n\n\n28\n2002\n5\n1931\n\n\n...\n...\n...\n...\n\n\n162\n2013\n7\n3376\n\n\n173\n2014\n6\n3195\n\n\n186\n2015\n7\n3466\n\n\n\n\n16 rows × 3 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.6.2 Practice Q: Filter for Time of Day with Highest Wind Speed\nWith the gibraltar dataframe created above, group by the date of measurement, calculate the time of day with the highest recorded wind speed wind_speed for each day, then filter the dataframe to keep only the rows with the highest wind speed for each day.\nNote that for some days, there may be a tie in the highest wind speed.\n\n# Your code here\n\n\n\n\n\n18.6.3 Month with the Most “Typical” Sales\nWe can also use groupby() with transform() to find the month with the most “typical” sales. We’ll use the mean sales volume for each year as the “typical” sales volume, then get the month with the closest sales to that.\n\n(\nhousing\n  .assign(\n    mean_sales=housing.groupby(\"year\").sales.transform(\"mean\"),\n    sales_diff_abs=lambda x: (x.sales - x.mean_sales).abs()\n    )\n  .sort_values(\"sales_diff_abs\")\n  .groupby(\"year\")\n  .first()\n)\n\n\n\n\n\n\n\n\nmonth\nsales\nmean_sales\nsales_diff_abs\n\n\nyear\n\n\n\n\n\n\n\n\n2000\n4\n1556\n1551.750000\n4.250000\n\n\n2001\n3\n1553\n1532.666667\n20.333333\n\n\n2002\n3\n1550\n1559.666667\n9.666667\n\n\n...\n...\n...\n...\n...\n\n\n2013\n9\n2544\n2536.333333\n7.666667\n\n\n2014\n10\n2588\n2579.416667\n8.583333\n\n\n2015\n3\n2677\n2696.857143\n19.857143\n\n\n\n\n16 rows × 4 columns\n\n\n\n\n\n\n\n\n\nPractice\n\n\n\n18.6.4 Practice Q: Filter for Times with Above Mean Temperature\nGroup the gibraltar dataframe by date, then use apply(), assign(), and query() as needed to subset to only times when the temperature is above the mean temperature for that day. You should have 698 rows in the output.\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_other_grouped_operations.html#wrap-up",
    "href": "p_untangled_other_grouped_operations.html#wrap-up",
    "title": "18  Grouped Transformations and Filtering",
    "section": "18.7 Wrap Up",
    "text": "18.7 Wrap Up\nIn this lesson, we explored powerful techniques for grouped operations in pandas:\n\nUsing groupby() with transform() and assign() for grouped transformations\nCombining groupby(), transform(), and query() for grouped filtering\n\nThese methods significantly enhance our ability to analyze and manipulate data within specific groups.\nCongratulations on making it through!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Grouped Transformations and Filtering</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html",
    "href": "p_untangled_pivoting.html",
    "title": "19  Reshaping data",
    "section": "",
    "text": "19.1 Intro\nPivoting or reshaping is a data manipulation technique that involves re-orienting the rows and columns of a dataset. This is often required to make data easier to analyze or understand.\nIn this lesson, we will cover how to effectively pivot data using pandas functions.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#learning-objectives",
    "href": "p_untangled_pivoting.html#learning-objectives",
    "title": "19  Reshaping data",
    "section": "19.2 Learning Objectives",
    "text": "19.2 Learning Objectives\n\nUnderstand what wide data format is, and what long data format is.\nLearn how to pivot long data to wide data using melt().\nLearn how to pivot wide data to long data using pivot().",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "href": "p_untangled_pivoting.html#what-do-wide-and-long-mean",
    "title": "19  Reshaping data",
    "section": "19.3 What do wide and long mean?",
    "text": "19.3 What do wide and long mean?\nThe terms wide and long are best understood in the context of example datasets. Let’s take a look at some now.\nImagine that you have three products for which you collect sales data over three months.\nYou can record the data in a wide format like this:\n\n\n\nProduct\nJan\nFeb\nMar\n\n\n\n\nA\n100\n120\n110\n\n\nB\n90\n95\n100\n\n\nC\n80\n85\n90\n\n\n\n\nOr you could record the data in a long format as so:\n\n\n\nProduct\nMonth\nSales\n\n\n\n\nA\nJan\n100\n\n\nA\nFeb\n120\n\n\nA\nMar\n110\n\n\nB\nJan\n90\n\n\nB\nFeb\n95\n\n\nB\nMar\n100\n\n\nC\nJan\n80\n\n\nC\nFeb\n85\n\n\nC\nMar\n90\n\n\n\nTake a minute to study the two datasets to make sure you understand the relationship between them.\nIn the wide dataset, each observational unit (each product) occupies only one row. And each measurement (sales in Jan, Feb, Mar) is in a separate column.\nIn the long dataset, on the other hand, each observational unit (each product) occupies multiple rows, with one row for each measurement.\n\nHere is another example with mock data, in which the observational units are countries:\nLong format:\n\n\n\nCountry\nYear\nGDP\n\n\n\n\nUSA\n2020\n21433\n\n\nUSA\n2021\n22940\n\n\nChina\n2020\n14723\n\n\nChina\n2021\n17734\n\n\n\nWide format:\n\n\n\nCountry\nGDP_2020\nGDP_2021\n\n\n\n\nUSA\n21433\n22940\n\n\nChina\n14723\n17734\n\n\n\n\nThe examples above are both time-series datasets, because the measurements are repeated across time. But the concepts of long and wide are relevant to other kinds of data too.\nConsider the example below, showing the number of employees in different departments of three companies:\nWide format:\n\n\n\nCompany\nHR\nSales\nIT\n\n\n\n\nA\n10\n20\n15\n\n\nB\n8\n25\n20\n\n\nC\n12\n18\n22\n\n\n\nLong format:\n\n\n\nCompany\nDepartment\nEmployees\n\n\n\n\nA\nHR\n10\n\n\nA\nSales\n20\n\n\nA\nIT\n15\n\n\nB\nHR\n8\n\n\nB\nSales\n25\n\n\nB\nIT\n20\n\n\nC\nHR\n12\n\n\nC\nSales\n18\n\n\nC\nIT\n22\n\n\n\nIn the wide dataset, again, each observational unit (each company) occupies only one row, with the repeated measurements for that unit (number of employees in different departments) spread across multiple columns.\nIn the long dataset, each observational unit is spread over multiple lines.\n\n\n\n\n\n\nVocab\n\n\n\nThe “observational units”, sometimes called “statistical units” of a dataset are the primary entities or items described by the columns in that dataset.\nIn the first example, the observational/statistical units were products; in the second example, countries, and in the third example, companies.\n\n\n\n\n\n\n\n\nPractice\n\n\n\nConsider the mock dataset created below:\n\nimport pandas as pd\n\ntemperatures = pd.DataFrame(\n    {\n        \"country\": [\"Sweden\", \"Denmark\", \"Norway\"],\n        \"avgtemp.1994\": [1, 2, 3],\n        \"avgtemp.1995\": [3, 4, 5],\n        \"avgtemp.1996\": [5, 6, 7],\n    }\n)\ntemperatures\n\n\n\n\n\n\n\n\ncountry\navgtemp.1994\navgtemp.1995\navgtemp.1996\n\n\n\n\n0\nSweden\n1\n3\n5\n\n\n1\nDenmark\n2\n4\n6\n\n\n2\nNorway\n3\n5\n7\n\n\n\n\n\n\n\nIs this data in a wide or long format?\n\n# Write your answer here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#when-should-you-use-wide-vs-long-data",
    "href": "p_untangled_pivoting.html#when-should-you-use-wide-vs-long-data",
    "title": "19  Reshaping data",
    "section": "19.4 When should you use wide vs long data?",
    "text": "19.4 When should you use wide vs long data?\nThe truth is: it really depends on what you want to do! The wide format is great for displaying data because it’s easy to visually compare values this way. Long data is best for some data analysis tasks, like grouping and plotting.\nIt will therefore be essential for you to know how to switch from one format to the other easily. Switching from the wide to the long format, or the other way around, is called pivoting.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-wide-to-long",
    "href": "p_untangled_pivoting.html#pivoting-wide-to-long",
    "title": "19  Reshaping data",
    "section": "19.5 Pivoting wide to long",
    "text": "19.5 Pivoting wide to long\nTo practice pivoting from a wide to a long format, we’ll consider data from Our World in Data on fossil fuel consumption per capita. You can find the data here.\nBelow, we read in and view this data on fossil fuel consumption per capita:\n\nfuels_wide = pd.read_csv(\"data/oil_per_capita_wide.csv\")\nfuels_wide\n\n\n\n\n\n\n\n\nEntity\nCode\ny_1970\ny_1980\ny_1990\ny_2000\ny_2010\ny_2020\n\n\n\n\n0\nAlgeria\nDZA\n1764.8470\n3532.7976\n4381.6636\n3351.2180\n5064.9863\n4877.2680\n\n\n1\nArgentina\nARG\n11677.9680\n10598.3990\n7046.2485\n7146.8154\n7966.7827\n6399.2114\n\n\n2\nAustralia\nAUS\n23040.4550\n25007.4380\n23046.9510\n23976.3550\n23584.3070\n20332.4100\n\n\n3\nAustria\nAUT\n14338.8090\n19064.0920\n16595.1930\n18189.0920\n18424.1170\n14934.0650\n\n\n4\nAzerbaijan\nAZE\nNaN\nNaN\n13516.0190\n9119.3470\n4031.9407\n5615.1157\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\nUnited States\nUSA\n40813.9530\n42365.6500\n37525.5160\n37730.1600\n31791.3070\n26895.4770\n\n\n77\nUzbekistan\nUZB\nNaN\nNaN\n6324.8677\n3197.1330\n1880.1338\n1859.1548\n\n\n78\nVenezuela\nVEN\n11138.2210\n16234.0960\n12404.5570\n11239.9260\n14948.3070\n4742.6226\n\n\n79\nVietnam\nVNM\n1757.6117\n439.9465\n523.2565\n1280.3065\n2296.7590\n2927.7446\n\n\n80\nWorld\nOWID_WRL\n7217.8340\n8002.0854\n7074.2583\n6990.4272\n6879.6110\n6216.8060\n\n\n\n\n81 rows × 8 columns\n\n\n\nWe observe that each observational unit (each country) occupies only one row, with the repeated measurements of fossil fuel consumption (in Kilowatt-hour equivalents) spread out across multiple columns. Hence this dataset is in a wide format.\nTo convert to a long format, we can use the convenient melt function. Within melt we define which columns we want to pivot:\n\n(fuels_wide\n .melt(id_vars=['Entity', 'Code'], \n       value_vars=fuels_wide.columns[2:])\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nvariable\nvalue\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n1\nArgentina\nARG\ny_1970\n11677.9680\n\n\n2\nAustralia\nAUS\ny_1970\n23040.4550\n\n\n3\nAustria\nAUT\ny_1970\n14338.8090\n\n\n4\nAzerbaijan\nAZE\ny_1970\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n481\nUnited States\nUSA\ny_2020\n26895.4770\n\n\n482\nUzbekistan\nUZB\ny_2020\n1859.1548\n\n\n483\nVenezuela\nVEN\ny_2020\n4742.6226\n\n\n484\nVietnam\nVNM\ny_2020\n2927.7446\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nVery easy!\nLet’s break down the code:\n\nid_vars refers to the column(s) that will not be pivoted. In this case, it’s the country and the code.\nvalue_vars is the column(s) that will be pivoted. In this case, it’s the years columns.\n\nThe years are now indicated in the variable variable, and all the consumption values occupy a single variable, value.We may wish to rename the variable column to year, and the value column to oil_consumption. This can be done directly in the melt function:\n\n(fuels_wide\n .melt(id_vars=['Entity', 'Code'], \n       value_vars=fuels_wide.columns[2:],\n       var_name='year', \n       value_name='oil_consumption')\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\ny_1970\n1764.8470\n\n\n1\nArgentina\nARG\ny_1970\n11677.9680\n\n\n2\nAustralia\nAUS\ny_1970\n23040.4550\n\n\n3\nAustria\nAUT\ny_1970\n14338.8090\n\n\n4\nAzerbaijan\nAZE\ny_1970\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n481\nUnited States\nUSA\ny_2020\n26895.4770\n\n\n482\nUzbekistan\nUZB\ny_2020\n1859.1548\n\n\n483\nVenezuela\nVEN\ny_2020\n4742.6226\n\n\n484\nVietnam\nVNM\ny_2020\n2927.7446\n\n\n485\nWorld\nOWID_WRL\ny_2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nYou may also want to remove the y_ in front of each year. This can be achieved with as string operation. We’ll also arrange the data by country and year:\n\n(\n    fuels_wide.melt(\n        id_vars=[\"Entity\", \"Code\"],\n        value_vars=fuels_wide.columns[2:],\n        var_name=\"year\",\n        value_name=\"oil_consumption\",\n    )\n    .assign(year=lambda df: df[\"year\"].str.replace(\"y_\", \"\").astype(int))\n    .sort_values(by=[\"Entity\", \"year\"])\n)\n\n\n\n\n\n\n\n\nEntity\nCode\nyear\noil_consumption\n\n\n\n\n0\nAlgeria\nDZA\n1970\n1764.8470\n\n\n81\nAlgeria\nDZA\n1980\n3532.7976\n\n\n162\nAlgeria\nDZA\n1990\n4381.6636\n\n\n243\nAlgeria\nDZA\n2000\n3351.2180\n\n\n324\nAlgeria\nDZA\n2010\n5064.9863\n\n\n...\n...\n...\n...\n...\n\n\n161\nWorld\nOWID_WRL\n1980\n8002.0854\n\n\n242\nWorld\nOWID_WRL\n1990\n7074.2583\n\n\n323\nWorld\nOWID_WRL\n2000\n6990.4272\n\n\n404\nWorld\nOWID_WRL\n2010\n6879.6110\n\n\n485\nWorld\nOWID_WRL\n2020\n6216.8060\n\n\n\n\n486 rows × 4 columns\n\n\n\nHere’s what we added above:\n\nIn the assign function, we used a lambda function to replace the y_ in front of each year with an empty string, and then convert the year to an integer.\nWe used the sort_values function to sort the data by country and year.\n\nNow we have a clean, long dataset; great! For later use, let’s now store this data:\n\nfuels_long = (\n    fuels_wide.melt(\n        id_vars=[\"Entity\", \"Code\"],\n        value_vars=fuels_wide.columns[2:],\n        var_name=\"year\",\n        value_name=\"oil_consumption\",\n    )\n    .assign(year=lambda df: df[\"year\"].str.replace(\"y_\", \"\").astype(int))\n    .sort_values(by=[\"Entity\", \"year\"])\n)\n\n\n\n\n\n\n\nPractice\n\n\n\nFor this practice question, you will use the euro_births_wide dataset from Eurostat. It shows the annual number of births in 50 European countries:\n\neuro_births_wide = pd.read_csv(\"data/euro_births_wide.csv\")\neuro_births_wide\n\n\n\n\n\n\n\n\ncountry\nx2015\nx2016\nx2017\nx2018\nx2019\nx2020\nx2021\n\n\n\n\n0\nBelgium\n122274.0\n121896.0\n119690.0\n118319.0\n117695.0\n114350.0\n118349.0\n\n\n1\nBulgaria\n65950.0\n64984.0\n63955.0\n62197.0\n61538.0\n59086.0\n58678.0\n\n\n2\nCzechia\n110764.0\n112663.0\n114405.0\n114036.0\n112231.0\n110200.0\n111793.0\n\n\n3\nDenmark\n58205.0\n61614.0\n61397.0\n61476.0\n61167.0\n60937.0\n63473.0\n\n\n4\nGermany\n737575.0\n792141.0\n784901.0\n787523.0\n778090.0\n773144.0\n795492.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n45\nUkraine\n411781.0\n397037.0\n363987.0\n235.0\n232.0\nNaN\n212.0\n\n\n46\nArmenia\n41763.0\n40592.0\n37700.0\n335874.0\n308817.0\n293457.0\n271983.0\n\n\n47\nAzerbaijan\n166210.0\n159464.0\n144041.0\n36574.0\n36041.0\n36353.0\nNaN\n\n\n48\nGeorgia\n59249.0\n56569.0\n53293.0\n138982.0\n141179.0\n126571.0\n112284.0\n\n\n49\nNaN\nNaN\nNaN\nNaN\n51138.0\n48296.0\n46520.0\n45946.0\n\n\n\n\n50 rows × 8 columns\n\n\n\nThe data is in a wide format. Convert it to a long format data frame that has the following column names: “country”, “year” and “births_count”\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-long-to-wide",
    "href": "p_untangled_pivoting.html#pivoting-long-to-wide",
    "title": "19  Reshaping data",
    "section": "19.6 Pivoting long to wide",
    "text": "19.6 Pivoting long to wide\nNow you know how to pivot from wide to long with melt. How about going the other way, from long to wide? For this, you can use the pivot function.\nBut before we consider how to use this function to manipulate long data, let’s first consider where you’re likely to run into long data.\nWhile wide data tends to come from external sources (as we have seen above), long data on the other hand, is likely to be created by you while data wrangling, especially in the course of grouped aggregations.\nLet’s see an example of this now.\nWe will use a dataset of contracts granted by the city of Chicago in the years 2020 to 2023. You can find more information about the data here. Below we import the data and do some data processing to prepare it for analysis.\n\ncontracts_raw = pd.read_csv(\"data/chicago_contracts_20_23.csv\")\n\ncontracts = (\n    contracts_raw\n    .assign(year_of_contract=lambda df: pd.to_datetime(df[\"approval_date\"]).dt.year)\n    .reindex(columns=[\"year_of_contract\"] + list(contracts_raw.columns.drop(\"approval_date\")))\n)\n\ncontracts\n\n\n\n\n\n\n\n\nyear_of_contract\ndescription\ncontract_num\nrevision_num\nspecification_num\ncontract_type\nstart_date\nend_date\ndepartment\nvendor_name\nvendor_id\naddress_1\naddress_2\ncity\nstate\nzip\naward_amount\nprocurement_type\ncontract_pdf\n\n\n\n\n0\n2020\nLEASE\n24406\n32\n96136\nPROPERTY LEASE\nNaN\nNaN\nNaN\n8700 BUILDING LLC\n89123305A\n7300 S NARRAGANSETT\nNaN\nBEDFORD PARK\nIllinois\n60638\n321.1\nNaN\nNaN\n\n\n1\n2020\nDFSS-HHS-CS-CEL:\n113798\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nCATHOLIC CHARITIES OF THE ARCHDIOCESE OF CHICAGO\n102484615A\n1 E BANKS ST\nNaN\nCHICAGO\nIllinois\n60670\n17692515.0\nNaN\nNaN\n\n\n2\n2020\nDFSS-HHS-CS-CEL:\n113819\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nKIMBALL DAYCARE CENTER & KINDERGARTEN INC\n105458567Z\n1636-1638 N KIMBALL AVE\nNaN\nCHICAGO\nIllinois\n60647\n11461500.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n3\n2020\nDFSS-HHS-CS-CEL:\n113818\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nJUDAH INTERNATIONAL OUTREACH MINISTRIES, INC\n94219962X\n856 N PULASKI RD\nNaN\nCHICAGO\nIllinois\n60651\n2356515.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n4\n2020\nDFSS-HHS-CS-CEL:\n113820\n0\n1070196\nDELEGATE AGENCY\n12/01/2019\n11/30/2022\nDEPT OF FAMILY AND SUPPORT SERVICES\nMarillac St. Vincent Family Services Inc DBA S...\n97791861L\n212 S FRANCISCO AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60612\n3666015.0\nNaN\nhttp://ecm.cityofchicago.org/eSMARTContracts/s...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n28823\n2023\nDFSS-CORP-HL-PSH:\n220413\n3\n1221503\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nINNER VOICE INC.\n6231926M\n1621 W WALNUT ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60612\n0.0\nNaN\nNaN\n\n\n28824\n2023\nDFSS-CORP-YS-OST:\n253846\n0\n1247493\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nAFTER-SCHOOL MATTERS, INC.|CLEANED-UP\n72580818P\n66 E RANDOLPH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60601\n32000.0\nNaN\nNaN\n\n\n28825\n2023\nDFSS-IDHS-HL-INTHS:\n253843\n0\n1235949\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nBREAKTHROUGH URBAN MINISTRIES, INC.\n94722896V\n402 N ST LOUIS AVENUE EFT\nNaN\nCHICAGO\nIllinois\n60624\n14400.0\nNaN\nNaN\n\n\n28826\n2023\nCDPH-RW-PA: ESS-HRSA PO 116685 CHICAGO HOUSE A...\n192085\n1\n1095441\nDELEGATE AGENCY\nNaN\nNaN\nDEPARTMENT OF HEALTH\nCHICAGO HOUSE & SOCIAL SERVICE AGENCY\n105470138T\n2229 S MICHIGAN AVE 304 EFT\nNaN\nCHICAGO\nIllinois\n60616\n-32025.2\nNaN\nNaN\n\n\n28827\n2023\nDFSS-HHS-CS-CEL:\n222199\n1\n1070196\nDELEGATE AGENCY\nNaN\nNaN\nDEPT OF FAMILY AND SUPPORT SERVICES\nALLISON'S INFANT & TODDLER CENTER INC\n62751817Z\n234 E 115TH ST FL 1ST\nNaN\nCHICAGO\nIllinois\n60628\n141923.0\nNaN\nNaN\n\n\n\n\n28828 rows × 19 columns\n\n\n\nEach row corresponds to one contract, and we have each contract’s id number, the year in which they were granted, the amount of the contract, and the vendor’s name and address, among other variables.\nNow, consider the following grouped summary of the contracts dataset, which shows the number of contracts by state of the vendor in each year:\n\ncontracts_summary = (\n    contracts.groupby([\"state\", \"year_of_contract\"]).size().reset_index(name=\"n\")\n)\n\ncontracts_summary\n\n\n\n\n\n\n\n\nstate\nyear_of_contract\nn\n\n\n\n\n0\nAlabama\n2020\n1\n\n\n1\nAlabama\n2021\n2\n\n\n2\nAlabama\n2022\n1\n\n\n3\nAlabama\n2023\n7\n\n\n4\nArizona\n2020\n3\n\n\n...\n...\n...\n...\n\n\n128\nWashington\n2021\n1\n\n\n129\nWisconsin\n2020\n18\n\n\n130\nWisconsin\n2021\n15\n\n\n131\nWisconsin\n2022\n17\n\n\n132\nWisconsin\n2023\n25\n\n\n\n\n133 rows × 3 columns\n\n\n\nThe output of this grouped operation is a quintessentially “long” dataset! Each observational unit (each contract type) occupies multiple rows (four rows per state, to be exact), with one row for each measurement (each year).\nSo, as you now see, long data often can arrive as an output of grouped summaries, among other data manipulations.\nNow, let’s see how to convert such long data into a wide format with pivot.\nThe code is quite straightforward:\n\n(\n    contracts_summary.pivot(\n        index=\"state\", columns=\"year_of_contract\", values=\"n\"\n    ).reset_index()\n)\n\n\n\n\n\n\n\nyear_of_contract\nstate\n2020\n2021\n2022\n2023\n\n\n\n\n0\nAlabama\n1.0\n2.0\n1.0\n7.0\n\n\n1\nArizona\n3.0\n1.0\n3.0\n2.0\n\n\n2\nArkansas\n1.0\nNaN\n1.0\nNaN\n\n\n3\nBritish Columbia\nNaN\n1.0\nNaN\nNaN\n\n\n4\nCalifornia\n36.0\n42.0\n43.0\n38.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n38\nTexas\n25.0\n24.0\n37.0\n28.0\n\n\n39\nVermont\nNaN\n1.0\nNaN\nNaN\n\n\n40\nVirginia\n4.0\n4.0\n7.0\n9.0\n\n\n41\nWashington\nNaN\n1.0\nNaN\nNaN\n\n\n42\nWisconsin\n18.0\n15.0\n17.0\n25.0\n\n\n\n\n43 rows × 5 columns\n\n\n\nAs you can see, pivot has three important arguments: index, columns, and values.\n\nindex defines which column(s) to use as the new index. In our case, it’s the state, since we want each row to represent one state.\ncolumns identifies which variable to use to define column names in the wide format. In our case, it’s the year of the contract. You can see that the years are now the column names.\nvalues specifies which values will become the core of the wide data format. In our case, it’s the number of contracts. data format.\n\nYou might also want to have the years be your primary observational/statistical unit, with each year occupying one row. This can be carried out similarly to the above example, but with year_of_contract as the index and state as the columns:\n\n(\n    contracts_summary.pivot(\n        index=\"year_of_contract\", columns=\"state\", values=\"n\"\n    ).reset_index()\n)\n\n\n\n\n\n\n\nstate\nyear_of_contract\nAlabama\nArizona\nArkansas\nBritish Columbia\nCalifornia\nCanada\nColorado\nConnecticut\nDelaware\n...\nOregon\nPennsylvania\nRhode Island\nSouth Carolina\nTennessee\nTexas\nVermont\nVirginia\nWashington\nWisconsin\n\n\n\n\n0\n2020\n1.0\n3.0\n1.0\nNaN\n36.0\n1.0\n6.0\n1.0\nNaN\n...\n5.0\n20.0\n1.0\n2.0\n2.0\n25.0\nNaN\n4.0\nNaN\n18.0\n\n\n1\n2021\n2.0\n1.0\nNaN\n1.0\n42.0\n1.0\n2.0\n5.0\nNaN\n...\nNaN\n24.0\nNaN\n3.0\n2.0\n24.0\n1.0\n4.0\n1.0\n15.0\n\n\n2\n2022\n1.0\n3.0\n1.0\nNaN\n43.0\n1.0\n7.0\n3.0\n1.0\n...\nNaN\n31.0\nNaN\n2.0\n3.0\n37.0\nNaN\n7.0\nNaN\n17.0\n\n\n3\n2023\n7.0\n2.0\nNaN\nNaN\n38.0\nNaN\n6.0\n5.0\n2.0\n...\nNaN\n37.0\nNaN\n1.0\n3.0\n28.0\nNaN\n9.0\nNaN\n25.0\n\n\n\n\n4 rows × 44 columns\n\n\n\nHere the unique observation units (our rows) are now the years (2021, 2022, 2023).\n\n\n\n\n\n\nPractice\n\n\n\nThe population dataset shows the populations of 219 countries over time.\nPivot this data into a wide format. Your answer should have 20 columns and 219 rows.\n\npopulation = pd.read_csv(\"data/tidyr_population.csv\")\npopulation\n\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\n0\nAfghanistan\n1995\n17586073\n\n\n1\nAfghanistan\n1996\n18415307\n\n\n2\nAfghanistan\n1997\n19021226\n\n\n3\nAfghanistan\n1998\n19496836\n\n\n4\nAfghanistan\n1999\n19987071\n\n\n...\n...\n...\n...\n\n\n4055\nZimbabwe\n2009\n12888918\n\n\n4056\nZimbabwe\n2010\n13076978\n\n\n4057\nZimbabwe\n2011\n13358738\n\n\n4058\nZimbabwe\n2012\n13724317\n\n\n4059\nZimbabwe\n2013\n14149648\n\n\n\n\n4060 rows × 3 columns\n\n\n\n\n# Your code here",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#pivoting-can-be-hard",
    "href": "p_untangled_pivoting.html#pivoting-can-be-hard",
    "title": "19  Reshaping data",
    "section": "19.7 Pivoting can be hard",
    "text": "19.7 Pivoting can be hard\nWe have mostly looked at very simple examples of pivoting here, but in the wild, pivoting can be very difficult to do accurately.\nWhen you run into such cases, we recommend looking at the official documentation of pivoting from the pandas team, as it is quite rich in examples.",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "p_untangled_pivoting.html#wrap-up",
    "href": "p_untangled_pivoting.html#wrap-up",
    "title": "19  Reshaping data",
    "section": "19.8 Wrap up",
    "text": "19.8 Wrap up\nCongratulations! You’ve mastered the art of reshaping data with pandas.\nYou now understand the differences between wide and long formats, and can skillfully use melt() and pivot() to transform your data as needed.\nRemember, there’s no universally “best” format – it depends on your specific analysis or visualization needs. With these skills, you’re now equipped to handle data in any shape it comes in. Keep practicing with different datasets to reinforce your learning!",
    "crumbs": [
      "Data Manipulation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  }
]